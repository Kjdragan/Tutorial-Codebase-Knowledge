<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>04 Topic Processing Orchestrator</title>
    <style>
        pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #F00 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #04D } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #00F; font-weight: bold } /* Name.Class */
.highlight .no { color: #800 } /* Name.Constant */
.highlight .nd { color: #A2F } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #00F } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.highlight .w { color: #BBB } /* Text.Whitespace */
.highlight .mb { color: #666 } /* Literal.Number.Bin */
.highlight .mf { color: #666 } /* Literal.Number.Float */
.highlight .mh { color: #666 } /* Literal.Number.Hex */
.highlight .mi { color: #666 } /* Literal.Number.Integer */
.highlight .mo { color: #666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #00F } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666 } /* Literal.Number.Integer.Long */
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        .navigation {
            display: flex;
            justify-content: space-between;
            margin: 20px 0;
            padding: 10px;
            background-color: #f6f8fa;
            border-radius: 6px;
        }
    </style>
</head>
<body>
    <div class="navigation">
        <a class="nav-link" href="03_shared_memory.html">&larr; Previous</a>
        <a class="nav-link" href="index.html">Index</a>
        <a class="nav-link" href="05_youtube_data_utilities.html">Next &rarr;</a>
    </div>
    <h1>Chapter 4: Topic Processing Orchestrator</h1>

<p>Welcome back! In <a rel="noopener" target="_blank" href="03_shared_memory.md">Chapter 3: Shared Memory</a>, we learned how different steps (Nodes) in our pipeline share information using a central dictionary called <code>shared_memory</code>. We saw how the <code>TopicExtractionNode</code> adds a list of topics it found into this shared space.</p>

<p>Now, imagine our video analyzer found 10 different topics in a long video! For each topic, we want to:</p>

<ol>
<li>Generate some Questions and Answers (Q&amp;A).</li>
<li>Create a super simple "Explain Like I'm 5" (ELI5) summary.</li>
</ol>

<p>If we did this one topic at a time, it could take a while, especially since generating Q&amp;A and ELI5 often involves asking an AI model (like the ones we'll see in <a rel="noopener" target="_blank" href="06_llm_utility.md">Chapter 6: LLM Utility</a>), which can be slow. How can we speed this up?</p>

<h2>The Problem: Analyzing Many Topics Takes Time</h2>

<p>Let's say analyzing one topic takes 30 seconds. If we have 10 topics, doing them one after another would take 300 seconds (5 minutes!). That's a long coffee break!</p>

<pre><code>Topic 1 Analysis --&gt; Wait 30s --&gt; Topic 2 Analysis --&gt; Wait 30s --&gt; ... --&gt; Topic 10 Analysis --&gt; Wait 30s
Total Time = 10 * 30s = 300s
</code></pre>

<p>We need a smarter way to handle this.</p>

<h2>The Solution: Divide and Conquer with Parallelism!</h2>

<p>Think about a big research project given to a team. Does the team lead make everyone wait while one person researches the first part, then the next person researches the second part? No! The lead divides the work: "Alice, you research Topic A. Bob, you research Topic B. Charlie, you research Topic C. Go!" Everyone works <em>at the same time</em> (in parallel). When they're done, the lead collects all their findings.</p>

<p>This is exactly what the <strong>Topic Processing Orchestrator</strong> (<code>TopicOrchestratorNode</code>) does! It acts like that efficient team lead for analyzing topics.</p>

<ul>
<li><strong>The Big Task:</strong> Analyze all topics found in the video.</li>
<li><strong>The Team Lead:</strong> <code>TopicOrchestratorNode</code>.</li>
<li><strong>The Team Members:</strong> Specialized workers called <code>TopicProcessorNode</code> (we'll see these workers in action soon).</li>
<li><strong>The Assignment:</strong> The <code>TopicOrchestratorNode</code> gives <em>each</em> topic to a <em>separate</em> <code>TopicProcessorNode</code>.</li>
<li><strong>Working in Parallel:</strong> It lets all the <code>TopicProcessorNode</code>s work simultaneously to generate Q&amp;A and ELI5 for their assigned topic.</li>
<li><strong>Collecting Results:</strong> Once all workers are finished, the <code>TopicOrchestratorNode</code> gathers their results and puts them together.</li>
</ul>

<p>If analyzing one topic still takes 30 seconds, but we have, say, 3 workers (or more!) operating in parallel, we can analyze 3 topics in roughly 30 seconds. If we have enough workers for all 10 topics, the total time might be closer to just 30 seconds (plus a little overhead for managing them), instead of 300!</p>

<pre><code>                                Topic 1 Analysis (Worker 1) \
Simultaneously Start --&gt;        Topic 2 Analysis (Worker 2)  --&gt; Wait ~30s --&gt; All Done!
                                Topic 3 Analysis (Worker 3) /
                                ... and so on ...
</code></pre>

<p>This parallel approach is sometimes called <strong>Map-Reduce</strong>:</p>

<ol>
<li><strong>Map:</strong> Assign (map) each piece of work (topic) to a worker (<code>TopicProcessorNode</code>).</li>
<li><strong>Reduce:</strong> Collect (reduce) the results from all workers into a final combined output.</li>
</ol>

<h2>How the Topic Orchestrator Fits In</h2>

<p>The <code>TopicOrchestratorNode</code> is just another <a rel="noopener" target="_blank" href="02_node__pipeline_step_.md">Node (Pipeline Step)</a> in our main pipeline, run by the <a rel="noopener" target="_blank" href="01_pipeline_orchestration.md">Pipeline Orchestration</a> (<code>run_pipeline</code> function).</p>

<p>Here's what it does in the pipeline flow:</p>

<ol>
<li><strong>Input:</strong> It reads the <code>topics</code> (a list of strings) and the full <code>transcript</code> (a long string) from the <a rel="noopener" target="_blank" href="03_shared_memory.md">Shared Memory</a>, put there by previous Nodes.</li>
<li><strong>Processing (Parallel):</strong> It performs the "Map-Reduce" strategy described above, using multiple <code>TopicProcessorNode</code> instances internally.</li>
<li><strong>Output:</strong> It writes the collected results back into <a rel="noopener" target="_blank" href="03_shared_memory.md">Shared Memory</a>. The results are typically stored in dictionaries keyed by topic, for example:
<ul>
<li><code>"qa_pairs"</code>: A dictionary where keys are topics and values are lists of Q&amp;A for that topic.</li>
<li><code>"eli5_content"</code>: A dictionary where keys are topics and values are the ELI5 explanations.</li>
<li><code>"topic_results"</code>: A combined dictionary holding both Q&amp;A and ELI5 for each topic.</li>
</ul></li>
</ol>

<h2>Using the Topic Orchestrator Node</h2>

<p>We don't usually call the <code>TopicOrchestratorNode</code> directly. The main <code>run_pipeline</code> function in <code>src/main.py</code> calls it after the topics have been extracted.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified snippet from src/main.py</span>

<span class="c1"># ... (Previous nodes ran, shared_memory contains &#39;topics&#39; and &#39;transcript&#39;)</span>

<span class="c1"># 4. Topic Orchestration Node</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;[4/5] Starting Topic Processing Orchestration...&quot;</span><span class="p">)</span>
<span class="c1"># Create the orchestrator node, giving it shared memory</span>
<span class="c1"># We can configure how many &#39;workers&#39; (max_workers) it can use</span>
<span class="n">topic_orchestrator</span> <span class="o">=</span> <span class="n">TopicOrchestratorNode</span><span class="p">(</span><span class="n">shared_memory</span><span class="p">,</span> <span class="n">max_workers</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Run the node - this triggers the parallel processing</span>
<span class="n">shared_memory</span> <span class="o">=</span> <span class="n">topic_orchestrator</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>

<span class="c1"># Check for errors</span>
<span class="k">if</span> <span class="s2">&quot;error&quot;</span> <span class="ow">in</span> <span class="n">shared_memory</span><span class="p">:</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Topic Orchestration failed...&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">shared_memory</span>

<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Successfully processed all topics.&quot;</span><span class="p">)</span>

<span class="c1"># Now, shared_memory contains the new keys like &#39;qa_pairs&#39; and &#39;eli5_content&#39;</span>
<span class="c1"># Example: shared_memory[&#39;qa_pairs&#39;][&#39;Machine Learning Basics&#39;] might be a list of Q&amp;A</span>
<span class="c1"># Example: shared_memory[&#39;eli5_content&#39;][&#39;Machine Learning Basics&#39;] might be the ELI5 text</span>

<span class="c1"># ... (Next node, like HTML Generation, can use these results) ...</span>
</code></pre>
</div>

<p>In this example:</p>

<ul>
<li>We create <code>TopicOrchestratorNode</code>, passing the current <code>shared_memory</code> (which must contain <code>"topics"</code> and <code>"transcript"</code>).</li>
<li>We also tell it it can use up to <code>max_workers=3</code> parallel workers.</li>
<li>Calling <code>topic_orchestrator.run()</code> kicks off the parallel processing.</li>
<li>After it finishes, the <code>shared_memory</code> dictionary is updated with the results (<code>"qa_pairs"</code>, <code>"eli5_content"</code>, etc.).</li>
</ul>

<h2>Under the Hood: How Parallelism Works</h2>

<p>Let's visualize how the <code>TopicOrchestratorNode</code> (TON) manages the <code>TopicProcessorNode</code> (TPN) workers.</p>

<pre><code>sequenceDiagram
    participant RP as run_pipeline
    participant TON as TopicOrchestratorNode
    participant SM as Shared Memory
    participant TPN1 as TopicProcessorNode (Worker 1)
    participant TPN2 as TopicProcessorNode (Worker 2)

    RP-&gt;&gt;TON: run(SM) [SM has topics: T1, T2]
    TON-&gt;&gt;SM: prep() [Reads topics=[T1, T2], transcript]
    Note over TON, TPN2: exec() starts Map Phase (using Thread Pool)
    TON-&gt;&gt;TPN1: Create &amp; submit _process_topic(T1)
    TON-&gt;&gt;TPN2: Create &amp; submit _process_topic(T2)
    Note over TPN1, TPN2: TPN1 and TPN2 run in parallel
    TPN1-&gt;&gt;TPN1: Runs its own prep/exec/post (gets Q&amp;A/ELI5 for T1)
    TPN2-&gt;&gt;TPN2: Runs its own prep/exec/post (gets Q&amp;A/ELI5 for T2)
    TPN1--&gt;&gt;TON: Returns result for T1
    TPN2--&gt;&gt;TON: Returns result for T2
    Note over TON: Map Phase ends, Reduce Phase starts
    TON-&gt;&gt;TON: _reduce_phase() [Collects results for T1, T2]
    TON-&gt;&gt;SM: post() [Writes combined 'qa_pairs', 'eli5_content' to SM]
    TON--&gt;&gt;RP: Return updated SM
</code></pre>

<p>This diagram shows:</p>

<ol>
<li><code>run_pipeline</code> calls the <code>TopicOrchestratorNode</code> (<code>TON</code>).</li>
<li><code>TON</code> reads the list of topics (e.g., T1, T2) from <code>Shared Memory</code>.</li>
<li><code>TON</code> starts its <code>exec</code> phase, creating and launching worker <code>TopicProcessorNode</code>s (<code>TPN1</code>, <code>TPN2</code>) in parallel, one for each topic.</li>
<li>Each <code>TPN</code> does its work (generating Q&amp;A/ELI5 for its specific topic).</li>
<li><code>TON</code> waits for all <code>TPN</code> workers to finish and collects their individual results.</li>
<li><code>TON</code> combines these results and writes them back to <code>Shared Memory</code>.</li>
</ol>

<h2>Diving into the Code (<code>src/nodes/topic_orchestrator_node.py</code>)</h2>

<p>Let's peek inside the <code>TopicOrchestratorNode</code>. Remember, it follows the standard <a rel="noopener" target="_blank" href="02_node__pipeline_step_.md">Node (Pipeline Step)</a> structure: <code>prep</code>, <code>exec</code>, <code>post</code>.</p>

<p><strong>1. Preparation (<code>prep</code>)</strong></p>

<p>The <code>prep</code> method gets the necessary inputs from <code>shared_memory</code>.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_orchestrator_node.py</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TopicOrchestratorNode</span><span class="p">(</span><span class="n">BaseNode</span><span class="p">):</span>
    <span class="c1"># ... (init method stores max_workers) ...</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prep</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;TopicOrchestrator: Preparing...&quot;</span><span class="p">)</span>
        <span class="c1"># Check if needed data exists in shared memory</span>
        <span class="k">if</span> <span class="s2">&quot;topics&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Topics not found in shared memory&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;transcript&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Transcript not found in shared memory&quot;</span><span class="p">)</span>

        <span class="c1"># Get the topics and transcript</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">topics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;topics&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transcript</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;transcript&quot;</span><span class="p">]</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">topics</span><span class="p">)</span><span class="si">}</span><span class="s2"> topics to process.&quot;</span><span class="p">)</span>
</code></pre>
</div>

<p>This part just checks if the <code>"topics"</code> list and <code>"transcript"</code> string are available in the shared memory and stores them within the node instance for later use.</p>

<p><strong>2. Execution (<code>exec</code>)</strong></p>

<p>The <code>exec</code> method orchestrates the Map and Reduce phases.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_orchestrator_node.py</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">exec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;TopicOrchestrator: Executing...&quot;</span><span class="p">)</span>
        <span class="c1"># Map phase: Process topics in parallel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_map_phase</span><span class="p">()</span>

        <span class="c1"># Reduce phase: Combine the results</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reduce_phase</span><span class="p">()</span>
</code></pre>
</div>

<p>It simply calls two helper methods: <code>_map_phase</code> to run the workers and <code>_reduce_phase</code> to collect the results.</p>

<p><strong>3. Map Phase (<code>_map_phase</code>)</strong></p>

<p>This is where the parallelism happens using Python's <code>concurrent.futures.ThreadPoolExecutor</code>. Think of the executor as a manager for a pool of worker threads.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_orchestrator_node.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">concurrent.futures</span> <span class="c1"># Tool for running things in parallel</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_map_phase</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Starting Map phase with </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_workers</span><span class="si">}</span><span class="s2"> workers...&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">topic_results</span> <span class="o">=</span> <span class="p">{}</span> <span class="c1"># Dictionary to store results</span>

        <span class="c1"># Create a pool of workers</span>
        <span class="k">with</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">(</span><span class="n">max_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_workers</span><span class="p">)</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
            <span class="c1"># Give each topic to a worker by submitting the _process_topic task</span>
            <span class="n">future_to_topic</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_process_topic</span><span class="p">,</span> <span class="n">topic</span><span class="p">):</span> <span class="n">topic</span>
                <span class="k">for</span> <span class="n">topic</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">topics</span>
            <span class="p">}</span>

            <span class="c1"># Wait for workers to finish and collect results</span>
            <span class="k">for</span> <span class="n">future</span> <span class="ow">in</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">as_completed</span><span class="p">(</span><span class="n">future_to_topic</span><span class="p">):</span>
                <span class="n">topic</span> <span class="o">=</span> <span class="n">future_to_topic</span><span class="p">[</span><span class="n">future</span><span class="p">]</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Get the result from the finished worker</span>
                    <span class="n">result</span> <span class="o">=</span> <span class="n">future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">topic_results</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span> <span class="c1"># Store result</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished processing topic: </span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error processing topic &#39;</span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="c1"># Store an error message for this topic</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">topic_results</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;error&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)}</span>
</code></pre>
</div>

<ul>
<li>It creates a <code>ThreadPoolExecutor</code> which manages the worker threads.</li>
<li><code>executor.submit(self._process_topic, topic)</code> tells the executor: "Hey, run the <code>_process_topic</code> function with this <code>topic</code> as input, using one of your available workers." This happens for <em>all</em> topics.</li>
<li><code>concurrent.futures.as_completed(...)</code> waits for any worker to finish, gets its result (<code>future.result()</code>), and stores it in <code>self.topic_results</code> dictionary, keyed by the topic name.</li>
</ul>

<p><strong>4. Processing a Single Topic (<code>_process_topic</code>)</strong></p>

<p>This helper function is what each worker thread actually runs. It creates and runs the specialized <code>TopicProcessorNode</code> for a single topic.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_orchestrator_node.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.nodes.topic_processor_node</span><span class="w"> </span><span class="kn">import</span> <span class="n">TopicProcessorNode</span> <span class="c1"># The worker node</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_process_topic</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">topic</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Worker starting on topic: </span><span class="si">{</span><span class="n">topic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="c1"># Create the specialist node for *this specific topic*</span>
        <span class="n">processor</span> <span class="o">=</span> <span class="n">TopicProcessorNode</span><span class="p">(</span>
            <span class="n">topic</span><span class="o">=</span><span class="n">topic</span><span class="p">,</span>
            <span class="n">transcript</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">transcript</span><span class="p">,</span>
            <span class="n">questions_per_topic</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">questions_per_topic</span> <span class="c1"># Configurable option</span>
        <span class="p">)</span>
        <span class="c1"># Run the specialist node and get its results</span>
        <span class="c1"># The result is a dictionary with &#39;qa_pairs&#39; and &#39;eli5_content&#39; for this topic</span>
        <span class="n">result_dict</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
        <span class="c1"># We only need the part specific to this topic from the processor&#39;s output</span>
        <span class="k">return</span> <span class="n">result_dict</span><span class="p">[</span><span class="s2">&quot;topic_results&quot;</span><span class="p">][</span><span class="n">topic</span><span class="p">]</span>
</code></pre>
</div>

<p>This function takes one <code>topic</code>, creates a <code>TopicProcessorNode</code> specifically for it (giving it the topic and the full transcript), runs that node, and returns the Q&amp;A/ELI5 results just for that single topic.</p>

<p><strong>5. Reduce Phase (<code>_reduce_phase</code>)</strong></p>

<p>After the <code>_map_phase</code> has collected results from all workers into <code>self.topic_results</code>, the <code>_reduce_phase</code> combines them into the final structure needed in <code>shared_memory</code>.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_orchestrator_node.py</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_reduce_phase</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Starting Reduce phase...&quot;</span><span class="p">)</span>
        <span class="n">qa_pairs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">eli5_content</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Go through results collected from workers</span>
        <span class="k">for</span> <span class="n">topic</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;error&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span> <span class="c1"># Skip topics that had errors</span>
                <span class="n">qa_pairs</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;qa_pairs&quot;</span><span class="p">,</span> <span class="p">[])</span>
                <span class="n">eli5_content</span><span class="p">[</span><span class="n">topic</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eli5_content&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># Store the combined results in shared memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;qa_pairs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">qa_pairs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;eli5_content&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">eli5_content</span>
        <span class="c1"># Also store the raw results per topic</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;topic_results&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_results</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Reduce phase complete. Results added to shared memory.&quot;</span><span class="p">)</span>
</code></pre>
</div>

<p>This function iterates through the <code>self.topic_results</code> gathered during the map phase. It creates two dictionaries, <code>qa_pairs</code> and <code>eli5_content</code>, where the keys are the topic strings. It then populates these dictionaries with the results from each worker and finally adds these combined dictionaries to the main <code>shared_memory</code>.</p>

<p><strong>6. Post-Processing (<code>post</code>)</strong></p>

<p>The <code>post</code> method mainly does final checks and logging.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_orchestrator_node.py</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">post</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Basic check if any results were generated</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">topic_results</span><span class="p">:</span>
             <span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;error&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;No topic results were generated&quot;</span>
             <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">[</span><span class="s2">&quot;error&quot;</span><span class="p">])</span>
             <span class="k">return</span> <span class="c1"># Stop if no results</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Topic Orchestrator Node completed successfully.&quot;</span><span class="p">)</span>
</code></pre>
</div>

<p>And that's how the <code>TopicOrchestratorNode</code> uses parallelism (Map-Reduce) to speed up the processing of multiple topics by delegating the work to <code>TopicProcessorNode</code> instances!</p>

<h2>Conclusion</h2>

<p>In this chapter, we explored the <code>TopicOrchestratorNode</code>, a smart manager within our pipeline. We learned:</p>

<ul>
<li><strong>Problem:</strong> Analyzing many topics one-by-one is slow.</li>
<li><strong>Solution:</strong> Use parallelism with a <strong>Map-Reduce</strong> approach.</li>
<li><strong>TopicOrchestratorNode:</strong> Acts as a team lead, assigning each topic to a worker (<code>TopicProcessorNode</code>).</li>
<li><strong>Parallel Execution:</strong> Workers run simultaneously using threads, significantly speeding up the process.</li>
<li><strong>Map Phase:</strong> Assigns tasks to workers and runs them.</li>
<li><strong>Reduce Phase:</strong> Collects results from all workers and combines them.</li>
<li><strong>Integration:</strong> It reads <code>topics</code> and <code>transcript</code> from <a rel="noopener" target="_blank" href="03_shared_memory.md">Shared Memory</a> and writes back <code>qa_pairs</code> and <code>eli5_content</code>.</li>
</ul>

<p>This node demonstrates a more advanced pattern within our pipeline architecture, showing how we can optimize for performance.</p>

<p>So far, we've seen how the pipeline flows, how individual nodes work, how they share data, and how we can use parallelism. But where does data like the transcript or video title actually come from? Next, we'll look at the tools used to interact with YouTube itself.</p>

<p>Let's move on to <a rel="noopener" target="_blank" href="05_youtube_data_utilities.md">Chapter 5: YouTube Data Utilities</a>!</p>

<hr />

<p>Generated by <a rel="noopener" target="_blank" href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>

    <div class="navigation">
        <a class="nav-link" href="03_shared_memory.html">&larr; Previous</a>
        <a class="nav-link" href="index.html">Index</a>
        <a class="nav-link" href="05_youtube_data_utilities.html">Next &rarr;</a>
    </div>
</body>
</html>