{"\nFor the project `pf1`:\n\nCodebase Context:\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "Okay, I need the actual list of file indices and paths from your codebase context to provide accurate `file_indices`.\n\n**Assuming you provide that list**, here is a template based on common software patterns. I will identify potential core abstractions. **Please replace the example `file_indices` with the correct ones from your context.**\n\n```yaml\n# --- Core Abstractions for pf1 ---\n# NOTE: Please replace the example file_indices below with the actual indices\n#       corresponding to the files in your specific codebase context.\n\n- name: Configuration Management\n  description: |\n    Handles loading and accessing application settings. It allows different parts\n    of the application to know how they should behave (e.g., database connection\n    details, feature flags, API keys).\n    It's like the settings menu for an application, defining how it runs without\n    changing the core code itself. Centralizes all tunable parameters.\n  file_indices:\n    - 0 # example/path/to/config/loader.py\n    - 1 # example/path/to/config/settings.py\n\n- name: Request Handling / API Endpoints\n  description: |\n    Defines how the application responds to incoming requests from users or other\n    systems, typically over the network (like HTTP requests). It routes requests\n    to the appropriate logic and formats the responses.\n    Think of it as the reception desk or customer service counter for the\n    application, directing incoming queries to the right department and providing answers.\n  file_indices:\n    - 2 # example/path/to/api/routes.py\n    - 3 # example/path/to/server/main.py\n    - 4 # example/path/to/handlers/user_handler.py\n\n- name: Core Logic / Service Layer\n  description: |\n    Contains the main business logic and orchestrates operations. It processes\n    data, applies rules, and performs the primary functions of the application,\n    often triggered by request handlers.\n    It's like the kitchen in a restaurant where the actual cooking (processing)\n    happens based on the orders received from the counter (API).\n  file_indices:\n    - 5 # example/path/to/services/processing_service.py\n    - 6 # example/path/to/core/logic.py\n\n- name: Data Modeling / Representation\n  description: |\n    Defines the structure and relationships of the data the application works\n    with. These are often classes or schemas that represent real-world entities\n    or concepts (e.g., User, Product, Order).\n    It's like the blueprints for different types of information, ensuring data\n    is consistent and organized throughout the application.\n  file_indices:\n    - 7 # example/path/to/models/user.py\n    - 8 # example/path/to/schemas/order_schema.py\n    - 9 # example/path/to/db/definitions.py\n\n- name: Data Access / Persistence\n  description: |\n    Manages interactions with storage systems like databases or file systems.\n    It provides a way to save (persist) data models and retrieve them later,\n    abstracting away the specific database commands.\n    It's like the library's catalog and shelving system, responsible for storing\n    books (data) and finding them when requested.\n  file_indices:\n    - 10 # example/path/to/db/database_client.py\n    - 11 # example/path/to/repositories/product_repo.py\n\n- name: Task Queue / Background Processing\n  description: |\n    Handles operations that shouldn't block the main application flow, like\n    sending emails, processing large files, or running scheduled jobs. Tasks\n    are added to a queue and processed by separate workers.\n    It's like a separate workshop or back office where time-consuming tasks\n    are handled without holding up the main customer service counter.\n  file_indices:\n    - 12 # example/path/to/tasks/email_sender.py\n    - 13 # example/path/to/workers/main_worker.py\n    - 14 # example/path/to/queues/celery_app.py # (If using Celery)\n\n# - name: Authentication / Authorization (Optional - Add if prominent)\n#   description: |\n#     Manages user identity verification (Authentication) and controls what logged-in\n#     users are allowed to do (Authorization). Ensures security and access control.\n#     It's like the security guard at the entrance checking IDs (AuthN) and verifying\n#     access badges for specific areas (AuthZ).\n#   file_indices:\n#     - 15 # example/path/to/auth/jwt_handler.py\n#     - 16 # example/path/to/auth/permissions.py\n\n```\n\n**To use this effectively:**\n\n1.  **Provide the list of file indices and paths** from your `pf1` project context.\n2.  I (or you) will then go through the list and map the *actual* relevant files to these (or similar) abstractions.\n3.  The names and descriptions might need slight adjustments based on the specific terminology and domain of the `pf1` project.", "\nFor the project `pf1`:\n\nCodebase Context:\n--- File Index 0: Kevin/covert to png.py ---\nimport subprocess\nimport os\nimport webbrowser\n\n# Paths\ninput_path = r\"C:\\Users\\kevin\\repos\\pf1\\Kevin\\gitdiagram.mmd\"\noutput_path = r\"C:\\Users\\kevin\\repos\\pf1\\Kevin\\gitdiagram.png\"\npuppeteer_config_path = r\"C:\\Users\\kevin\\repos\\pf1\\Kevin\\puppeteer-config.json\"\n\n# Image quality settings\nwidth = 2000  # Width in pixels\nheight = 2000  # Height in pixels (will maintain aspect ratio)\nscale = 2.0    # Scale factor (2.0 = double resolution)\n\n# Find the mmdc executable path\n# Common locations for npm global installs\npossible_mmdc_paths = [\n    r\"C:\\Users\\kevin\\AppData\\Roaming\\npm\\mmdc.cmd\",  # User install\n    r\"C:\\Users\\kevin\\AppData\\Roaming\\npm\\mmdc\",\n    r\"C:\\Program Files\\nodejs\\node_modules\\@mermaid-js\\mermaid-cli\\bin\\mmdc.js\",\n    r\"C:\\Program Files\\nodejs\\node_modules\\.bin\\mmdc\",\n    r\"C:\\Users\\kevin\\AppData\\Roaming\\npm\\node_modules\\@mermaid-js\\mermaid-cli\\bin\\mmdc.js\"\n]\n\n# Find the first path that exists\nmmdc_path = None\nfor path in possible_mmdc_paths:\n    if os.path.exists(path):\n        mmdc_path = path\n        print(f\"Found mmdc at: {mmdc_path}\")\n        break\n\nif not mmdc_path:\n    # Try to find it using where command (Windows equivalent of which)\n    try:\n        result = subprocess.run([\"where\", \"mmdc\"], capture_output=True, text=True, check=False)\n        if result.returncode == 0:\n            mmdc_path = result.stdout.strip().split('\\n')[0]\n            print(f\"Found mmdc using 'where' command: {mmdc_path}\")\n    except Exception as e:\n        print(f\"Error trying to locate mmdc: {e}\")\n\n# Mermaid CLI command\ntry:\n    if not mmdc_path:\n        raise FileNotFoundError(\"Could not find mmdc executable\")\n\n    print(f\"Executing: {mmdc_path}\")\n    print(f\"Input file: {input_path}\")\n    print(f\"Output file: {output_path}\")\n    print(f\"Puppeteer config: {puppeteer_config_path}\")\n    print(f\"Image size: {width}x{height}, Scale: {scale}\")\n\n    # Check if input file exists\n    if not os.path.exists(input_path):\n        print(f\"\u274c Input file does not exist: {input_path}\")\n        exit(1)\n\n    # Check if puppeteer config exists\n    if not os.path.exists(puppeteer_config_path):\n        print(f\"\u274c Puppeteer config file does not exist: {puppeteer_config_path}\")\n        exit(1)\n\n    # Create command based on whether it's a .cmd file or .js file\n    if mmdc_path.endswith('.cmd'):\n        # For .cmd files, run directly\n        command = [\n            mmdc_path,\n            \"-i\", input_path,\n            \"-o\", output_path,\n            \"--backgroundColor\", \"transparent\",\n            \"--puppeteerConfigFile\", puppeteer_config_path,\n            \"--width\", str(width),\n            \"--height\", str(height),\n            \"--scale\", str(scale)\n        ]\n    elif mmdc_path.endswith('.js'):\n        # For .js files, run with node\n        command = [\n            \"node\",\n            mmdc_path,\n            \"-i\", input_path,\n            \"-o\", output_path,\n            \"--backgroundColor\", \"transparent\",\n            \"--puppeteerConfigFile\", puppeteer_config_path,\n            \"--width\", str(width),\n            \"--height\", str(height),\n            \"--scale\", str(scale)\n        ]\n    else:\n        # For other executables\n        command = [\n            mmdc_path,\n            \"-i\", input_path,\n            \"-o\", output_path,\n            \"--backgroundColor\", \"transparent\",\n            \"--puppeteerConfigFile\", puppeteer_config_path,\n            \"--width\", str(width),\n            \"--height\", str(height),\n            \"--scale\", str(scale)\n        ]\n\n    # Execute the command\n    result = subprocess.run(command, check=True, capture_output=True, text=True)\n    print(f\"Command output: {result.stdout}\")\n\n    print(f\"\u2705 High-resolution diagram successfully generated: {output_path}\")\n\n    # Optionally open the output file (Windows default image viewer)\n    os.startfile(output_path)\n\nexcept FileNotFoundError:\n    print(\"\u274c Could not find the mmdc executable.\")\n    print(\"Make sure you've installed Mermaid CLI correctly:\")\n    print(\"    npm install -g @mermaid-js/mermaid-cli\")\n    print(\"\\nTry running these commands to troubleshoot:\")\n    print(\"    where mmdc\")\n    print(\"    npm list -g | findstr mermaid\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"\u274c Mermaid CLI failed with error code {e.returncode}:\")\n    print(f\"Command: {' '.join(e.cmd)}\")\n    print(f\"Output: {e.stdout}\")\n    print(f\"Error: {e.stderr}\")\n\n    # Check for Chrome/Puppeteer error\n    if \"Could not find Chrome\" in e.stderr:\n        print(\"\\n\ud83d\udd27 It looks like Puppeteer can't find Chrome. Try installing it with:\")\n        print(\"    npx puppeteer browsers install chrome-headless-shell\")\n        print(\"\\nOr try using a different renderer with the --puppeteerConfigFile option:\")\n        print(\"    mmdc -i input.md -o output.png --puppeteerConfigFile puppeteer-config.json\")\n        print(\"\\nExample puppeteer-config.json:\")\n        print(\"\"\"\n{\n  \"executablePath\": \"C:\\\\Users\\\\kevin\\\\.cache\\\\puppeteer\\\\chrome-headless-shell\\\\win64-135.0.7049.42\\\\chrome-headless-shell-win64\\\\chrome-headless-shell.exe\",\n  \"args\": [\"--no-sandbox\"]\n}\n        \"\"\")\nexcept Exception as e:\n    print(f\"\u274c Error: {str(e)}\")\n\n\n--- File Index 1: flow.py ---\nfrom pocketflow import Flow\nfrom nodes import GetQuestionNode, AnswerNode\n\ndef create_qa_flow():\n    \"\"\"Create and return a question-answering flow.\"\"\"\n    # Create nodes\n    get_question_node = GetQuestionNode()\n    answer_node = AnswerNode()\n    \n    # Connect nodes in sequence\n    get_question_node >> answer_node\n    \n    # Create flow starting with input node\n    return Flow(start=get_question_node)\n\n--- File Index 2: main.py ---\nfrom flow import qa_flow\n\n# Example main function\n# Please replace this with your own main function\ndef main():\n    shared = {\n        \"question\": \"In one sentence, what's the end of universe?\",\n        \"answer\": None\n    }\n\n    qa_flow.run(shared)\n    print(\"Question:\", shared[\"question\"])\n    print(\"Answer:\", shared[\"answer\"])\n\nif __name__ == \"__main__\":\n    main()\n\n--- File Index 3: nodes.py ---\nfrom pocketflow import Node\nfrom utils.call_llm import call_llm\n\nclass GetQuestionNode(Node):\n    def exec(self, _):\n        # Get question directly from user input\n        user_question = input(\"Enter your question: \")\n        return user_question\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the user's question\n        shared[\"question\"] = exec_res\n        return \"default\"  # Go to the next node\n\nclass AnswerNode(Node):\n    def prep(self, shared):\n        # Read question from shared\n        return shared[\"question\"]\n    \n    def exec(self, question):\n        # Call LLM to get the answer\n        return call_llm(question)\n    \n    def post(self, shared, prep_res, exec_res):\n        # Store the answer in shared\n        shared[\"answer\"] = exec_res\n\n--- File Index 4: src/main.py ---\n\"\"\"\nYouTube Video Summarizer - Main Application\n\"\"\"\nimport os\nimport sys\nimport argparse\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Add the project root to the path so we can import from src\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom src.nodes.input_processing_node import InputProcessingNode\nfrom src.nodes.content_extraction_node import ContentExtractionNode\nfrom src.nodes.topic_extraction_node import TopicExtractionNode\nfrom src.nodes.topic_orchestrator_node import TopicOrchestratorNode\nfrom src.nodes.html_generation_node import HTMLGenerationNode\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", enable_chunking=False, max_workers=3):\n    \"\"\"\n    Run the complete YouTube video summarization pipeline.\n    \n    Args:\n        youtube_url (str): URL of the YouTube video to summarize\n        output_dir (str): Directory to save the output HTML file\n        enable_chunking (bool): Whether to enable transcript chunking\n        max_workers (int): Maximum number of parallel workers for topic processing\n    \"\"\"\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"YouTube Video Summarizer\")\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n    logger.info(f\"Chunking: {'Enabled' if enable_chunking else 'Disabled'}\")\n    logger.info(f\"Parallel Workers: {max_workers}\")\n    logger.info(f\"{'='*60}\")\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": youtube_url}\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    logger.debug(f\"Output directory created/confirmed: {output_dir}\")\n    \n    # Configure chunking parameters if enabled\n    chunk_size = 4000 if enable_chunking else 0\n    overlap = 500 if enable_chunking else 0\n    \n    try:\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        input_node = InputProcessingNode(shared_memory)\n        shared_memory = input_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Input Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        logger.info(f\"Successfully processed video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        \n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Content Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        transcript_length = len(shared_memory.get('transcript', ''))\n        logger.info(f\"Successfully extracted transcript ({transcript_length} characters)\")\n        \n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, chunk_size=chunk_size, overlap=overlap)\n        shared_memory = topic_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        topics = shared_memory.get('topics', [])\n        logger.info(f\"Successfully extracted {len(topics)} topics\")\n        for i, topic in enumerate(topics):\n            logger.info(f\"  Topic {i+1}: {topic}\")\n        \n        # 4. Topic Processing Orchestrator Node\n        logger.info(\"[4/5] Starting Topic Processing...\")\n        orchestrator_node = TopicOrchestratorNode(shared_memory, max_workers=max_workers, questions_per_topic=3)\n        shared_memory = orchestrator_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        qa_pairs = shared_memory.get('qa_pairs', {})\n        eli5_content = shared_memory.get('eli5_content', {})\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        \n        logger.info(f\"Successfully processed {len(topics)} topics\")\n        logger.info(f\"Generated {total_qa_pairs} Q&A pairs and {len(eli5_content)} ELI5 explanations\")\n        \n        # 5. HTML Generation Node\n        logger.info(\"[5/5] Starting HTML Generation...\")\n        # Create a timestamped filename\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        video_id = shared_memory.get(\"video_id\", \"unknown\")\n        output_file = os.path.join(output_dir, f\"summary_{video_id}_{timestamp}.html\")\n        \n        html_node = HTMLGenerationNode(shared_memory, output_file)\n        shared_memory = html_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"HTML Generation failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        # Print summary\n        logger.info(f\"{'='*60}\")\n        logger.info(\"Summary Generation Complete!\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        logger.info(f\"Topics: {len(shared_memory.get('topics', []))}\")\n        logger.info(f\"Q&A Pairs: {sum(len(pairs) for pairs in shared_memory.get('qa_pairs', {}).values())}\")\n        logger.info(f\"Output File: {output_file}\")\n        logger.info(f\"{'='*60}\")\n        \n        return shared_memory\n        \n    except Exception as e:\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n\ndef main():\n    \"\"\"\n    Main entry point for the application.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"YouTube Video Summarizer\")\n    parser.add_argument(\"url\", nargs='?', default=None, help=\"YouTube video URL to summarize\")\n    parser.add_argument(\"--output\", \"-o\", default=\"output\", help=\"Output directory for HTML summary\")\n    parser.add_argument(\"--chunk\", action=\"store_true\", help=\"Enable transcript chunking for long videos\")\n    parser.add_argument(\"--workers\", \"-w\", type=int, default=3, help=\"Number of parallel workers for topic processing\")\n    \n    args = parser.parse_args()\n    \n    # If no URL is provided as a command-line argument, ask for user input\n    youtube_url = args.url\n    if not youtube_url:\n        youtube_url = input(\"Please enter a YouTube URL to summarize: \")\n    \n    logger.info(f\"Starting YouTube Video Summarizer with URL: {youtube_url}\")\n    \n    # Run the pipeline\n    run_pipeline(youtube_url, args.output, args.chunk, args.workers)\n\nif __name__ == \"__main__\":\n    main()\n\n\n--- File Index 5: src/nodes/base_node.py ---\n\"\"\"\nBase node class for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom abc import ABC, abstractmethod\nfrom src.utils.logger import logger\n\nclass BaseNode(ABC):\n    \"\"\"\n    Abstract base class for all nodes in the YouTube Video Summarizer.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary for data exchange between nodes\n        \"\"\"\n        self.shared_memory = shared_memory or {}\n        self.node_name = self.__class__.__name__\n        logger.debug(f\"{self.node_name} initialized\")\n    \n    @abstractmethod\n    def prep(self):\n        \"\"\"\n        Prepare for execution by reading necessary data from shared memory.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def exec(self):\n        \"\"\"\n        Execute the node's main functionality.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def post(self):\n        \"\"\"\n        Post-process and write results to shared memory.\n        \"\"\"\n        pass\n    \n    def run(self):\n        \"\"\"\n        Run the complete node workflow: prep, exec, post.\n        \n        Returns:\n            dict: The updated shared memory\n        \"\"\"\n        logger.debug(f\"{self.node_name} starting run\")\n        try:\n            self.prep()\n            if \"error\" not in self.shared_memory:\n                self.exec()\n            if \"error\" not in self.shared_memory:\n                self.post()\n            \n            if \"error\" in self.shared_memory:\n                logger.error(f\"{self.node_name} failed: {self.shared_memory['error']}\")\n            else:\n                logger.debug(f\"{self.node_name} completed successfully\")\n                \n        except Exception as e:\n            logger.exception(f\"Unexpected error in {self.node_name}: {str(e)}\")\n            self.shared_memory[\"error\"] = f\"{self.node_name} error: {str(e)}\"\n            \n        return self.shared_memory\n\n\n--- File Index 6: src/nodes/content_extraction_node.py ---\n\"\"\"\nContent Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\nfrom src.utils.logger import logger\n\nclass ContentExtractionNode(BaseNode):\n    \"\"\"\n    Node for extracting transcript from a YouTube video.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_id exists in shared memory.\n        \"\"\"\n        if \"video_id\" not in self.shared_memory:\n            error_msg = \"YouTube video ID not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Content Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(f\"Extracting transcript for video ID: {self.shared_memory['video_id']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute transcript extraction.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Extract the transcript\n        video_id = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Calling YouTube transcript API for video ID: {video_id}\")\n        transcript = extract_youtube_transcript(video_id)\n        \n        # Check if there was an error\n        if transcript.startswith(\"Error:\"):\n            logger.error(f\"Transcript extraction failed: {transcript}\")\n            self.shared_memory[\"error\"] = transcript\n            return\n        \n        self.shared_memory[\"transcript\"] = transcript\n        \n        # Log a preview of the transcript\n        preview_length = min(150, len(transcript))\n        logger.info(f\"Extracted transcript ({len(transcript)} characters)\")\n        logger.debug(f\"Transcript preview: {transcript[:preview_length]}...\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Content Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Failed to extract transcript\"\n            logger.error(f\"Error: {error_msg}\")\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Content Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample video ID\n    test_video_id = \"dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_id\": test_video_id}\n    \n    # Create and run the node\n    node = ContentExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    if \"error\" in updated_memory:\n        logger.error(f\"Error: {updated_memory['error']}\")\n    else:\n        transcript = updated_memory.get(\"transcript\", \"\")\n        logger.info(f\"Transcript length: {len(transcript)} characters\")\n        logger.info(f\"Transcript preview: {transcript[:200]}...\")\n\n\n--- File Index 7: src/nodes/eli5_transformation_node.py ---\n\"\"\"\nELI5 (Explain Like I'm 5) Transformation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass ELI5TransformationNode(BaseNode):\n    \"\"\"\n    Node for transforming content into child-friendly explanations (ELI5).\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n        self.eli5_content = {}\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and qa_pairs exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"qa_pairs\" not in self.shared_memory:\n            error_msg = \"Q&A pairs not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping ELI5 Transformation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        topics_count = len(self.shared_memory['topics'])\n        logger.info(f\"Transforming content into child-friendly explanations for {topics_count} topics\")\n        logger.debug(f\"Topics: {self.shared_memory['topics']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute ELI5 transformation for each topic and its Q&A pairs.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        topics = self.shared_memory[\"topics\"]\n        qa_pairs = self.shared_memory[\"qa_pairs\"]\n        \n        # Process each topic to create ELI5 explanations\n        for i, topic in enumerate(topics):\n            logger.info(f\"Creating ELI5 explanation for topic {i+1}/{len(topics)}: {topic}\")\n            \n            # Get Q&A pairs for this topic\n            topic_qa_pairs = qa_pairs.get(topic, [])\n            logger.debug(f\"Found {len(topic_qa_pairs)} Q&A pairs for topic '{topic}'\")\n            \n            # Combine Q&A pairs into a single text for context\n            qa_text = \"\"\n            for qa in topic_qa_pairs:\n                question = qa.get(\"question\", \"\")\n                answer = qa.get(\"answer\", \"\")\n                qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n            \n            # Create prompt for ELI5 transformation\n            prompt = textwrap.dedent(f\"\"\"\n            You are an expert at explaining complex topics to young children (5-7 years old).\n            \n            I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n            Your task is to:\n            1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n            2. Use simple words, short sentences, and concrete examples\n            3. Avoid jargon and technical terms\n            4. Use analogies to familiar concepts when possible\n            5. Keep the explanation under 200 words\n            6. Maintain the core information while simplifying the language\n            \n            Topic: {topic}\n            \n            Q&A Context:\n            {qa_text}\n            \n            Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n            \"\"\")\n            \n            # Call LLM to generate ELI5 explanation\n            try:\n                logger.debug(f\"Calling LLM for topic '{topic}' ELI5 transformation\")\n                response = call_llm(prompt, temperature=0.7, max_tokens=500)\n                logger.debug(f\"LLM response for topic '{topic}' ELI5: {response[:100]}...\")\n                \n                # Clean up the response\n                explanation = response.strip()\n                \n                # Store the ELI5 explanation\n                self.eli5_content[topic] = explanation\n                logger.info(f\"Generated ELI5 explanation for topic '{topic}' ({len(explanation)} characters)\")\n                \n            except Exception as e:\n                error_msg = f\"Error calling LLM for topic '{topic}': {str(e)}\"\n                logger.error(error_msg)\n                self.eli5_content[topic] = f\"Sorry, I couldn't create a simple explanation for {topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and store the ELI5 explanations in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in ELI5 Transformation Node: {self.shared_memory['error']}\")\n            return\n        \n        # Store the ELI5 explanations in shared memory\n        self.shared_memory[\"eli5_content\"] = self.eli5_content\n        \n        # Print a summary of the generated ELI5 explanations\n        logger.info(f\"Generated ELI5 explanations for {len(self.eli5_content)} topics\")\n        \n        for topic, explanation in self.eli5_content.items():\n            preview = explanation[:100] + \"...\" if len(explanation) > 100 else explanation\n            logger.info(f\"  Topic '{topic}':\")\n            logger.info(f\"    {preview}\")\n        \n        logger.info(\"ELI5 Transformation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and Q&A pairs\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_qa_pairs = {\n        \"Machine Learning Basics\": [\n            {\n                \"question\": \"What is machine learning?\",\n                \"answer\": \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Unlike traditional programming where you explicitly program rules, in machine learning, you train models on data and they learn patterns.\"\n            },\n            {\n                \"question\": \"Why is machine learning important?\",\n                \"answer\": \"Machine learning is important because it allows computers to find insights and make predictions without being explicitly programmed. It can handle complex tasks that would be difficult to code manually.\"\n            }\n        ],\n        \"Types of Machine Learning\": [\n            {\n                \"question\": \"What is supervised learning?\",\n                \"answer\": \"Supervised learning is a type of machine learning where the model is trained on labeled data. For example, you might have images labeled as 'cat' or 'dog' and the model learns to distinguish between them.\"\n            },\n            {\n                \"question\": \"What is unsupervised learning?\",\n                \"answer\": \"Unsupervised learning is where the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task where the algorithm groups similar data points together.\"\n            }\n        ]\n    }\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"qa_pairs\": test_qa_pairs\n    }\n    \n    # Create and run the node\n    node = ELI5TransformationNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(explanation)\n\n\n--- File Index 8: src/nodes/html_generation_node.py ---\n\"\"\"\nHTML Generation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.generate_html import generate_html\nfrom src.utils.logger import logger\n\nclass HTMLGenerationNode(BaseNode):\n    \"\"\"\n    Node for generating HTML visualization of the video summary.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, output_path=None):\n        \"\"\"\n        Initialize the node with shared memory and output path.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            output_path (str): Path to save the HTML output file\n        \"\"\"\n        super().__init__(shared_memory)\n        self.output_path = output_path\n        logger.debug(f\"HTMLGenerationNode initialized with output_path={output_path}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if all required data exists in shared memory.\n        \"\"\"\n        required_keys = [\"video_id\", \"metadata\", \"topics\", \"qa_pairs\", \"eli5_content\"]\n        missing_keys = [key for key in required_keys if key not in self.shared_memory]\n        \n        if missing_keys:\n            error_msg = f\"Missing required data in shared memory: {', '.join(missing_keys)}\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping HTML Generation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Preparing to generate HTML summary\")\n        logger.debug(f\"Found {len(self.shared_memory['topics'])} topics and {len(self.shared_memory['eli5_content'])} ELI5 explanations\")\n    \n    def exec(self):\n        \"\"\"\n        Execute HTML generation using the data in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Prepare the summary data for HTML generation\n        summary_data = {\n            \"video_id\": self.shared_memory[\"video_id\"],\n            \"metadata\": self.shared_memory[\"metadata\"],\n            \"topics\": self.shared_memory[\"topics\"],\n            \"qa_pairs\": self.shared_memory[\"qa_pairs\"],\n            \"eli5_content\": self.shared_memory[\"eli5_content\"]\n        }\n        \n        # Generate the HTML content\n        logger.debug(\"Calling generate_html function\")\n        html_content = generate_html(summary_data)\n        \n        # Store the HTML content in shared memory\n        self.shared_memory[\"html_output\"] = html_content\n        logger.info(f\"Generated HTML content ({len(html_content)} characters)\")\n        \n        # Save the HTML to a file if output_path is provided\n        if self.output_path:\n            try:\n                # Create directory if it doesn't exist\n                os.makedirs(os.path.dirname(os.path.abspath(self.output_path)), exist_ok=True)\n                logger.debug(f\"Ensuring output directory exists: {os.path.dirname(os.path.abspath(self.output_path))}\")\n                \n                # Write the HTML content to the file\n                with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html_content)\n                \n                logger.info(f\"HTML summary saved to: {self.output_path}\")\n                \n            except Exception as e:\n                error_msg = f\"Error saving HTML file: {str(e)}\"\n                logger.error(error_msg)\n                self.shared_memory[\"error\"] = error_msg\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in HTML Generation Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"html_output\" not in self.shared_memory:\n            error_msg = \"Failed to generate HTML output\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        html_length = len(self.shared_memory[\"html_output\"])\n        logger.info(f\"Generated HTML summary ({html_length} characters)\")\n        logger.info(\"HTML Generation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    # Initialize shared memory\n    shared_memory = test_data\n    \n    # Create and run the node with a test output path\n    test_output_path = \"test_summary.html\"\n    node = HTMLGenerationNode(shared_memory, test_output_path)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"HTML output length: {len(updated_memory.get('html_output', ''))}\")\n    logger.info(f\"HTML file saved to: {test_output_path}\")\n\n\n--- File Index 9: src/nodes/input_processing_node.py ---\n\"\"\"\nInput Processing Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\nfrom src.utils.logger import logger\n\nclass InputProcessingNode(BaseNode):\n    \"\"\"\n    Node for validating YouTube URL and extracting video metadata.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, youtube_url=None):\n        \"\"\"\n        Initialize the node with shared memory and YouTube URL.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            youtube_url (str): YouTube URL to process\n        \"\"\"\n        super().__init__(shared_memory)\n        if youtube_url:\n            self.shared_memory[\"video_url\"] = youtube_url\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_url exists in shared memory.\n        \"\"\"\n        if \"video_url\" not in self.shared_memory:\n            error_msg = \"YouTube URL not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        logger.info(f\"Processing YouTube URL: {self.shared_memory['video_url']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute URL validation and metadata extraction.\n        \"\"\"\n        # Validate the YouTube URL\n        youtube_url = self.shared_memory[\"video_url\"]\n        is_valid, video_id = validate_youtube_url(youtube_url)\n        \n        if not is_valid:\n            error_msg = f\"Invalid YouTube URL: {youtube_url}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"video_id\"] = video_id\n        logger.info(f\"Valid YouTube video ID: {video_id}\")\n        \n        # Extract video metadata\n        logger.debug(f\"Extracting metadata for video ID: {video_id}\")\n        metadata = extract_youtube_metadata(video_id)\n        \n        if \"error\" in metadata:\n            error_msg = f\"Error extracting metadata: {metadata['error']}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"metadata\"] = metadata\n        logger.info(f\"Extracted metadata for video: {metadata.get('title', 'Unknown Title')}\")\n        logger.debug(f\"Video metadata: {metadata}\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Input Processing Node: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Input Processing Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample YouTube URL\n    test_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": test_url}\n    \n    # Create and run the node\n    node = InputProcessingNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Video ID: {updated_memory.get('video_id', 'Not found')}\")\n    logger.info(f\"Video Title: {updated_memory.get('metadata', {}).get('title', 'Not found')}\")\n    logger.info(f\"Channel: {updated_memory.get('metadata', {}).get('channel_name', 'Not found')}\")\n    logger.info(f\"Duration: {updated_memory.get('metadata', {}).get('duration', 'Not found')}\")\n\n\n--- File Index 10: src/nodes/qa_generation_node.py ---\n\"\"\"\nQ&A Generation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass QAGenerationNode(BaseNode):\n    \"\"\"\n    Node for generating Q&A pairs for each topic in the video.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory)\n        self.questions_per_topic = questions_per_topic\n        self.qa_pairs = {}\n        logger.debug(f\"QAGenerationNode initialized with questions_per_topic={questions_per_topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and transcript exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Q&A Generation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        topics_count = len(self.shared_memory['topics'])\n        logger.info(f\"Generating Q&A pairs for {topics_count} topics\")\n        logger.debug(f\"Topics: {self.shared_memory['topics']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute Q&A generation for each topic.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        topics = self.shared_memory[\"topics\"]\n        transcript = self.shared_memory[\"transcript\"]\n        \n        # Process each topic to generate Q&A pairs\n        for i, topic in enumerate(topics):\n            logger.info(f\"Generating Q&A pairs for topic {i+1}/{len(topics)}: {topic}\")\n            \n            # Create prompt for Q&A generation\n            prompt = textwrap.dedent(f\"\"\"\n            You are an expert at creating educational content for videos.\n            \n            I'll provide you with a transcript from a YouTube video and a specific topic from that video.\n            Your task is to:\n            1. Generate {self.questions_per_topic} insightful questions about this topic\n            2. Provide clear, accurate answers to each question based on the transcript\n            3. Make sure the questions cover different aspects of the topic\n            4. Ensure answers are based only on information in the transcript\n            \n            Topic: {topic}\n            \n            Transcript:\n            {transcript[:4000]}...\n            \n            Respond with ONLY a JSON array of question-answer objects. For example:\n            [\n                {{\n                    \"question\": \"What is the main purpose of X?\",\n                    \"answer\": \"According to the video, the main purpose of X is...\"\n                }},\n                {{\n                    \"question\": \"How does Y relate to Z?\",\n                    \"answer\": \"The video explains that Y and Z are connected through...\"\n                }}\n            ]\n            \"\"\")\n            \n            # Call LLM to generate Q&A pairs\n            try:\n                logger.debug(f\"Calling LLM for topic '{topic}'\")\n                response = call_llm(prompt, temperature=0.7, max_tokens=1000)\n                logger.debug(f\"LLM response for topic '{topic}': {response[:100]}...\")\n                \n                # Clean up the response to extract just the JSON array\n                cleaned_response = response.strip()\n                if cleaned_response.startswith(\"```json\"):\n                    cleaned_response = cleaned_response.split(\"```json\")[1]\n                elif cleaned_response.startswith(\"```\"):\n                    cleaned_response = cleaned_response.split(\"```\")[1]\n                if cleaned_response.endswith(\"```\"):\n                    cleaned_response = cleaned_response.split(\"```\")[0]\n                \n                # Try to parse as JSON, but handle errors gracefully\n                try:\n                    qa_pairs = json.loads(cleaned_response)\n                    if isinstance(qa_pairs, list):\n                        self.qa_pairs[topic] = qa_pairs\n                        logger.info(f\"Generated {len(qa_pairs)} Q&A pairs for topic '{topic}'\")\n                        for j, qa in enumerate(qa_pairs):\n                            logger.debug(f\"  Q{j+1}: {qa.get('question', '')}\")\n                            logger.debug(f\"  A{j+1}: {qa.get('answer', '')[:100]}...\")\n                    else:\n                        logger.warning(f\"Expected list but got {type(qa_pairs)} from LLM for topic '{topic}'\")\n                        self.qa_pairs[topic] = []\n                except json.JSONDecodeError:\n                    logger.warning(f\"Could not parse LLM response as JSON for topic '{topic}': {cleaned_response[:200]}...\")\n                    # Try to extract Q&A pairs from text response\n                    extracted_qa = self._extract_qa_from_text(cleaned_response)\n                    self.qa_pairs[topic] = extracted_qa\n                    logger.info(f\"Extracted {len(extracted_qa)} Q&A pairs from non-JSON response for topic '{topic}'\")\n            except Exception as e:\n                logger.error(f\"Error calling LLM for topic '{topic}': {str(e)}\")\n                self.qa_pairs[topic] = []\n    \n    def _extract_qa_from_text(self, text):\n        \"\"\"\n        Attempt to extract Q&A pairs from non-JSON text.\n        \n        Args:\n            text (str): Text containing Q&A pairs\n            \n        Returns:\n            list: List of Q&A pair dictionaries\n        \"\"\"\n        logger.debug(\"Attempting to extract Q&A pairs from non-JSON text\")\n        lines = text.split(\"\\n\")\n        qa_pairs = []\n        current_question = None\n        current_answer = \"\"\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line starts with Q: or Question:\n            if line.startswith((\"Q:\", \"Question:\")):\n                # If we have a previous question, save it\n                if current_question:\n                    qa_pairs.append({\n                        \"question\": current_question,\n                        \"answer\": current_answer.strip()\n                    })\n                \n                # Extract new question\n                current_question = line.split(\":\", 1)[1].strip()\n                current_answer = \"\"\n            # Check if line starts with A: or Answer:\n            elif line.startswith((\"A:\", \"Answer:\")) and current_question:\n                current_answer += line.split(\":\", 1)[1].strip() + \" \"\n            # Otherwise, add to current answer if we have a question\n            elif current_question:\n                current_answer += line + \" \"\n        \n        # Add the last Q&A pair if exists\n        if current_question:\n            qa_pairs.append({\n                \"question\": current_question,\n                \"answer\": current_answer.strip()\n            })\n            \n        logger.debug(f\"Extracted {len(qa_pairs)} Q&A pairs from text\")\n        return qa_pairs\n    \n    def post(self):\n        \"\"\"\n        Post-process and store the generated Q&A pairs in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Q&A Generation Node: {self.shared_memory['error']}\")\n            return\n        \n        # Store the Q&A pairs in shared memory\n        self.shared_memory[\"qa_pairs\"] = self.qa_pairs\n        \n        # Print a summary of the generated Q&A pairs\n        total_qa_pairs = sum(len(pairs) for pairs in self.qa_pairs.values())\n        logger.info(f\"Generated a total of {total_qa_pairs} Q&A pairs across {len(self.qa_pairs)} topics\")\n        \n        for topic, pairs in self.qa_pairs.items():\n            logger.info(f\"  Topic '{topic}': {len(pairs)} Q&A pairs\")\n        \n        logger.info(\"Q&A Generation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and transcript\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\", \"Applications of AI\"]\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \n    Now let's discuss some applications of machine learning. In healthcare, machine learning \n    is used for disease diagnosis, drug discovery, and personalized medicine. In finance, \n    it's used for fraud detection, algorithmic trading, and credit scoring. In transportation, \n    self-driving cars rely heavily on machine learning algorithms.\n    \n    Ethical considerations in machine learning include bias in training data, model interpretability, \n    and privacy concerns. It's important to develop responsible AI systems that are fair and transparent.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"transcript\": test_transcript\n    }\n    \n    # Create and run the node\n    node = QAGenerationNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    qa_pairs = updated_memory.get(\"qa_pairs\", {})\n    for topic, pairs in qa_pairs.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        for i, qa in enumerate(pairs):\n            logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n            logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n\n\n--- File Index 11: src/nodes/topic_extraction_node.py ---\n\"\"\"\nTopic Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport concurrent.futures\nfrom typing import List, Dict, Any\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicExtractionNode(BaseNode):\n    \"\"\"\n    Node for identifying main topics from the video transcript.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, chunk_size=0, overlap=0, max_topics=5):\n        \"\"\"\n        Initialize the node with shared memory and processing parameters.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            chunk_size (int): Size of transcript chunks (0 = no chunking)\n            overlap (int): Overlap between chunks (when chunk_size > 0)\n            max_topics (int): Maximum number of topics to extract\n        \"\"\"\n        super().__init__(shared_memory)\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_topics = max_topics\n        self.chunks = []\n        self.chunk_topics = []\n        logger.debug(f\"TopicExtractionNode initialized with chunk_size={chunk_size}, overlap={overlap}, max_topics={max_topics}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if transcript exists in shared memory\n        and splitting it into chunks if chunk_size > 0.\n        \"\"\"\n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        transcript = self.shared_memory[\"transcript\"]\n        logger.debug(f\"Preparing to extract topics from transcript ({len(transcript)} characters)\")\n        \n        # Only chunk if chunk_size is specified\n        if self.chunk_size > 0:\n            # Split into overlapping chunks\n            start = 0\n            while start < len(transcript):\n                end = min(start + self.chunk_size, len(transcript))\n                self.chunks.append(transcript[start:end])\n                start = end - self.overlap\n            logger.info(f\"Split transcript into {len(self.chunks)} chunks for processing\")\n            logger.debug(f\"Chunk sizes: {[len(chunk) for chunk in self.chunks]}\")\n        else:\n            # Process as a single chunk\n            self.chunks = [transcript]\n            logger.info(\"Processing transcript as a single chunk\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic extraction for each chunk and combine results.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Use ThreadPoolExecutor for parallel processing of chunks\n        max_workers = min(len(self.chunks), 4)  # Limit concurrent API calls\n        logger.info(f\"Processing {len(self.chunks)} chunks with {max_workers} parallel workers\")\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all chunk processing tasks\n            future_to_chunk = {\n                executor.submit(self._process_chunk, i, chunk): i \n                for i, chunk in enumerate(self.chunks)\n            }\n            \n            # Collect results as they complete\n            for future in concurrent.futures.as_completed(future_to_chunk):\n                chunk_index = future_to_chunk[future]\n                try:\n                    topics = future.result()\n                    if topics:\n                        self.chunk_topics.append(topics)\n                        logger.info(f\"Completed processing for chunk {chunk_index+1}/{len(self.chunks)}\")\n                except Exception as e:\n                    logger.error(f\"Error processing chunk {chunk_index+1}: {str(e)}\")\n                    \n        # If we couldn't extract any topics, add some default ones\n        if not self.chunk_topics:\n            logger.warning(\"No topics extracted, using default topics\")\n            self.chunk_topics.append([\"Main Content\", \"Key Points\", \"Summary\"])\n    \n    def _process_chunk(self, chunk_index: int, chunk: str) -> List[str]:\n        \"\"\"\n        Process a single chunk to extract topics.\n        \n        Args:\n            chunk_index (int): Index of the chunk\n            chunk (str): Content of the chunk\n            \n        Returns:\n            List[str]: Extracted topics\n        \"\"\"\n        logger.info(f\"Processing chunk {chunk_index+1}/{len(self.chunks)}...\")\n        \n        # Create prompt for topic extraction\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content and identifying main topics.\n        \n        I'll provide you with a transcript from a YouTube video. Your task is to:\n        1. Identify the main topics discussed in this segment\n        2. List each topic as a short, clear phrase (3-7 words)\n        3. Provide at most {self.max_topics} topics\n        4. Focus on substantive content, not introductions or conclusions\n        \n        Here is the transcript segment:\n        \n        {chunk[:2000]}...\n        \n        Respond with ONLY a JSON array of topic strings. For example:\n        [\"Topic One\", \"Topic Two\", \"Topic Three\"]\n        \"\"\")\n        \n        # Call LLM to extract topics\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1} (timeout: 30s)...\")\n            response = call_llm(prompt, temperature=0.3, max_tokens=200, timeout=30)\n            \n            # Check if we got an error response\n            if response.startswith(\"Error:\"):\n                logger.warning(f\"LLM API error for chunk {chunk_index+1}: {response}\")\n                # Add some default topics if we can't get them from the LLM\n                if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                    logger.info(\"Using default topics due to LLM error\")\n                    return [\"Main Content\", \"Key Points\", \"Summary\"]\n                return []\n            \n            logger.info(f\"Received LLM response for chunk {chunk_index+1} ({len(response)} characters)\")\n            logger.debug(f\"LLM response preview: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                import json\n                topics = json.loads(cleaned_response)\n                if isinstance(topics, list):\n                    logger.info(f\"Extracted {len(topics)} topics from chunk {chunk_index+1}\")\n                    logger.debug(f\"Topics from chunk {chunk_index+1}: {topics}\")\n                    return topics\n                else:\n                    logger.warning(f\"Expected list but got {type(topics)} from LLM\")\n                    # Try to extract topics from text response\n                    return [cleaned_response]\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response}\")\n                # Try to extract topics from text response\n                lines = cleaned_response.split(\"\\n\")\n                potential_topics = [line.strip().strip('\",[]') for line in lines if line.strip()]\n                logger.info(f\"Extracted {len(potential_topics)} topics from non-JSON response\")\n                return potential_topics\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # Add some default topics if we can't get them from the LLM\n            if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                logger.info(\"Using default topics due to exception\")\n                return [\"Main Content\", \"Key Points\", \"Summary\"]\n            return []\n    \n    def post(self):\n        \"\"\"\n        Post-process by combining topics from all chunks and selecting the most relevant ones.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.chunk_topics:\n            error_msg = \"Failed to extract topics from transcript\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        # Flatten the list of topics from all chunks\n        all_topics = []\n        for topics in self.chunk_topics:\n            all_topics.extend(topics)\n        \n        logger.debug(f\"All extracted topics (before deduplication): {all_topics}\")\n        \n        # Count topic occurrences to find the most common ones\n        topic_counts = {}\n        for topic in all_topics:\n            topic_lower = topic.lower()\n            topic_counts[topic_lower] = topic_counts.get(topic_lower, 0) + 1\n        \n        logger.debug(f\"Topic frequency counts: {topic_counts}\")\n        \n        # Sort topics by frequency\n        sorted_topics = sorted(\n            [(count, topic) for topic, count in topic_counts.items()], \n            reverse=True\n        )\n        \n        # Select the top topics (using the original casing)\n        top_topics = []\n        seen_topics = set()\n        \n        for _, topic_lower in sorted_topics:\n            # Find the original casing version\n            original_casing = next(\n                (t for t in all_topics if t.lower() == topic_lower), \n                topic_lower.title()\n            )\n            \n            # Skip if we've already added this topic\n            if topic_lower in seen_topics:\n                continue\n                \n            top_topics.append(original_casing)\n            seen_topics.add(topic_lower)\n            \n            # Limit to max_topics\n            if len(top_topics) >= self.max_topics:\n                break\n        \n        # Store the final list of topics\n        self.shared_memory[\"topics\"] = top_topics\n        \n        logger.info(f\"Extracted {len(top_topics)} final topics:\")\n        for i, topic in enumerate(top_topics):\n            logger.info(f\"  {i+1}. {topic}\")\n        \n        logger.info(\"Topic Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with a sample transcript\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \n    Now let's discuss some applications of machine learning. In healthcare, machine learning \n    is used for disease diagnosis, drug discovery, and personalized medicine. In finance, \n    it's used for fraud detection, algorithmic trading, and credit scoring. In transportation, \n    self-driving cars rely heavily on machine learning algorithms.\n    \n    Ethical considerations in machine learning include bias in training data, model interpretability, \n    and privacy concerns. It's important to develop responsible AI systems that are fair and transparent.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\"transcript\": test_transcript}\n    \n    # Create and run the node\n    node = TopicExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Topics: {updated_memory.get('topics', [])}\")\n\n\n--- File Index 12: src/nodes/topic_orchestrator_node.py ---\n\"\"\"\nTopic Processing Orchestrator Node for YouTube Video Summarizer.\nImplements a Map-Reduce approach for parallel topic processing.\n\"\"\"\nimport sys\nimport os\nfrom typing import List, Dict, Any\nimport concurrent.futures\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.nodes.topic_processor_node import TopicProcessorNode\nfrom src.utils.logger import logger\n\nclass TopicOrchestratorNode(BaseNode):\n    \"\"\"\n    Node for orchestrating parallel processing of topics using Map-Reduce pattern.\n    Maps topics to individual processors and reduces the results.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, max_workers=3, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            max_workers (int): Maximum number of parallel workers for topic processing\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory)\n        self.max_workers = max_workers\n        self.questions_per_topic = questions_per_topic\n        self.topics = []\n        self.transcript = \"\"\n        self.topic_results = {}\n        logger.debug(f\"TopicOrchestratorNode initialized with max_workers={max_workers}, questions_per_topic={questions_per_topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and transcript exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Orchestration due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        \n        topics_count = len(self.topics)\n        # Adjust max_workers if there are fewer topics than workers\n        self.max_workers = min(self.max_workers, topics_count)\n        logger.info(f\"Preparing to process {topics_count} topics with {self.max_workers} parallel workers\")\n        logger.debug(f\"Topics to process: {self.topics}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute Map-Reduce processing for topics.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n        \n        # Map phase: Process all topics in parallel\n        self._map_phase()\n        \n        # Reduce phase: Combine results\n        self._reduce_phase()\n    \n    def _map_phase(self):\n        \"\"\"\n        Map phase: Process all topics in parallel using a thread pool.\n        \"\"\"\n        logger.info(f\"Starting Map phase with {self.max_workers} workers for {len(self.topics)} topics\")\n        \n        # Use ThreadPoolExecutor for parallel processing\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all topics for processing\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic \n                for topic in self.topics\n            }\n            \n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    result = future.result()\n                    self.topic_results[topic] = result\n                    logger.info(f\"Completed processing for topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {str(e)}\")\n                    # Create an empty result for failed topics\n                    self.topic_results[topic] = {\n                        \"topic\": topic,\n                        \"qa_pairs\": [],\n                        \"eli5_content\": f\"Error processing topic: {str(e)}\"\n                    }\n        \n        logger.info(f\"Map phase complete: Processed {len(self.topic_results)} topics\")\n    \n    def _process_topic(self, topic):\n        \"\"\"\n        Process a single topic using a TopicProcessorNode.\n        \n        Args:\n            topic (str): The topic to process\n            \n        Returns:\n            dict: The processing result\n        \"\"\"\n        logger.info(f\"Processing topic: {topic}\")\n        \n        # Create and run a TopicProcessorNode for this topic\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic\n        )\n        \n        return processor.run()[\"topic_results\"][topic]\n    \n    def _reduce_phase(self):\n        \"\"\"\n        Reduce phase: Combine results from all topic processors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        logger.info(f\"Starting Reduce phase with {len(self.topic_results)} topic results\")\n        \n        # Initialize the combined results\n        qa_pairs = {}\n        eli5_content = {}\n        \n        # Combine results from all topics\n        for topic, result in self.topic_results.items():\n            # Add Q&A pairs\n            qa_pairs[topic] = result.get(\"qa_pairs\", [])\n            \n            # Add ELI5 content\n            eli5_content[topic] = result.get(\"eli5_content\", \"\")\n        \n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        \n        # Log summary of combined results\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        logger.info(f\"Reduce phase complete: Combined {total_qa_pairs} Q&A pairs across {len(qa_pairs)} topics\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Orchestrator Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.topic_results:\n            error_msg = \"No topic results were generated\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Topic Orchestrator Node completed successfully\")\n        logger.info(f\"Processed {len(self.topic_results)} topics using Map-Reduce pattern\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and transcript\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"transcript\": test_transcript\n    }\n    \n    # Create and run the node\n    node = TopicOrchestratorNode(shared_memory, max_workers=2)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    \n    # Print Q&A pairs\n    qa_pairs = updated_memory.get(\"qa_pairs\", {})\n    for topic, pairs in qa_pairs.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(f\"Q&A Pairs: {len(pairs)}\")\n        for i, qa in enumerate(pairs):\n            logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n            logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    \n    # Print ELI5 content\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic ELI5: {topic}\")\n        logger.info(explanation[:200] + \"...\" if len(explanation) > 200 else explanation)\n\n\n--- File Index 13: src/nodes/topic_processor_node.py ---\n\"\"\"\nTopic Processor Node for YouTube Video Summarizer.\nProcesses a single topic with Q&A generation and ELI5 transformation.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicProcessorNode(BaseNode):\n    \"\"\"\n    Node for processing a single topic, including Q&A generation and ELI5 transformation.\n    This node is designed to be used as part of a Map-Reduce pattern.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, topic=None, transcript=None, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with topic and transcript.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary (optional)\n            topic (str): The topic to process\n            transcript (str): The video transcript\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory or {})\n        self.topic = topic\n        self.transcript = transcript\n        self.questions_per_topic = questions_per_topic\n        self.result = {\n            \"topic\": topic,\n            \"qa_pairs\": [],\n            \"eli5_content\": \"\"\n        }\n        logger.debug(f\"TopicProcessorNode initialized for topic: {topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topic and transcript are available.\n        \"\"\"\n        # If topic and transcript were not provided in constructor, try to get from shared memory\n        if self.topic is None and \"current_topic\" in self.shared_memory:\n            self.topic = self.shared_memory[\"current_topic\"]\n            \n        if self.transcript is None and \"transcript\" in self.shared_memory:\n            self.transcript = self.shared_memory[\"transcript\"]\n            \n        if self.topic is None:\n            error_msg = \"No topic provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        if self.transcript is None:\n            error_msg = \"No transcript provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        logger.info(f\"Processing topic: {self.topic}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic processing: generate Q&A pairs and ELI5 explanation.\n        \"\"\"\n        # Step 1: Generate Q&A pairs\n        self._generate_qa_pairs()\n        \n        # Step 2: Create ELI5 explanation\n        self._create_eli5_explanation()\n    \n    def _generate_qa_pairs(self):\n        \"\"\"\n        Generate Q&A pairs for the topic.\n        \"\"\"\n        logger.info(f\"Generating Q&A pairs for topic: {self.topic}\")\n        \n        # Create prompt for Q&A generation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at creating educational content for videos.\n        \n        I'll provide you with a transcript from a YouTube video and a specific topic from that video.\n        Your task is to:\n        1. Generate {self.questions_per_topic} insightful questions about this topic\n        2. Provide clear, accurate answers to each question based on the transcript\n        3. Make sure the questions cover different aspects of the topic\n        4. Ensure answers are based only on information in the transcript\n        \n        Topic: {self.topic}\n        \n        Transcript:\n        {self.transcript[:4000]}...\n        \n        Respond with ONLY a JSON array of question-answer objects. For example:\n        [\n            {{\n                \"question\": \"What is the main purpose of X?\",\n                \"answer\": \"According to the video, the main purpose of X is...\"\n            }},\n            {{\n                \"question\": \"How does Y relate to Z?\",\n                \"answer\": \"The video explains that Y and Z are connected through...\"\n            }}\n        ]\n        \"\"\")\n        \n        # Call LLM to generate Q&A pairs\n        try:\n            logger.debug(f\"Calling LLM for Q&A generation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=1000)\n            logger.debug(f\"LLM response for Q&A generation: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            elif cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                qa_pairs = json.loads(cleaned_response)\n                if isinstance(qa_pairs, list):\n                    self.result[\"qa_pairs\"] = qa_pairs\n                    logger.info(f\"Generated {len(qa_pairs)} Q&A pairs for topic '{self.topic}'\")\n                    for i, qa in enumerate(qa_pairs):\n                        logger.debug(f\"  Q{i+1}: {qa.get('question', '')}\")\n                        logger.debug(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n                else:\n                    logger.warning(f\"Expected list but got {type(qa_pairs)} from LLM\")\n                    self.result[\"qa_pairs\"] = []\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response[:200]}...\")\n                # Try to extract Q&A pairs from text response\n                extracted_qa = self._extract_qa_from_text(cleaned_response)\n                self.result[\"qa_pairs\"] = extracted_qa\n                logger.info(f\"Extracted {len(extracted_qa)} Q&A pairs from non-JSON response\")\n        except Exception as e:\n            logger.error(f\"Error generating Q&A pairs for topic '{self.topic}': {str(e)}\")\n            self.result[\"qa_pairs\"] = []\n    \n    def _extract_qa_from_text(self, text):\n        \"\"\"\n        Attempt to extract Q&A pairs from non-JSON text.\n        \n        Args:\n            text (str): Text containing Q&A pairs\n            \n        Returns:\n            list: List of Q&A pair dictionaries\n        \"\"\"\n        logger.debug(\"Attempting to extract Q&A pairs from non-JSON text\")\n        lines = text.split(\"\\n\")\n        qa_pairs = []\n        current_question = None\n        current_answer = \"\"\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line starts with Q: or Question:\n            if line.startswith((\"Q:\", \"Question:\")):\n                # If we have a previous question, save it\n                if current_question:\n                    qa_pairs.append({\n                        \"question\": current_question,\n                        \"answer\": current_answer.strip()\n                    })\n                \n                # Extract new question\n                current_question = line.split(\":\", 1)[1].strip()\n                current_answer = \"\"\n            # Check if line starts with A: or Answer:\n            elif line.startswith((\"A:\", \"Answer:\")) and current_question:\n                current_answer += line.split(\":\", 1)[1].strip() + \" \"\n            # Otherwise, add to current answer if we have a question\n            elif current_question:\n                current_answer += line + \" \"\n        \n        # Add the last Q&A pair if exists\n        if current_question:\n            qa_pairs.append({\n                \"question\": current_question,\n                \"answer\": current_answer.strip()\n            })\n            \n        logger.debug(f\"Extracted {len(qa_pairs)} Q&A pairs from text\")\n        return qa_pairs\n    \n    def _create_eli5_explanation(self):\n        \"\"\"\n        Create an ELI5 (Explain Like I'm 5) explanation for the topic.\n        \"\"\"\n        logger.info(f\"Creating ELI5 explanation for topic: {self.topic}\")\n        \n        # Get Q&A pairs for context\n        qa_text = \"\"\n        for qa in self.result[\"qa_pairs\"]:\n            question = qa.get(\"question\", \"\")\n            answer = qa.get(\"answer\", \"\")\n            qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n        \n        # Create prompt for ELI5 transformation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at explaining complex topics to young children (5-7 years old).\n        \n        I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n        Your task is to:\n        1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n        2. Use simple words, short sentences, and concrete examples\n        3. Avoid jargon and technical terms\n        4. Use analogies to familiar concepts when possible\n        5. Keep the explanation under 200 words\n        6. Maintain the core information while simplifying the language\n        \n        Topic: {self.topic}\n        \n        Q&A Context:\n        {qa_text}\n        \n        Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n        \"\"\")\n        \n        # Call LLM to generate ELI5 explanation\n        try:\n            logger.debug(f\"Calling LLM for ELI5 transformation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=500)\n            logger.debug(f\"LLM response for ELI5 transformation: {response[:100]}...\")\n            \n            # Clean up the response\n            explanation = response.strip()\n            \n            # Store the ELI5 explanation\n            self.result[\"eli5_content\"] = explanation\n            logger.info(f\"Generated ELI5 explanation for topic '{self.topic}' ({len(explanation)} characters)\")\n            \n        except Exception as e:\n            error_msg = f\"Error creating ELI5 explanation for topic '{self.topic}': {str(e)}\"\n            logger.error(error_msg)\n            self.result[\"eli5_content\"] = f\"Sorry, I couldn't create a simple explanation for {self.topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and return the results.\n        \"\"\"\n        # Store the results in shared memory if needed\n        if \"topic_results\" not in self.shared_memory:\n            self.shared_memory[\"topic_results\"] = {}\n            \n        self.shared_memory[\"topic_results\"][self.topic] = self.result\n        \n        logger.info(f\"Topic processing completed for '{self.topic}'\")\n        logger.debug(f\"Generated {len(self.result['qa_pairs'])} Q&A pairs and {len(self.result['eli5_content'])} characters of ELI5 content\")\n        \n        return self.result\n\n\nif __name__ == \"__main__\":\n    # Test with a sample topic and transcript\n    test_topic = \"Machine Learning Basics\"\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Create and run the node\n    node = TopicProcessorNode(topic=test_topic, transcript=test_transcript)\n    result = node.run()\n    \n    # Print the results\n    logger.info(\"\\nTopic Processing Results:\")\n    logger.info(f\"Topic: {result['topic']}\")\n    logger.info(f\"Q&A Pairs: {len(result['qa_pairs'])}\")\n    for i, qa in enumerate(result['qa_pairs']):\n        logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n        logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    logger.info(f\"ELI5 Explanation: {result['eli5_content']}\")\n\n\n--- File Index 14: src/utils/call_llm.py ---\n\"\"\"\nUtility function to call a Large Language Model (LLM) API.\n\"\"\"\nimport os\nimport time\nfrom openai import OpenAI\nfrom openai import OpenAIError\nimport logging\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=1000, timeout=60):\n    \"\"\"\n    Calls an LLM API with the given prompt and returns the response.\n    \n    Args:\n        prompt (str): The prompt to send to the LLM\n        model (str): The model to use (default: gpt-4o)\n        temperature (float): Controls randomness (0.0-1.0)\n        max_tokens (int): Maximum number of tokens to generate\n        timeout (int): Maximum time to wait for a response in seconds\n        \n    Returns:\n        str: The LLM's response\n    \"\"\"\n    # Get API key from environment variable\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    \n    if not api_key:\n        logging.error(\"OpenAI API key not found in environment variables\")\n        return \"Error: OpenAI API key not found in environment variables.\"\n    \n    # Log API key first few and last few characters for debugging\n    masked_key = f\"{api_key[:5]}...{api_key[-5:]}\"\n    logging.debug(f\"Using API key: {masked_key}\")\n    \n    # Initialize the OpenAI client\n    try:\n        client = OpenAI(\n            api_key=api_key\n        )\n        \n        # Make the API call with proper timeout handling\n        start_time = time.time()\n        logging.debug(f\"Starting OpenAI API call to model {model}\")\n        \n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_tokens=max_tokens,\n                timeout=timeout\n            )\n            \n            # Calculate and log response time\n            elapsed = time.time() - start_time\n            logging.debug(f\"OpenAI API call completed in {elapsed:.2f} seconds\")\n            \n            # Extract and return the response content\n            content = response.choices[0].message.content\n            logging.debug(f\"Received response of length {len(content)} characters\")\n            return content\n            \n        except Exception as api_error:\n            elapsed = time.time() - start_time\n            logging.error(f\"OpenAI API error after {elapsed:.2f} seconds: {str(api_error)}\")\n            \n            # Handle specific error types\n            error_message = str(api_error).lower()\n            if \"timeout\" in error_message:\n                return f\"Error: LLM API call timed out after {elapsed:.1f} seconds.\"\n            elif \"rate limit\" in error_message:\n                return \"Error: Rate limit exceeded. Please try again later.\"\n            elif \"invalid auth\" in error_message or \"authentication\" in error_message:\n                return \"Error: Authentication failed. Please check your API key.\"\n            else:\n                return f\"Error calling LLM API: {str(api_error)}\"\n                \n    except Exception as e:\n        logging.exception(\"Unexpected error initializing OpenAI client\")\n        return f\"Error initializing OpenAI client: {str(e)}\"\n\n\nif __name__ == \"__main__\":\n    # Configure logging for testing\n    logging.basicConfig(level=logging.DEBUG)\n    \n    # Test the function with a simple prompt\n    test_prompt = \"Explain what a YouTube video summarizer does in one sentence.\"\n    \n    print(\"Testing call_llm function...\")\n    response = call_llm(test_prompt, max_tokens=100)\n    print(f\"Prompt: {test_prompt}\")\n    print(f\"Response: {response}\")\n\n\n--- File Index 15: src/utils/extract_youtube_metadata.py ---\n\"\"\"\nUtility function to extract metadata from YouTube videos.\n\"\"\"\nimport requests\nimport json\nimport os\nfrom datetime import datetime\n\ndef extract_youtube_metadata(video_id):\n    \"\"\"\n    Extracts metadata from a YouTube video using the YouTube Data API.\n    \n    Args:\n        video_id (str): The YouTube video ID\n        \n    Returns:\n        dict: A dictionary containing video metadata (title, channel, duration, etc.)\n    \"\"\"\n    # In a production environment, you would use an API key from environment variables\n    # For this example, we'll use a placeholder\n    # You would need to get your own API key from the Google Developer Console\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    \n    # Base URL for YouTube Data API v3\n    base_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    \n    # Parameters for the API request\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\",\n        \"id\": video_id,\n        \"key\": api_key\n    }\n    \n    try:\n        # Make the API request\n        response = requests.get(base_url, params=params)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Check if the video exists\n        if not data.get(\"items\"):\n            return {\n                \"error\": \"Video not found or API key invalid\",\n                \"video_id\": video_id\n            }\n        \n        # Extract relevant metadata\n        video_data = data[\"items\"][0]\n        snippet = video_data.get(\"snippet\", {})\n        content_details = video_data.get(\"contentDetails\", {})\n        statistics = video_data.get(\"statistics\", {})\n        \n        # Format the duration (ISO 8601 format to human-readable)\n        duration_iso = content_details.get(\"duration\", \"PT0S\")\n        # Simple conversion for common formats (not handling all ISO 8601 cases)\n        hours = 0\n        minutes = 0\n        seconds = 0\n        \n        if \"H\" in duration_iso:\n            hours = int(duration_iso.split(\"H\")[0].split(\"PT\")[1])\n            duration_iso = duration_iso.split(\"H\")[1]\n        elif \"PT\" in duration_iso:\n            duration_iso = duration_iso.split(\"PT\")[1]\n            \n        if \"M\" in duration_iso:\n            minutes = int(duration_iso.split(\"M\")[0])\n            duration_iso = duration_iso.split(\"M\")[1]\n            \n        if \"S\" in duration_iso:\n            seconds = int(duration_iso.split(\"S\")[0])\n        \n        duration_str = \"\"\n        if hours > 0:\n            duration_str += f\"{hours} hour{'s' if hours > 1 else ''} \"\n        if minutes > 0:\n            duration_str += f\"{minutes} minute{'s' if minutes > 1 else ''} \"\n        if seconds > 0 or (hours == 0 and minutes == 0):\n            duration_str += f\"{seconds} second{'s' if seconds > 1 else ''}\"\n        \n        # Format the published date\n        published_at = snippet.get(\"publishedAt\", \"\")\n        if published_at:\n            try:\n                published_date = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n                published_at = published_date.strftime(\"%B %d, %Y\")\n            except ValueError:\n                pass\n        \n        # Construct the metadata dictionary\n        metadata = {\n            \"video_id\": video_id,\n            \"title\": snippet.get(\"title\", \"Unknown Title\"),\n            \"description\": snippet.get(\"description\", \"\"),\n            \"channel_name\": snippet.get(\"channelTitle\", \"Unknown Channel\"),\n            \"channel_id\": snippet.get(\"channelId\", \"\"),\n            \"published_at\": published_at,\n            \"duration\": duration_str,\n            \"duration_seconds\": hours * 3600 + minutes * 60 + seconds,\n            \"view_count\": int(statistics.get(\"viewCount\", 0)),\n            \"like_count\": int(statistics.get(\"likeCount\", 0)),\n            \"comment_count\": int(statistics.get(\"commentCount\", 0)),\n            \"thumbnail_url\": snippet.get(\"thumbnails\", {}).get(\"high\", {}).get(\"url\", \"\"),\n            \"tags\": snippet.get(\"tags\", []),\n            \"category_id\": snippet.get(\"categoryId\", \"\"),\n        }\n        \n        return metadata\n        \n    except requests.exceptions.RequestException as e:\n        # Handle request errors\n        return {\n            \"error\": f\"API request error: {str(e)}\",\n            \"video_id\": video_id\n        }\n    except (KeyError, ValueError, json.JSONDecodeError) as e:\n        # Handle parsing errors\n        return {\n            \"error\": f\"Data parsing error: {str(e)}\",\n            \"video_id\": video_id\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the function with an example video ID\n    test_video_id = \"dQw4w9WgXcQ\"  # Rick Astley - Never Gonna Give You Up\n    metadata = extract_youtube_metadata(test_video_id)\n    \n    # Print the metadata in a readable format\n    print(json.dumps(metadata, indent=2))\n\n\n--- File Index 16: src/utils/extract_youtube_transcript.py ---\n\"\"\"\nUtility function to extract transcript from YouTube videos.\n\"\"\"\nimport os\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    \"\"\"\n    Extracts the transcript/captions from a YouTube video.\n    \n    Args:\n        video_id (str): The YouTube video ID\n        \n    Returns:\n        str: The full transcript text or an error message\n    \"\"\"\n    try:\n        # Get available transcripts\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        \n        # Try to get English transcript first, fall back to other languages if needed\n        try:\n            transcript = transcript_list.find_transcript(['en'])\n        except NoTranscriptFound:\n            # If no English transcript is found, use the first available one\n            transcript = transcript_list.find_transcript([])\n            \n            # If it's not in English, try to translate it\n            if transcript.language_code != 'en':\n                transcript = transcript.translate('en')\n        \n        # Fetch the transcript data\n        transcript_data = transcript.fetch()\n        \n        # Combine all transcript segments into a single text\n        full_transcript = \"\"\n        for segment in transcript_data:\n            # The segment is a dictionary with 'text', 'start', and 'duration' keys\n            if isinstance(segment, dict) and 'text' in segment:\n                text = segment['text']\n            else:\n                # Handle FetchedTranscriptSnippet objects\n                text = segment.text if hasattr(segment, 'text') else str(segment)\n                \n            # Add a space if the text doesn't end with punctuation\n            if text and text[-1] not in '.!?':\n                text += ' '\n            full_transcript += text\n        \n        return full_transcript.strip()\n        \n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n\n\nif __name__ == \"__main__\":\n    # Test the function with an example video ID\n    test_video_id = \"dQw4w9WgXcQ\"  # Rick Astley - Never Gonna Give You Up\n    transcript = extract_youtube_transcript(test_video_id)\n    \n    # Print the first 500 characters of the transcript\n    print(transcript[:500] + \"...\" if len(transcript) > 500 else transcript)\n\n\n--- File Index 17: src/utils/generate_html.py ---\n\"\"\"\nUtility function to generate HTML output for the YouTube video summary.\n\"\"\"\nimport html\n\ndef generate_html(summary_data):\n    \"\"\"\n    Generates an HTML page to visualize the YouTube video summary.\n    \n    Args:\n        summary_data (dict): A dictionary containing all the processed information\n            - video_id (str): YouTube video ID\n            - metadata (dict): Video metadata\n            - topics (list): List of extracted topics\n            - qa_pairs (dict): Q&A pairs organized by topic\n            - eli5_content (dict): Child-friendly explanations\n            \n    Returns:\n        str: HTML content as a string\n    \"\"\"\n    if not summary_data:\n        return \"<html><body><h1>Error: No summary data provided</h1></body></html>\"\n    \n    video_id = summary_data.get(\"video_id\", \"\")\n    metadata = summary_data.get(\"metadata\", {})\n    topics = summary_data.get(\"topics\", [])\n    qa_pairs = summary_data.get(\"qa_pairs\", {})\n    eli5_content = summary_data.get(\"eli5_content\", {})\n    \n    # Escape HTML special characters to prevent XSS\n    title = html.escape(metadata.get(\"title\", \"YouTube Video Summary\"))\n    channel = html.escape(metadata.get(\"channel_name\", \"Unknown Channel\"))\n    duration = html.escape(metadata.get(\"duration\", \"\"))\n    published_at = html.escape(metadata.get(\"published_at\", \"\"))\n    thumbnail_url = metadata.get(\"thumbnail_url\", \"\")\n    \n    # Generate the HTML content\n    html_content = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{title} - Summary</title>\n    <style>\n        :root {{\n            --primary-color: #ff5252;\n            --secondary-color: #3f51b5;\n            --background-color: #f9f9f9;\n            --card-color: #ffffff;\n            --text-color: #333333;\n            --border-radius: 10px;\n            --box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n        }}\n        \n        body {{\n            font-family: 'Comic Sans MS', 'Chalkboard SE', 'Arial', sans-serif;\n            background-color: var(--background-color);\n            color: var(--text-color);\n            line-height: 1.6;\n            margin: 0;\n            padding: 0;\n        }}\n        \n        .container {{\n            max-width: 1000px;\n            margin: 0 auto;\n            padding: 20px;\n        }}\n        \n        header {{\n            text-align: center;\n            margin-bottom: 30px;\n        }}\n        \n        h1 {{\n            color: var(--primary-color);\n            font-size: 2.5rem;\n            margin-bottom: 10px;\n        }}\n        \n        h2 {{\n            color: var(--secondary-color);\n            font-size: 1.8rem;\n            border-bottom: 2px solid var(--secondary-color);\n            padding-bottom: 10px;\n            margin-top: 30px;\n        }}\n        \n        h3 {{\n            color: var(--primary-color);\n            font-size: 1.5rem;\n        }}\n        \n        .video-info {{\n            display: flex;\n            align-items: center;\n            background-color: var(--card-color);\n            border-radius: var(--border-radius);\n            box-shadow: var(--box-shadow);\n            padding: 20px;\n            margin-bottom: 30px;\n        }}\n        \n        .video-thumbnail {{\n            flex: 0 0 280px;\n            margin-right: 20px;\n        }}\n        \n        .video-thumbnail img {{\n            width: 100%;\n            border-radius: var(--border-radius);\n        }}\n        \n        .video-details {{\n            flex: 1;\n        }}\n        \n        .topic-card {{\n            background-color: var(--card-color);\n            border-radius: var(--border-radius);\n            box-shadow: var(--box-shadow);\n            padding: 20px;\n            margin-bottom: 20px;\n        }}\n        \n        .qa-section {{\n            background-color: rgba(63, 81, 181, 0.1);\n            border-radius: var(--border-radius);\n            padding: 15px;\n            margin-top: 15px;\n        }}\n        \n        .question {{\n            font-weight: bold;\n            color: var(--secondary-color);\n            margin-bottom: 5px;\n        }}\n        \n        .answer {{\n            margin-bottom: 15px;\n            padding-left: 15px;\n            border-left: 3px solid var(--primary-color);\n        }}\n        \n        footer {{\n            text-align: center;\n            margin-top: 50px;\n            padding: 20px;\n            font-size: 0.9rem;\n            color: #666;\n        }}\n        \n        @media (max-width: 768px) {{\n            .video-info {{\n                flex-direction: column;\n            }}\n            \n            .video-thumbnail {{\n                margin-right: 0;\n                margin-bottom: 20px;\n                width: 100%;\n            }}\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <header>\n            <h1>\ud83c\udfac Kid-Friendly Video Summary \ud83c\udfac</h1>\n            <p>Here's a simple explanation of what this video is all about!</p>\n        </header>\n        \n        <div class=\"video-info\">\n            <div class=\"video-thumbnail\">\n                <img src=\"{thumbnail_url}\" alt=\"{title}\" onerror=\"this.src='https://via.placeholder.com/280x158?text=No+Thumbnail'\">\n                <p style=\"text-align: center; margin-top: 10px;\">\n                    <a href=\"https://www.youtube.com/watch?v={video_id}\" target=\"_blank\" style=\"text-decoration: none; color: var(--primary-color);\">\n                        Watch on YouTube\n                    </a>\n                </p>\n            </div>\n            <div class=\"video-details\">\n                <h2>{title}</h2>\n                <p><strong>Channel:</strong> {channel}</p>\n                <p><strong>Duration:</strong> {duration}</p>\n                <p><strong>Published:</strong> {published_at}</p>\n            </div>\n        </div>\n\"\"\"\n    \n    # Add topics and their content\n    for i, topic in enumerate(topics):\n        topic_escaped = html.escape(topic)\n        topic_id = f\"topic-{i+1}\"\n        \n        # Get ELI5 explanation for this topic\n        eli5_explanation = html.escape(eli5_content.get(topic, \"\"))\n        \n        # Get Q&A pairs for this topic\n        topic_qa_pairs = qa_pairs.get(topic, [])\n        \n        html_content += f\"\"\"\n        <h2 id=\"{topic_id}\">Topic {i+1}: {topic_escaped}</h2>\n        <div class=\"topic-card\">\n            <p>{eli5_explanation}</p>\n            \n            <div class=\"qa-section\">\n                <h3>Questions & Answers</h3>\n\"\"\"\n        \n        # Add Q&A pairs\n        if topic_qa_pairs:\n            for qa in topic_qa_pairs:\n                question = html.escape(qa.get(\"question\", \"\"))\n                answer = html.escape(qa.get(\"answer\", \"\"))\n                \n                html_content += f\"\"\"\n                <div class=\"question\">Q: {question}</div>\n                <div class=\"answer\">A: {answer}</div>\n\"\"\"\n        else:\n            html_content += \"\"\"\n                <p>No questions available for this topic.</p>\n\"\"\"\n        \n        html_content += \"\"\"\n            </div>\n        </div>\n\"\"\"\n    \n    # Close the HTML document\n    html_content += \"\"\"\n        <footer>\n            <p>This summary was created by YouTube Video Summarizer</p>\n        </footer>\n    </div>\n</body>\n</html>\n\"\"\"\n    \n    return html_content\n\n\nif __name__ == \"__main__\":\n    # Test the function with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    html_output = generate_html(test_data)\n    \n    # Save the HTML to a file for testing\n    with open(\"test_summary.html\", \"w\", encoding=\"utf-8\") as f:\n        f.write(html_output)\n    \n    print(\"HTML generated and saved to test_summary.html\")\n\n\n--- File Index 18: src/utils/logger.py ---\n\"\"\"\nLogging module for YouTube Video Summarizer.\n\"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom loguru import logger\n\ndef setup_logger(log_dir=\"logs\"):\n    \"\"\"\n    Set up the logger with appropriate configuration.\n    \n    Args:\n        log_dir (str): Directory to store log files\n        \n    Returns:\n        logger: Configured logger instance\n    \"\"\"\n    # Create logs directory if it doesn't exist\n    os.makedirs(log_dir, exist_ok=True)\n    \n    # Generate timestamp for the log file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(log_dir, f\"youtube_summarizer_{timestamp}.log\")\n    \n    # Remove any existing handlers\n    logger.remove()\n    \n    # Add console handler with color\n    logger.add(\n        sys.stdout,\n        format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\",\n        level=\"INFO\",\n        colorize=True\n    )\n    \n    # Add file handler\n    logger.add(\n        log_file,\n        format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}\",\n        level=\"DEBUG\",\n        rotation=\"20 MB\",\n        retention=\"1 week\"\n    )\n    \n    logger.info(f\"Logging initialized. Log file: {log_file}\")\n    return logger\n\n# Initialize logger\nlogger = setup_logger()\n\nif __name__ == \"__main__\":\n    # Test the logger\n    logger.debug(\"This is a debug message\")\n    logger.info(\"This is an info message\")\n    logger.warning(\"This is a warning message\")\n    logger.error(\"This is an error message\")\n    logger.critical(\"This is a critical message\")\n    \n    print(f\"Log file created in the logs directory\")\n\n\n--- File Index 19: src/utils/validate_youtube_url.py ---\n\"\"\"\nUtility function to validate YouTube URLs and extract video IDs.\n\"\"\"\nimport re\nfrom urllib.parse import urlparse, parse_qs\n\ndef validate_youtube_url(url):\n    \"\"\"\n    Validates if the given URL is a valid YouTube video URL and extracts the video ID.\n    \n    Args:\n        url (str): The YouTube URL to validate\n        \n    Returns:\n        tuple: (is_valid (bool), video_id (str))\n    \"\"\"\n    if not url:\n        return False, \"\"\n    \n    # Regular expression patterns for different YouTube URL formats\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})'\n    )\n    \n    youtube_match = re.match(youtube_regex, url)\n    \n    if youtube_match:\n        video_id = youtube_match.group(6)\n        return True, video_id\n    \n    # Handle youtu.be format\n    parsed_url = urlparse(url)\n    if parsed_url.netloc == 'youtu.be':\n        video_id = parsed_url.path.lstrip('/')\n        if len(video_id) == 11:\n            return True, video_id\n    \n    # Handle youtube.com/watch?v= format\n    if parsed_url.netloc in ('youtube.com', 'www.youtube.com'):\n        query_params = parse_qs(parsed_url.query)\n        if 'v' in query_params:\n            video_id = query_params['v'][0]\n            if len(video_id) == 11:\n                return True, video_id\n    \n    return False, \"\"\n\n\nif __name__ == \"__main__\":\n    # Test the function with some example URLs\n    test_urls = [\n        \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n        \"https://youtu.be/dQw4w9WgXcQ\",\n        \"https://youtube.com/embed/dQw4w9WgXcQ\",\n        \"https://www.youtube.com/shorts/dQw4w9WgXcQ\",\n        \"https://www.notayoutubeurl.com/watch?v=dQw4w9WgXcQ\",\n        \"invalid_url\"\n    ]\n    \n    for url in test_urls:\n        is_valid, video_id = validate_youtube_url(url)\n        print(f\"URL: {url}\")\n        print(f\"Valid: {is_valid}\")\n        print(f\"Video ID: {video_id}\")\n        print(\"-\" * 40)\n\n\n--- File Index 20: utils/__init__.py ---\n\n\n--- File Index 21: utils/call_llm.py ---\nfrom openai import OpenAI\n\n# Learn more about calling the LLM: https://the-pocket.github.io/PocketFlow/utility_function/llm.html\ndef call_llm(prompt):    \n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n    \nif __name__ == \"__main__\":\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt))\n\n\n\n\nAnalyze the codebase context.\nIdentify the top 5-10 core most important abstractions to help those new to the codebase.\n\nFor each abstraction, provide:\n1. A concise `name`.\n2. A beginner-friendly `description` explaining what it is with a simple analogy, in around 100 words.\n3. A list of relevant `file_indices` (integers) using the format `idx # path/comment`.\n\nList of file indices and paths present in the context:\n- 0 # Kevin/covert to png.py\n- 1 # flow.py\n- 2 # main.py\n- 3 # nodes.py\n- 4 # src/main.py\n- 5 # src/nodes/base_node.py\n- 6 # src/nodes/content_extraction_node.py\n- 7 # src/nodes/eli5_transformation_node.py\n- 8 # src/nodes/html_generation_node.py\n- 9 # src/nodes/input_processing_node.py\n- 10 # src/nodes/qa_generation_node.py\n- 11 # src/nodes/topic_extraction_node.py\n- 12 # src/nodes/topic_orchestrator_node.py\n- 13 # src/nodes/topic_processor_node.py\n- 14 # src/utils/call_llm.py\n- 15 # src/utils/extract_youtube_metadata.py\n- 16 # src/utils/extract_youtube_transcript.py\n- 17 # src/utils/generate_html.py\n- 18 # src/utils/logger.py\n- 19 # src/utils/validate_youtube_url.py\n- 20 # utils/__init__.py\n- 21 # utils/call_llm.py\n\nFormat the output as a YAML list of dictionaries:\n\n```yaml\n- name: Query Processing\n  description: | \n    Explains what the abstraction does.\n    It's like a central dispatcher routing requests.\n  file_indices:\n    - 0 # path/to/file1.py\n    - 3 # path/to/related.py\n- name: Query Optimization\n  description: |\n    Another core concept, similar to a blueprint for objects.\n  file_indices:\n    - 5 # path/to/another.js\n# ... up to 10 abstractions\n```": "```yaml\n- name: Pipeline Orchestration\n  description: |\n    This manages the overall flow of the YouTube summarization process. It defines the sequence of steps (Nodes) that need to run to transform a YouTube URL into a summary.\n    Think of it like a project manager for the summarization task, ensuring each specialist (Node) does their job in the right order, starting with the URL and ending with the HTML report.\n  file_indices:\n    - 4 # src/main.py\n\n- name: Node (Pipeline Step)\n  description: |\n    A Node represents a single, self-contained step within the summarization pipeline. Each Node performs a specific task, such as extracting the transcript, identifying topics, or generating Q&A pairs. They inherit from a base class that defines a standard structure (`prep`, `exec`, `post`).\n    It's like a specialized worker on an assembly line. One worker validates the input, another extracts parts, another processes those parts, and a final one assembles the output.\n  file_indices:\n    - 5 # src/nodes/base_node.py\n    - 6 # src/nodes/content_extraction_node.py\n    - 7 # src/nodes/eli5_transformation_node.py\n    - 8 # src/nodes/html_generation_node.py\n    - 9 # src/nodes/input_processing_node.py\n    - 11 # src/nodes/topic_extraction_node.py\n    - 12 # src/nodes/topic_orchestrator_node.py\n    - 13 # src/nodes/topic_processor_node.py\n\n- name: Shared Memory\n  description: |\n    This is the mechanism used to pass data between different Nodes in the pipeline. It's essentially a Python dictionary where each Node can read the results from previous Nodes and write its own results for subsequent Nodes to use.\n    Imagine a shared whiteboard where each worker writes down their results (like video ID, transcript, topics) so the next worker in line knows what to work with.\n  file_indices:\n    - 5 # src/nodes/base_node.py\n    - 4 # src/main.py\n    # Note: All specific node files also interact with shared_memory\n\n- name: Topic Processing Orchestrator\n  description: |\n    This Node manages the processing of multiple topics identified in the video. It uses a parallel approach (Map-Reduce style) by assigning each topic to a dedicated `TopicProcessorNode` to generate Q&A and ELI5 explanations simultaneously, making the process faster.\n    It's like a team lead who divides a big research task (all topics) among team members (TopicProcessorNodes), lets them work in parallel, and then collects and combines their findings.\n  file_indices:\n    - 12 # src/nodes/topic_orchestrator_node.py\n    - 13 # src/nodes/topic_processor_node.py\n\n- name: LLM Utility\n  description: |\n    This utility function provides a standardized way to communicate with a Large Language Model (like GPT-4). Nodes use this function to send prompts (instructions and context) to the AI and receive generated content, such as topics, answers to questions, or simplified explanations.\n    It acts like a dedicated phone line to a very knowledgeable expert (the LLM). You dial the number, ask your question (prompt), and get an answer back.\n  file_indices:\n    - 14 # src/utils/call_llm.py\n    - 11 # src/nodes/topic_extraction_node.py\n    - 13 # src/nodes/topic_processor_node.py # Indirectly uses it via LLM calls\n\n- name: YouTube Data Utilities\n  description: |\n    These are helper functions specifically designed to interact with YouTube. One utility extracts metadata (like title, channel, duration), another extracts the video's transcript (spoken words), and another validates the input URL.\n    Think of these as specialized tools in a toolbox for YouTube: one tool fetches the video's label information, another gets the script, and a third checks if the link is correct.\n  file_indices:\n    - 15 # src/utils/extract_youtube_metadata.py\n    - 16 # src/utils/extract_youtube_transcript.py\n    - 19 # src/utils/validate_youtube_url.py\n    - 9 # src/nodes/input_processing_node.py # Uses metadata & validation\n    - 6 # src/nodes/content_extraction_node.py # Uses transcript\n\n- name: HTML Generation\n  description: |\n    This component is responsible for creating the final output: a user-friendly HTML file. It takes all the processed data (metadata, topics, Q&A, ELI5 explanations) and formats it into a structured web page with styling.\n    It's like a report generator or a desktop publisher that takes raw data and results, and lays them out nicely in a final document (the HTML page) for easy reading.\n  file_indices:\n    - 8 # src/nodes/html_generation_node.py\n    - 17 # src/utils/generate_html.py\n\n- name: Logging\n  description: |\n    Provides a consistent way to record messages about the application's execution, including informational messages, warnings, and errors. This helps in monitoring the pipeline's progress and debugging issues.\n    It's like a detailed ship's log, recording every significant event and observation as the application sails through its tasks, making it easier to understand what happened later.\n  file_indices:\n    - 18 # src/utils/logger.py\n    # Note: logger is imported and used in most src/* files\n```", "\nBased on the following abstractions and relevant code snippets from the project `pf1`:\n\nList of Abstraction Indices and Names:\n0 # Pipeline Orchestration\n1 # Node (Pipeline Step)\n2 # Shared Memory\n3 # Topic Processing Orchestrator\n4 # LLM Utility\n5 # YouTube Data Utilities\n6 # HTML Generation\n7 # Logging\n\nContext (Abstractions, Descriptions, Code):\nIdentified Abstractions:\n- Index 0: Pipeline Orchestration (Relevant file indices: [4])\n  Description: This manages the overall flow of the YouTube summarization process. It defines the sequence of steps (Nodes) that need to run to transform a YouTube URL into a summary.\nThink of it like a project manager for the summarization task, ensuring each specialist (Node) does their job in the right order, starting with the URL and ending with the HTML report.\n\n- Index 1: Node (Pipeline Step) (Relevant file indices: [5, 6, 7, 8, 9, 11, 12, 13])\n  Description: A Node represents a single, self-contained step within the summarization pipeline. Each Node performs a specific task, such as extracting the transcript, identifying topics, or generating Q&A pairs. They inherit from a base class that defines a standard structure (`prep`, `exec`, `post`).\nIt's like a specialized worker on an assembly line. One worker validates the input, another extracts parts, another processes those parts, and a final one assembles the output.\n\n- Index 2: Shared Memory (Relevant file indices: [4, 5])\n  Description: This is the mechanism used to pass data between different Nodes in the pipeline. It's essentially a Python dictionary where each Node can read the results from previous Nodes and write its own results for subsequent Nodes to use.\nImagine a shared whiteboard where each worker writes down their results (like video ID, transcript, topics) so the next worker in line knows what to work with.\n\n- Index 3: Topic Processing Orchestrator (Relevant file indices: [12, 13])\n  Description: This Node manages the processing of multiple topics identified in the video. It uses a parallel approach (Map-Reduce style) by assigning each topic to a dedicated `TopicProcessorNode` to generate Q&A and ELI5 explanations simultaneously, making the process faster.\nIt's like a team lead who divides a big research task (all topics) among team members (TopicProcessorNodes), lets them work in parallel, and then collects and combines their findings.\n\n- Index 4: LLM Utility (Relevant file indices: [11, 13, 14])\n  Description: This utility function provides a standardized way to communicate with a Large Language Model (like GPT-4). Nodes use this function to send prompts (instructions and context) to the AI and receive generated content, such as topics, answers to questions, or simplified explanations.\nIt acts like a dedicated phone line to a very knowledgeable expert (the LLM). You dial the number, ask your question (prompt), and get an answer back.\n\n- Index 5: YouTube Data Utilities (Relevant file indices: [6, 9, 15, 16, 19])\n  Description: These are helper functions specifically designed to interact with YouTube. One utility extracts metadata (like title, channel, duration), another extracts the video's transcript (spoken words), and another validates the input URL.\nThink of these as specialized tools in a toolbox for YouTube: one tool fetches the video's label information, another gets the script, and a third checks if the link is correct.\n\n- Index 6: HTML Generation (Relevant file indices: [8, 17])\n  Description: This component is responsible for creating the final output: a user-friendly HTML file. It takes all the processed data (metadata, topics, Q&A, ELI5 explanations) and formats it into a structured web page with styling.\nIt's like a report generator or a desktop publisher that takes raw data and results, and lays them out nicely in a final document (the HTML page) for easy reading.\n\n- Index 7: Logging (Relevant file indices: [18])\n  Description: Provides a consistent way to record messages about the application's execution, including informational messages, warnings, and errors. This helps in monitoring the pipeline's progress and debugging issues.\nIt's like a detailed ship's log, recording every significant event and observation as the application sails through its tasks, making it easier to understand what happened later.\n\n\nRelevant File Snippets (Referenced by Index and Path):\n--- File: 4 # src/main.py ---\n\"\"\"\nYouTube Video Summarizer - Main Application\n\"\"\"\nimport os\nimport sys\nimport argparse\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Add the project root to the path so we can import from src\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom src.nodes.input_processing_node import InputProcessingNode\nfrom src.nodes.content_extraction_node import ContentExtractionNode\nfrom src.nodes.topic_extraction_node import TopicExtractionNode\nfrom src.nodes.topic_orchestrator_node import TopicOrchestratorNode\nfrom src.nodes.html_generation_node import HTMLGenerationNode\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", enable_chunking=False, max_workers=3):\n    \"\"\"\n    Run the complete YouTube video summarization pipeline.\n    \n    Args:\n        youtube_url (str): URL of the YouTube video to summarize\n        output_dir (str): Directory to save the output HTML file\n        enable_chunking (bool): Whether to enable transcript chunking\n        max_workers (int): Maximum number of parallel workers for topic processing\n    \"\"\"\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"YouTube Video Summarizer\")\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n    logger.info(f\"Chunking: {'Enabled' if enable_chunking else 'Disabled'}\")\n    logger.info(f\"Parallel Workers: {max_workers}\")\n    logger.info(f\"{'='*60}\")\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": youtube_url}\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    logger.debug(f\"Output directory created/confirmed: {output_dir}\")\n    \n    # Configure chunking parameters if enabled\n    chunk_size = 4000 if enable_chunking else 0\n    overlap = 500 if enable_chunking else 0\n    \n    try:\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        input_node = InputProcessingNode(shared_memory)\n        shared_memory = input_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Input Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        logger.info(f\"Successfully processed video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        \n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Content Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        transcript_length = len(shared_memory.get('transcript', ''))\n        logger.info(f\"Successfully extracted transcript ({transcript_length} characters)\")\n        \n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, chunk_size=chunk_size, overlap=overlap)\n        shared_memory = topic_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        topics = shared_memory.get('topics', [])\n        logger.info(f\"Successfully extracted {len(topics)} topics\")\n        for i, topic in enumerate(topics):\n            logger.info(f\"  Topic {i+1}: {topic}\")\n        \n        # 4. Topic Processing Orchestrator Node\n        logger.info(\"[4/5] Starting Topic Processing...\")\n        orchestrator_node = TopicOrchestratorNode(shared_memory, max_workers=max_workers, questions_per_topic=3)\n        shared_memory = orchestrator_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        qa_pairs = shared_memory.get('qa_pairs', {})\n        eli5_content = shared_memory.get('eli5_content', {})\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        \n        logger.info(f\"Successfully processed {len(topics)} topics\")\n        logger.info(f\"Generated {total_qa_pairs} Q&A pairs and {len(eli5_content)} ELI5 explanations\")\n        \n        # 5. HTML Generation Node\n        logger.info(\"[5/5] Starting HTML Generation...\")\n        # Create a timestamped filename\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        video_id = shared_memory.get(\"video_id\", \"unknown\")\n        output_file = os.path.join(output_dir, f\"summary_{video_id}_{timestamp}.html\")\n        \n        html_node = HTMLGenerationNode(shared_memory, output_file)\n        shared_memory = html_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"HTML Generation failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        # Print summary\n        logger.info(f\"{'='*60}\")\n        logger.info(\"Summary Generation Complete!\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        logger.info(f\"Topics: {len(shared_memory.get('topics', []))}\")\n        logger.info(f\"Q&A Pairs: {sum(len(pairs) for pairs in shared_memory.get('qa_pairs', {}).values())}\")\n        logger.info(f\"Output File: {output_file}\")\n        logger.info(f\"{'='*60}\")\n        \n        return shared_memory\n        \n    except Exception as e:\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n\ndef main():\n    \"\"\"\n    Main entry point for the application.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"YouTube Video Summarizer\")\n    parser.add_argument(\"url\", nargs='?', default=None, help=\"YouTube video URL to summarize\")\n    parser.add_argument(\"--output\", \"-o\", default=\"output\", help=\"Output directory for HTML summary\")\n    parser.add_argument(\"--chunk\", action=\"store_true\", help=\"Enable transcript chunking for long videos\")\n    parser.add_argument(\"--workers\", \"-w\", type=int, default=3, help=\"Number of parallel workers for topic processing\")\n    \n    args = parser.parse_args()\n    \n    # If no URL is provided as a command-line argument, ask for user input\n    youtube_url = args.url\n    if not youtube_url:\n        youtube_url = input(\"Please enter a YouTube URL to summarize: \")\n    \n    logger.info(f\"Starting YouTube Video Summarizer with URL: {youtube_url}\")\n    \n    # Run the pipeline\n    run_pipeline(youtube_url, args.output, args.chunk, args.workers)\n\nif __name__ == \"__main__\":\n    main()\n\n\n--- File: 5 # src/nodes/base_node.py ---\n\"\"\"\nBase node class for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom abc import ABC, abstractmethod\nfrom src.utils.logger import logger\n\nclass BaseNode(ABC):\n    \"\"\"\n    Abstract base class for all nodes in the YouTube Video Summarizer.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary for data exchange between nodes\n        \"\"\"\n        self.shared_memory = shared_memory or {}\n        self.node_name = self.__class__.__name__\n        logger.debug(f\"{self.node_name} initialized\")\n    \n    @abstractmethod\n    def prep(self):\n        \"\"\"\n        Prepare for execution by reading necessary data from shared memory.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def exec(self):\n        \"\"\"\n        Execute the node's main functionality.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def post(self):\n        \"\"\"\n        Post-process and write results to shared memory.\n        \"\"\"\n        pass\n    \n    def run(self):\n        \"\"\"\n        Run the complete node workflow: prep, exec, post.\n        \n        Returns:\n            dict: The updated shared memory\n        \"\"\"\n        logger.debug(f\"{self.node_name} starting run\")\n        try:\n            self.prep()\n            if \"error\" not in self.shared_memory:\n                self.exec()\n            if \"error\" not in self.shared_memory:\n                self.post()\n            \n            if \"error\" in self.shared_memory:\n                logger.error(f\"{self.node_name} failed: {self.shared_memory['error']}\")\n            else:\n                logger.debug(f\"{self.node_name} completed successfully\")\n                \n        except Exception as e:\n            logger.exception(f\"Unexpected error in {self.node_name}: {str(e)}\")\n            self.shared_memory[\"error\"] = f\"{self.node_name} error: {str(e)}\"\n            \n        return self.shared_memory\n\n\n--- File: 6 # src/nodes/content_extraction_node.py ---\n\"\"\"\nContent Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\nfrom src.utils.logger import logger\n\nclass ContentExtractionNode(BaseNode):\n    \"\"\"\n    Node for extracting transcript from a YouTube video.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_id exists in shared memory.\n        \"\"\"\n        if \"video_id\" not in self.shared_memory:\n            error_msg = \"YouTube video ID not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Content Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(f\"Extracting transcript for video ID: {self.shared_memory['video_id']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute transcript extraction.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Extract the transcript\n        video_id = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Calling YouTube transcript API for video ID: {video_id}\")\n        transcript = extract_youtube_transcript(video_id)\n        \n        # Check if there was an error\n        if transcript.startswith(\"Error:\"):\n            logger.error(f\"Transcript extraction failed: {transcript}\")\n            self.shared_memory[\"error\"] = transcript\n            return\n        \n        self.shared_memory[\"transcript\"] = transcript\n        \n        # Log a preview of the transcript\n        preview_length = min(150, len(transcript))\n        logger.info(f\"Extracted transcript ({len(transcript)} characters)\")\n        logger.debug(f\"Transcript preview: {transcript[:preview_length]}...\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Content Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Failed to extract transcript\"\n            logger.error(f\"Error: {error_msg}\")\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Content Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample video ID\n    test_video_id = \"dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_id\": test_video_id}\n    \n    # Create and run the node\n    node = ContentExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    if \"error\" in updated_memory:\n        logger.error(f\"Error: {updated_memory['error']}\")\n    else:\n        transcript = updated_memory.get(\"transcript\", \"\")\n        logger.info(f\"Transcript length: {len(transcript)} characters\")\n        logger.info(f\"Transcript preview: {transcript[:200]}...\")\n\n\n--- File: 7 # src/nodes/eli5_transformation_node.py ---\n\"\"\"\nELI5 (Explain Like I'm 5) Transformation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass ELI5TransformationNode(BaseNode):\n    \"\"\"\n    Node for transforming content into child-friendly explanations (ELI5).\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n        self.eli5_content = {}\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and qa_pairs exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"qa_pairs\" not in self.shared_memory:\n            error_msg = \"Q&A pairs not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping ELI5 Transformation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        topics_count = len(self.shared_memory['topics'])\n        logger.info(f\"Transforming content into child-friendly explanations for {topics_count} topics\")\n        logger.debug(f\"Topics: {self.shared_memory['topics']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute ELI5 transformation for each topic and its Q&A pairs.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        topics = self.shared_memory[\"topics\"]\n        qa_pairs = self.shared_memory[\"qa_pairs\"]\n        \n        # Process each topic to create ELI5 explanations\n        for i, topic in enumerate(topics):\n            logger.info(f\"Creating ELI5 explanation for topic {i+1}/{len(topics)}: {topic}\")\n            \n            # Get Q&A pairs for this topic\n            topic_qa_pairs = qa_pairs.get(topic, [])\n            logger.debug(f\"Found {len(topic_qa_pairs)} Q&A pairs for topic '{topic}'\")\n            \n            # Combine Q&A pairs into a single text for context\n            qa_text = \"\"\n            for qa in topic_qa_pairs:\n                question = qa.get(\"question\", \"\")\n                answer = qa.get(\"answer\", \"\")\n                qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n            \n            # Create prompt for ELI5 transformation\n            prompt = textwrap.dedent(f\"\"\"\n            You are an expert at explaining complex topics to young children (5-7 years old).\n            \n            I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n            Your task is to:\n            1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n            2. Use simple words, short sentences, and concrete examples\n            3. Avoid jargon and technical terms\n            4. Use analogies to familiar concepts when possible\n            5. Keep the explanation under 200 words\n            6. Maintain the core information while simplifying the language\n            \n            Topic: {topic}\n            \n            Q&A Context:\n            {qa_text}\n            \n            Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n            \"\"\")\n            \n            # Call LLM to generate ELI5 explanation\n            try:\n                logger.debug(f\"Calling LLM for topic '{topic}' ELI5 transformation\")\n                response = call_llm(prompt, temperature=0.7, max_tokens=500)\n                logger.debug(f\"LLM response for topic '{topic}' ELI5: {response[:100]}...\")\n                \n                # Clean up the response\n                explanation = response.strip()\n                \n                # Store the ELI5 explanation\n                self.eli5_content[topic] = explanation\n                logger.info(f\"Generated ELI5 explanation for topic '{topic}' ({len(explanation)} characters)\")\n                \n            except Exception as e:\n                error_msg = f\"Error calling LLM for topic '{topic}': {str(e)}\"\n                logger.error(error_msg)\n                self.eli5_content[topic] = f\"Sorry, I couldn't create a simple explanation for {topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and store the ELI5 explanations in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in ELI5 Transformation Node: {self.shared_memory['error']}\")\n            return\n        \n        # Store the ELI5 explanations in shared memory\n        self.shared_memory[\"eli5_content\"] = self.eli5_content\n        \n        # Print a summary of the generated ELI5 explanations\n        logger.info(f\"Generated ELI5 explanations for {len(self.eli5_content)} topics\")\n        \n        for topic, explanation in self.eli5_content.items():\n            preview = explanation[:100] + \"...\" if len(explanation) > 100 else explanation\n            logger.info(f\"  Topic '{topic}':\")\n            logger.info(f\"    {preview}\")\n        \n        logger.info(\"ELI5 Transformation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and Q&A pairs\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_qa_pairs = {\n        \"Machine Learning Basics\": [\n            {\n                \"question\": \"What is machine learning?\",\n                \"answer\": \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Unlike traditional programming where you explicitly program rules, in machine learning, you train models on data and they learn patterns.\"\n            },\n            {\n                \"question\": \"Why is machine learning important?\",\n                \"answer\": \"Machine learning is important because it allows computers to find insights and make predictions without being explicitly programmed. It can handle complex tasks that would be difficult to code manually.\"\n            }\n        ],\n        \"Types of Machine Learning\": [\n            {\n                \"question\": \"What is supervised learning?\",\n                \"answer\": \"Supervised learning is a type of machine learning where the model is trained on labeled data. For example, you might have images labeled as 'cat' or 'dog' and the model learns to distinguish between them.\"\n            },\n            {\n                \"question\": \"What is unsupervised learning?\",\n                \"answer\": \"Unsupervised learning is where the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task where the algorithm groups similar data points together.\"\n            }\n        ]\n    }\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"qa_pairs\": test_qa_pairs\n    }\n    \n    # Create and run the node\n    node = ELI5TransformationNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(explanation)\n\n\n--- File: 8 # src/nodes/html_generation_node.py ---\n\"\"\"\nHTML Generation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.generate_html import generate_html\nfrom src.utils.logger import logger\n\nclass HTMLGenerationNode(BaseNode):\n    \"\"\"\n    Node for generating HTML visualization of the video summary.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, output_path=None):\n        \"\"\"\n        Initialize the node with shared memory and output path.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            output_path (str): Path to save the HTML output file\n        \"\"\"\n        super().__init__(shared_memory)\n        self.output_path = output_path\n        logger.debug(f\"HTMLGenerationNode initialized with output_path={output_path}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if all required data exists in shared memory.\n        \"\"\"\n        required_keys = [\"video_id\", \"metadata\", \"topics\", \"qa_pairs\", \"eli5_content\"]\n        missing_keys = [key for key in required_keys if key not in self.shared_memory]\n        \n        if missing_keys:\n            error_msg = f\"Missing required data in shared memory: {', '.join(missing_keys)}\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping HTML Generation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Preparing to generate HTML summary\")\n        logger.debug(f\"Found {len(self.shared_memory['topics'])} topics and {len(self.shared_memory['eli5_content'])} ELI5 explanations\")\n    \n    def exec(self):\n        \"\"\"\n        Execute HTML generation using the data in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Prepare the summary data for HTML generation\n        summary_data = {\n            \"video_id\": self.shared_memory[\"video_id\"],\n            \"metadata\": self.shared_memory[\"metadata\"],\n            \"topics\": self.shared_memory[\"topics\"],\n            \"qa_pairs\": self.shared_memory[\"qa_pairs\"],\n            \"eli5_content\": self.shared_memory[\"eli5_content\"]\n        }\n        \n        # Generate the HTML content\n        logger.debug(\"Calling generate_html function\")\n        html_content = generate_html(summary_data)\n        \n        # Store the HTML content in shared memory\n        self.shared_memory[\"html_output\"] = html_content\n        logger.info(f\"Generated HTML content ({len(html_content)} characters)\")\n        \n        # Save the HTML to a file if output_path is provided\n        if self.output_path:\n            try:\n                # Create directory if it doesn't exist\n                os.makedirs(os.path.dirname(os.path.abspath(self.output_path)), exist_ok=True)\n                logger.debug(f\"Ensuring output directory exists: {os.path.dirname(os.path.abspath(self.output_path))}\")\n                \n                # Write the HTML content to the file\n                with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html_content)\n                \n                logger.info(f\"HTML summary saved to: {self.output_path}\")\n                \n            except Exception as e:\n                error_msg = f\"Error saving HTML file: {str(e)}\"\n                logger.error(error_msg)\n                self.shared_memory[\"error\"] = error_msg\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in HTML Generation Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"html_output\" not in self.shared_memory:\n            error_msg = \"Failed to generate HTML output\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        html_length = len(self.shared_memory[\"html_output\"])\n        logger.info(f\"Generated HTML summary ({html_length} characters)\")\n        logger.info(\"HTML Generation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    # Initialize shared memory\n    shared_memory = test_data\n    \n    # Create and run the node with a test output path\n    test_output_path = \"test_summary.html\"\n    node = HTMLGenerationNode(shared_memory, test_output_path)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"HTML output length: {len(updated_memory.get('html_output', ''))}\")\n    logger.info(f\"HTML file saved to: {test_output_path}\")\n\n\n--- File: 9 # src/nodes/input_processing_node.py ---\n\"\"\"\nInput Processing Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\nfrom src.utils.logger import logger\n\nclass InputProcessingNode(BaseNode):\n    \"\"\"\n    Node for validating YouTube URL and extracting video metadata.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, youtube_url=None):\n        \"\"\"\n        Initialize the node with shared memory and YouTube URL.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            youtube_url (str): YouTube URL to process\n        \"\"\"\n        super().__init__(shared_memory)\n        if youtube_url:\n            self.shared_memory[\"video_url\"] = youtube_url\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_url exists in shared memory.\n        \"\"\"\n        if \"video_url\" not in self.shared_memory:\n            error_msg = \"YouTube URL not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        logger.info(f\"Processing YouTube URL: {self.shared_memory['video_url']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute URL validation and metadata extraction.\n        \"\"\"\n        # Validate the YouTube URL\n        youtube_url = self.shared_memory[\"video_url\"]\n        is_valid, video_id = validate_youtube_url(youtube_url)\n        \n        if not is_valid:\n            error_msg = f\"Invalid YouTube URL: {youtube_url}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"video_id\"] = video_id\n        logger.info(f\"Valid YouTube video ID: {video_id}\")\n        \n        # Extract video metadata\n        logger.debug(f\"Extracting metadata for video ID: {video_id}\")\n        metadata = extract_youtube_metadata(video_id)\n        \n        if \"error\" in metadata:\n            error_msg = f\"Error extracting metadata: {metadata['error']}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"metadata\"] = metadata\n        logger.info(f\"Extracted metadata for video: {metadata.get('title', 'Unknown Title')}\")\n        logger.debug(f\"Video metadata: {metadata}\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Input Processing Node: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Input Processing Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample YouTube URL\n    test_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": test_url}\n    \n    # Create and run the node\n    node = InputProcessingNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Video ID: {updated_memory.get('video_id', 'Not found')}\")\n    logger.info(f\"Video Title: {updated_memory.get('metadata', {}).get('title', 'Not found')}\")\n    logger.info(f\"Channel: {updated_memory.get('metadata', {}).get('channel_name', 'Not found')}\")\n    logger.info(f\"Duration: {updated_memory.get('metadata', {}).get('duration', 'Not found')}\")\n\n\n--- File: 11 # src/nodes/topic_extraction_node.py ---\n\"\"\"\nTopic Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport concurrent.futures\nfrom typing import List, Dict, Any\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicExtractionNode(BaseNode):\n    \"\"\"\n    Node for identifying main topics from the video transcript.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, chunk_size=0, overlap=0, max_topics=5):\n        \"\"\"\n        Initialize the node with shared memory and processing parameters.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            chunk_size (int): Size of transcript chunks (0 = no chunking)\n            overlap (int): Overlap between chunks (when chunk_size > 0)\n            max_topics (int): Maximum number of topics to extract\n        \"\"\"\n        super().__init__(shared_memory)\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_topics = max_topics\n        self.chunks = []\n        self.chunk_topics = []\n        logger.debug(f\"TopicExtractionNode initialized with chunk_size={chunk_size}, overlap={overlap}, max_topics={max_topics}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if transcript exists in shared memory\n        and splitting it into chunks if chunk_size > 0.\n        \"\"\"\n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        transcript = self.shared_memory[\"transcript\"]\n        logger.debug(f\"Preparing to extract topics from transcript ({len(transcript)} characters)\")\n        \n        # Only chunk if chunk_size is specified\n        if self.chunk_size > 0:\n            # Split into overlapping chunks\n            start = 0\n            while start < len(transcript):\n                end = min(start + self.chunk_size, len(transcript))\n                self.chunks.append(transcript[start:end])\n                start = end - self.overlap\n            logger.info(f\"Split transcript into {len(self.chunks)} chunks for processing\")\n            logger.debug(f\"Chunk sizes: {[len(chunk) for chunk in self.chunks]}\")\n        else:\n            # Process as a single chunk\n            self.chunks = [transcript]\n            logger.info(\"Processing transcript as a single chunk\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic extraction for each chunk and combine results.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Use ThreadPoolExecutor for parallel processing of chunks\n        max_workers = min(len(self.chunks), 4)  # Limit concurrent API calls\n        logger.info(f\"Processing {len(self.chunks)} chunks with {max_workers} parallel workers\")\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all chunk processing tasks\n            future_to_chunk = {\n                executor.submit(self._process_chunk, i, chunk): i \n                for i, chunk in enumerate(self.chunks)\n            }\n            \n            # Collect results as they complete\n            for future in concurrent.futures.as_completed(future_to_chunk):\n                chunk_index = future_to_chunk[future]\n                try:\n                    topics = future.result()\n                    if topics:\n                        self.chunk_topics.append(topics)\n                        logger.info(f\"Completed processing for chunk {chunk_index+1}/{len(self.chunks)}\")\n                except Exception as e:\n                    logger.error(f\"Error processing chunk {chunk_index+1}: {str(e)}\")\n                    \n        # If we couldn't extract any topics, add some default ones\n        if not self.chunk_topics:\n            logger.warning(\"No topics extracted, using default topics\")\n            self.chunk_topics.append([\"Main Content\", \"Key Points\", \"Summary\"])\n    \n    def _process_chunk(self, chunk_index: int, chunk: str) -> List[str]:\n        \"\"\"\n        Process a single chunk to extract topics.\n        \n        Args:\n            chunk_index (int): Index of the chunk\n            chunk (str): Content of the chunk\n            \n        Returns:\n            List[str]: Extracted topics\n        \"\"\"\n        logger.info(f\"Processing chunk {chunk_index+1}/{len(self.chunks)}...\")\n        \n        # Create prompt for topic extraction\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content and identifying main topics.\n        \n        I'll provide you with a transcript from a YouTube video. Your task is to:\n        1. Identify the main topics discussed in this segment\n        2. List each topic as a short, clear phrase (3-7 words)\n        3. Provide at most {self.max_topics} topics\n        4. Focus on substantive content, not introductions or conclusions\n        \n        Here is the transcript segment:\n        \n        {chunk[:2000]}...\n        \n        Respond with ONLY a JSON array of topic strings. For example:\n        [\"Topic One\", \"Topic Two\", \"Topic Three\"]\n        \"\"\")\n        \n        # Call LLM to extract topics\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1} (timeout: 30s)...\")\n            response = call_llm(prompt, temperature=0.3, max_tokens=200, timeout=30)\n            \n            # Check if we got an error response\n            if response.startswith(\"Error:\"):\n                logger.warning(f\"LLM API error for chunk {chunk_index+1}: {response}\")\n                # Add some default topics if we can't get them from the LLM\n                if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                    logger.info(\"Using default topics due to LLM error\")\n                    return [\"Main Content\", \"Key Points\", \"Summary\"]\n                return []\n            \n            logger.info(f\"Received LLM response for chunk {chunk_index+1} ({len(response)} characters)\")\n            logger.debug(f\"LLM response preview: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                import json\n                topics = json.loads(cleaned_response)\n                if isinstance(topics, list):\n                    logger.info(f\"Extracted {len(topics)} topics from chunk {chunk_index+1}\")\n                    logger.debug(f\"Topics from chunk {chunk_index+1}: {topics}\")\n                    return topics\n                else:\n                    logger.warning(f\"Expected list but got {type(topics)} from LLM\")\n                    # Try to extract topics from text response\n                    return [cleaned_response]\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response}\")\n                # Try to extract topics from text response\n                lines = cleaned_response.split(\"\\n\")\n                potential_topics = [line.strip().strip('\",[]') for line in lines if line.strip()]\n                logger.info(f\"Extracted {len(potential_topics)} topics from non-JSON response\")\n                return potential_topics\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # Add some default topics if we can't get them from the LLM\n            if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                logger.info(\"Using default topics due to exception\")\n                return [\"Main Content\", \"Key Points\", \"Summary\"]\n            return []\n    \n    def post(self):\n        \"\"\"\n        Post-process by combining topics from all chunks and selecting the most relevant ones.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.chunk_topics:\n            error_msg = \"Failed to extract topics from transcript\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        # Flatten the list of topics from all chunks\n        all_topics = []\n        for topics in self.chunk_topics:\n            all_topics.extend(topics)\n        \n        logger.debug(f\"All extracted topics (before deduplication): {all_topics}\")\n        \n        # Count topic occurrences to find the most common ones\n        topic_counts = {}\n        for topic in all_topics:\n            topic_lower = topic.lower()\n            topic_counts[topic_lower] = topic_counts.get(topic_lower, 0) + 1\n        \n        logger.debug(f\"Topic frequency counts: {topic_counts}\")\n        \n        # Sort topics by frequency\n        sorted_topics = sorted(\n            [(count, topic) for topic, count in topic_counts.items()], \n            reverse=True\n        )\n        \n        # Select the top topics (using the original casing)\n        top_topics = []\n        seen_topics = set()\n        \n        for _, topic_lower in sorted_topics:\n            # Find the original casing version\n            original_casing = next(\n                (t for t in all_topics if t.lower() == topic_lower), \n                topic_lower.title()\n            )\n            \n            # Skip if we've already added this topic\n            if topic_lower in seen_topics:\n                continue\n                \n            top_topics.append(original_casing)\n            seen_topics.add(topic_lower)\n            \n            # Limit to max_topics\n            if len(top_topics) >= self.max_topics:\n                break\n        \n        # Store the final list of topics\n        self.shared_memory[\"topics\"] = top_topics\n        \n        logger.info(f\"Extracted {len(top_topics)} final topics:\")\n        for i, topic in enumerate(top_topics):\n            logger.info(f\"  {i+1}. {topic}\")\n        \n        logger.info(\"Topic Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with a sample transcript\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \n    Now let's discuss some applications of machine learning. In healthcare, machine learning \n    is used for disease diagnosis, drug discovery, and personalized medicine. In finance, \n    it's used for fraud detection, algorithmic trading, and credit scoring. In transportation, \n    self-driving cars rely heavily on machine learning algorithms.\n    \n    Ethical considerations in machine learning include bias in training data, model interpretability, \n    and privacy concerns. It's important to develop responsible AI systems that are fair and transparent.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\"transcript\": test_transcript}\n    \n    # Create and run the node\n    node = TopicExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Topics: {updated_memory.get('topics', [])}\")\n\n\n--- File: 12 # src/nodes/topic_orchestrator_node.py ---\n\"\"\"\nTopic Processing Orchestrator Node for YouTube Video Summarizer.\nImplements a Map-Reduce approach for parallel topic processing.\n\"\"\"\nimport sys\nimport os\nfrom typing import List, Dict, Any\nimport concurrent.futures\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.nodes.topic_processor_node import TopicProcessorNode\nfrom src.utils.logger import logger\n\nclass TopicOrchestratorNode(BaseNode):\n    \"\"\"\n    Node for orchestrating parallel processing of topics using Map-Reduce pattern.\n    Maps topics to individual processors and reduces the results.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, max_workers=3, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            max_workers (int): Maximum number of parallel workers for topic processing\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory)\n        self.max_workers = max_workers\n        self.questions_per_topic = questions_per_topic\n        self.topics = []\n        self.transcript = \"\"\n        self.topic_results = {}\n        logger.debug(f\"TopicOrchestratorNode initialized with max_workers={max_workers}, questions_per_topic={questions_per_topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and transcript exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Orchestration due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        \n        topics_count = len(self.topics)\n        # Adjust max_workers if there are fewer topics than workers\n        self.max_workers = min(self.max_workers, topics_count)\n        logger.info(f\"Preparing to process {topics_count} topics with {self.max_workers} parallel workers\")\n        logger.debug(f\"Topics to process: {self.topics}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute Map-Reduce processing for topics.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n        \n        # Map phase: Process all topics in parallel\n        self._map_phase()\n        \n        # Reduce phase: Combine results\n        self._reduce_phase()\n    \n    def _map_phase(self):\n        \"\"\"\n        Map phase: Process all topics in parallel using a thread pool.\n        \"\"\"\n        logger.info(f\"Starting Map phase with {self.max_workers} workers for {len(self.topics)} topics\")\n        \n        # Use ThreadPoolExecutor for parallel processing\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all topics for processing\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic \n                for topic in self.topics\n            }\n            \n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    result = future.result()\n                    self.topic_results[topic] = result\n                    logger.info(f\"Completed processing for topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {str(e)}\")\n                    # Create an empty result for failed topics\n                    self.topic_results[topic] = {\n                        \"topic\": topic,\n                        \"qa_pairs\": [],\n                        \"eli5_content\": f\"Error processing topic: {str(e)}\"\n                    }\n        \n        logger.info(f\"Map phase complete: Processed {len(self.topic_results)} topics\")\n    \n    def _process_topic(self, topic):\n        \"\"\"\n        Process a single topic using a TopicProcessorNode.\n        \n        Args:\n            topic (str): The topic to process\n            \n        Returns:\n            dict: The processing result\n        \"\"\"\n        logger.info(f\"Processing topic: {topic}\")\n        \n        # Create and run a TopicProcessorNode for this topic\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic\n        )\n        \n        return processor.run()[\"topic_results\"][topic]\n    \n    def _reduce_phase(self):\n        \"\"\"\n        Reduce phase: Combine results from all topic processors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        logger.info(f\"Starting Reduce phase with {len(self.topic_results)} topic results\")\n        \n        # Initialize the combined results\n        qa_pairs = {}\n        eli5_content = {}\n        \n        # Combine results from all topics\n        for topic, result in self.topic_results.items():\n            # Add Q&A pairs\n            qa_pairs[topic] = result.get(\"qa_pairs\", [])\n            \n            # Add ELI5 content\n            eli5_content[topic] = result.get(\"eli5_content\", \"\")\n        \n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        \n        # Log summary of combined results\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        logger.info(f\"Reduce phase complete: Combined {total_qa_pairs} Q&A pairs across {len(qa_pairs)} topics\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Orchestrator Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.topic_results:\n            error_msg = \"No topic results were generated\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Topic Orchestrator Node completed successfully\")\n        logger.info(f\"Processed {len(self.topic_results)} topics using Map-Reduce pattern\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and transcript\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"transcript\": test_transcript\n    }\n    \n    # Create and run the node\n    node = TopicOrchestratorNode(shared_memory, max_workers=2)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    \n    # Print Q&A pairs\n    qa_pairs = updated_memory.get(\"qa_pairs\", {})\n    for topic, pairs in qa_pairs.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(f\"Q&A Pairs: {len(pairs)}\")\n        for i, qa in enumerate(pairs):\n            logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n            logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    \n    # Print ELI5 content\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic ELI5: {topic}\")\n        logger.info(explanation[:200] + \"...\" if len(explanation) > 200 else explanation)\n\n\n--- File: 13 # src/nodes/topic_processor_node.py ---\n\"\"\"\nTopic Processor Node for YouTube Video Summarizer.\nProcesses a single topic with Q&A generation and ELI5 transformation.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicProcessorNode(BaseNode):\n    \"\"\"\n    Node for processing a single topic, including Q&A generation and ELI5 transformation.\n    This node is designed to be used as part of a Map-Reduce pattern.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, topic=None, transcript=None, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with topic and transcript.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary (optional)\n            topic (str): The topic to process\n            transcript (str): The video transcript\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory or {})\n        self.topic = topic\n        self.transcript = transcript\n        self.questions_per_topic = questions_per_topic\n        self.result = {\n            \"topic\": topic,\n            \"qa_pairs\": [],\n            \"eli5_content\": \"\"\n        }\n        logger.debug(f\"TopicProcessorNode initialized for topic: {topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topic and transcript are available.\n        \"\"\"\n        # If topic and transcript were not provided in constructor, try to get from shared memory\n        if self.topic is None and \"current_topic\" in self.shared_memory:\n            self.topic = self.shared_memory[\"current_topic\"]\n            \n        if self.transcript is None and \"transcript\" in self.shared_memory:\n            self.transcript = self.shared_memory[\"transcript\"]\n            \n        if self.topic is None:\n            error_msg = \"No topic provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        if self.transcript is None:\n            error_msg = \"No transcript provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        logger.info(f\"Processing topic: {self.topic}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic processing: generate Q&A pairs and ELI5 explanation.\n        \"\"\"\n        # Step 1: Generate Q&A pairs\n        self._generate_qa_pairs()\n        \n        # Step 2: Create ELI5 explanation\n        self._create_eli5_explanation()\n    \n    def _generate_qa_pairs(self):\n        \"\"\"\n        Generate Q&A pairs for the topic.\n        \"\"\"\n        logger.info(f\"Generating Q&A pairs for topic: {self.topic}\")\n        \n        # Create prompt for Q&A generation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at creating educational content for videos.\n        \n        I'll provide you with a transcript from a YouTube video and a specific topic from that video.\n        Your task is to:\n        1. Generate {self.questions_per_topic} insightful questions about this topic\n        2. Provide clear, accurate answers to each question based on the transcript\n        3. Make sure the questions cover different aspects of the topic\n        4. Ensure answers are based only on information in the transcript\n        \n        Topic: {self.topic}\n        \n        Transcript:\n        {self.transcript[:4000]}...\n        \n        Respond with ONLY a JSON array of question-answer objects. For example:\n        [\n            {{\n                \"question\": \"What is the main purpose of X?\",\n                \"answer\": \"According to the video, the main purpose of X is...\"\n            }},\n            {{\n                \"question\": \"How does Y relate to Z?\",\n                \"answer\": \"The video explains that Y and Z are connected through...\"\n            }}\n        ]\n        \"\"\")\n        \n        # Call LLM to generate Q&A pairs\n        try:\n            logger.debug(f\"Calling LLM for Q&A generation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=1000)\n            logger.debug(f\"LLM response for Q&A generation: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            elif cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                qa_pairs = json.loads(cleaned_response)\n                if isinstance(qa_pairs, list):\n                    self.result[\"qa_pairs\"] = qa_pairs\n                    logger.info(f\"Generated {len(qa_pairs)} Q&A pairs for topic '{self.topic}'\")\n                    for i, qa in enumerate(qa_pairs):\n                        logger.debug(f\"  Q{i+1}: {qa.get('question', '')}\")\n                        logger.debug(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n                else:\n                    logger.warning(f\"Expected list but got {type(qa_pairs)} from LLM\")\n                    self.result[\"qa_pairs\"] = []\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response[:200]}...\")\n                # Try to extract Q&A pairs from text response\n                extracted_qa = self._extract_qa_from_text(cleaned_response)\n                self.result[\"qa_pairs\"] = extracted_qa\n                logger.info(f\"Extracted {len(extracted_qa)} Q&A pairs from non-JSON response\")\n        except Exception as e:\n            logger.error(f\"Error generating Q&A pairs for topic '{self.topic}': {str(e)}\")\n            self.result[\"qa_pairs\"] = []\n    \n    def _extract_qa_from_text(self, text):\n        \"\"\"\n        Attempt to extract Q&A pairs from non-JSON text.\n        \n        Args:\n            text (str): Text containing Q&A pairs\n            \n        Returns:\n            list: List of Q&A pair dictionaries\n        \"\"\"\n        logger.debug(\"Attempting to extract Q&A pairs from non-JSON text\")\n        lines = text.split(\"\\n\")\n        qa_pairs = []\n        current_question = None\n        current_answer = \"\"\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line starts with Q: or Question:\n            if line.startswith((\"Q:\", \"Question:\")):\n                # If we have a previous question, save it\n                if current_question:\n                    qa_pairs.append({\n                        \"question\": current_question,\n                        \"answer\": current_answer.strip()\n                    })\n                \n                # Extract new question\n                current_question = line.split(\":\", 1)[1].strip()\n                current_answer = \"\"\n            # Check if line starts with A: or Answer:\n            elif line.startswith((\"A:\", \"Answer:\")) and current_question:\n                current_answer += line.split(\":\", 1)[1].strip() + \" \"\n            # Otherwise, add to current answer if we have a question\n            elif current_question:\n                current_answer += line + \" \"\n        \n        # Add the last Q&A pair if exists\n        if current_question:\n            qa_pairs.append({\n                \"question\": current_question,\n                \"answer\": current_answer.strip()\n            })\n            \n        logger.debug(f\"Extracted {len(qa_pairs)} Q&A pairs from text\")\n        return qa_pairs\n    \n    def _create_eli5_explanation(self):\n        \"\"\"\n        Create an ELI5 (Explain Like I'm 5) explanation for the topic.\n        \"\"\"\n        logger.info(f\"Creating ELI5 explanation for topic: {self.topic}\")\n        \n        # Get Q&A pairs for context\n        qa_text = \"\"\n        for qa in self.result[\"qa_pairs\"]:\n            question = qa.get(\"question\", \"\")\n            answer = qa.get(\"answer\", \"\")\n            qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n        \n        # Create prompt for ELI5 transformation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at explaining complex topics to young children (5-7 years old).\n        \n        I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n        Your task is to:\n        1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n        2. Use simple words, short sentences, and concrete examples\n        3. Avoid jargon and technical terms\n        4. Use analogies to familiar concepts when possible\n        5. Keep the explanation under 200 words\n        6. Maintain the core information while simplifying the language\n        \n        Topic: {self.topic}\n        \n        Q&A Context:\n        {qa_text}\n        \n        Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n        \"\"\")\n        \n        # Call LLM to generate ELI5 explanation\n        try:\n            logger.debug(f\"Calling LLM for ELI5 transformation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=500)\n            logger.debug(f\"LLM response for ELI5 transformation: {response[:100]}...\")\n            \n            # Clean up the response\n            explanation = response.strip()\n            \n            # Store the ELI5 explanation\n            self.result[\"eli5_content\"] = explanation\n            logger.info(f\"Generated ELI5 explanation for topic '{self.topic}' ({len(explanation)} characters)\")\n            \n        except Exception as e:\n            error_msg = f\"Error creating ELI5 explanation for topic '{self.topic}': {str(e)}\"\n            logger.error(error_msg)\n            self.result[\"eli5_content\"] = f\"Sorry, I couldn't create a simple explanation for {self.topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and return the results.\n        \"\"\"\n        # Store the results in shared memory if needed\n        if \"topic_results\" not in self.shared_memory:\n            self.shared_memory[\"topic_results\"] = {}\n            \n        self.shared_memory[\"topic_results\"][self.topic] = self.result\n        \n        logger.info(f\"Topic processing completed for '{self.topic}'\")\n        logger.debug(f\"Generated {len(self.result['qa_pairs'])} Q&A pairs and {len(self.result['eli5_content'])} characters of ELI5 content\")\n        \n        return self.result\n\n\nif __name__ == \"__main__\":\n    # Test with a sample topic and transcript\n    test_topic = \"Machine Learning Basics\"\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Create and run the node\n    node = TopicProcessorNode(topic=test_topic, transcript=test_transcript)\n    result = node.run()\n    \n    # Print the results\n    logger.info(\"\\nTopic Processing Results:\")\n    logger.info(f\"Topic: {result['topic']}\")\n    logger.info(f\"Q&A Pairs: {len(result['qa_pairs'])}\")\n    for i, qa in enumerate(result['qa_pairs']):\n        logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n        logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    logger.info(f\"ELI5 Explanation: {result['eli5_content']}\")\n\n\n--- File: 14 # src/utils/call_llm.py ---\n\"\"\"\nUtility function to call a Large Language Model (LLM) API.\n\"\"\"\nimport os\nimport time\nfrom openai import OpenAI\nfrom openai import OpenAIError\nimport logging\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=1000, timeout=60):\n    \"\"\"\n    Calls an LLM API with the given prompt and returns the response.\n    \n    Args:\n        prompt (str): The prompt to send to the LLM\n        model (str): The model to use (default: gpt-4o)\n        temperature (float): Controls randomness (0.0-1.0)\n        max_tokens (int): Maximum number of tokens to generate\n        timeout (int): Maximum time to wait for a response in seconds\n        \n    Returns:\n        str: The LLM's response\n    \"\"\"\n    # Get API key from environment variable\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    \n    if not api_key:\n        logging.error(\"OpenAI API key not found in environment variables\")\n        return \"Error: OpenAI API key not found in environment variables.\"\n    \n    # Log API key first few and last few characters for debugging\n    masked_key = f\"{api_key[:5]}...{api_key[-5:]}\"\n    logging.debug(f\"Using API key: {masked_key}\")\n    \n    # Initialize the OpenAI client\n    try:\n        client = OpenAI(\n            api_key=api_key\n        )\n        \n        # Make the API call with proper timeout handling\n        start_time = time.time()\n        logging.debug(f\"Starting OpenAI API call to model {model}\")\n        \n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_tokens=max_tokens,\n                timeout=timeout\n            )\n            \n            # Calculate and log response time\n            elapsed = time.time() - start_time\n            logging.debug(f\"OpenAI API call completed in {elapsed:.2f} seconds\")\n            \n            # Extract and return the response content\n            content = response.choices[0].message.content\n            logging.debug(f\"Received response of length {len(content)} characters\")\n            return content\n            \n        except Exception as api_error:\n            elapsed = time.time() - start_time\n            logging.error(f\"OpenAI API error after {elapsed:.2f} seconds: {str(api_error)}\")\n            \n            # Handle specific error types\n            error_message = str(api_error).lower()\n            if \"timeout\" in error_message:\n                return f\"Error: LLM API call timed out after {elapsed:.1f} seconds.\"\n            elif \"rate limit\" in error_message:\n                return \"Error: Rate limit exceeded. Please try again later.\"\n            elif \"invalid auth\" in error_message or \"authentication\" in error_message:\n                return \"Error: Authentication failed. Please check your API key.\"\n            else:\n                return f\"Error calling LLM API: {str(api_error)}\"\n                \n    except Exception as e:\n        logging.exception(\"Unexpected error initializing OpenAI client\")\n        return f\"Error initializing OpenAI client: {str(e)}\"\n\n\nif __name__ == \"__main__\":\n    # Configure logging for testing\n    logging.basicConfig(level=logging.DEBUG)\n    \n    # Test the function with a simple prompt\n    test_prompt = \"Explain what a YouTube video summarizer does in one sentence.\"\n    \n    print(\"Testing call_llm function...\")\n    response = call_llm(test_prompt, max_tokens=100)\n    print(f\"Prompt: {test_prompt}\")\n    print(f\"Response: {response}\")\n\n\n--- File: 15 # src/utils/extract_youtube_metadata.py ---\n\"\"\"\nUtility function to extract metadata from YouTube videos.\n\"\"\"\nimport requests\nimport json\nimport os\nfrom datetime import datetime\n\ndef extract_youtube_metadata(video_id):\n    \"\"\"\n    Extracts metadata from a YouTube video using the YouTube Data API.\n    \n    Args:\n        video_id (str): The YouTube video ID\n        \n    Returns:\n        dict: A dictionary containing video metadata (title, channel, duration, etc.)\n    \"\"\"\n    # In a production environment, you would use an API key from environment variables\n    # For this example, we'll use a placeholder\n    # You would need to get your own API key from the Google Developer Console\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    \n    # Base URL for YouTube Data API v3\n    base_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    \n    # Parameters for the API request\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\",\n        \"id\": video_id,\n        \"key\": api_key\n    }\n    \n    try:\n        # Make the API request\n        response = requests.get(base_url, params=params)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Check if the video exists\n        if not data.get(\"items\"):\n            return {\n                \"error\": \"Video not found or API key invalid\",\n                \"video_id\": video_id\n            }\n        \n        # Extract relevant metadata\n        video_data = data[\"items\"][0]\n        snippet = video_data.get(\"snippet\", {})\n        content_details = video_data.get(\"contentDetails\", {})\n        statistics = video_data.get(\"statistics\", {})\n        \n        # Format the duration (ISO 8601 format to human-readable)\n        duration_iso = content_details.get(\"duration\", \"PT0S\")\n        # Simple conversion for common formats (not handling all ISO 8601 cases)\n        hours = 0\n        minutes = 0\n        seconds = 0\n        \n        if \"H\" in duration_iso:\n            hours = int(duration_iso.split(\"H\")[0].split(\"PT\")[1])\n            duration_iso = duration_iso.split(\"H\")[1]\n        elif \"PT\" in duration_iso:\n            duration_iso = duration_iso.split(\"PT\")[1]\n            \n        if \"M\" in duration_iso:\n            minutes = int(duration_iso.split(\"M\")[0])\n            duration_iso = duration_iso.split(\"M\")[1]\n            \n        if \"S\" in duration_iso:\n            seconds = int(duration_iso.split(\"S\")[0])\n        \n        duration_str = \"\"\n        if hours > 0:\n            duration_str += f\"{hours} hour{'s' if hours > 1 else ''} \"\n        if minutes > 0:\n            duration_str += f\"{minutes} minute{'s' if minutes > 1 else ''} \"\n        if seconds > 0 or (hours == 0 and minutes == 0):\n            duration_str += f\"{seconds} second{'s' if seconds > 1 else ''}\"\n        \n        # Format the published date\n        published_at = snippet.get(\"publishedAt\", \"\")\n        if published_at:\n            try:\n                published_date = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n                published_at = published_date.strftime(\"%B %d, %Y\")\n            except ValueError:\n                pass\n        \n        # Construct the metadata dictionary\n        metadata = {\n            \"video_id\": video_id,\n            \"title\": snippet.get(\"title\", \"Unknown Title\"),\n            \"description\": snippet.get(\"description\", \"\"),\n            \"channel_name\": snippet.get(\"channelTitle\", \"Unknown Channel\"),\n            \"channel_id\": snippet.get(\"channelId\", \"\"),\n            \"published_at\": published_at,\n            \"duration\": duration_str,\n            \"duration_seconds\": hours * 3600 + minutes * 60 + seconds,\n            \"view_count\": int(statistics.get(\"viewCount\", 0)),\n            \"like_count\": int(statistics.get(\"likeCount\", 0)),\n            \"comment_count\": int(statistics.get(\"commentCount\", 0)),\n            \"thumbnail_url\": snippet.get(\"thumbnails\", {}).get(\"high\", {}).get(\"url\", \"\"),\n            \"tags\": snippet.get(\"tags\", []),\n            \"category_id\": snippet.get(\"categoryId\", \"\"),\n        }\n        \n        return metadata\n        \n    except requests.exceptions.RequestException as e:\n        # Handle request errors\n        return {\n            \"error\": f\"API request error: {str(e)}\",\n            \"video_id\": video_id\n        }\n    except (KeyError, ValueError, json.JSONDecodeError) as e:\n        # Handle parsing errors\n        return {\n            \"error\": f\"Data parsing error: {str(e)}\",\n            \"video_id\": video_id\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the function with an example video ID\n    test_video_id = \"dQw4w9WgXcQ\"  # Rick Astley - Never Gonna Give You Up\n    metadata = extract_youtube_metadata(test_video_id)\n    \n    # Print the metadata in a readable format\n    print(json.dumps(metadata, indent=2))\n\n\n--- File: 16 # src/utils/extract_youtube_transcript.py ---\n\"\"\"\nUtility function to extract transcript from YouTube videos.\n\"\"\"\nimport os\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    \"\"\"\n    Extracts the transcript/captions from a YouTube video.\n    \n    Args:\n        video_id (str): The YouTube video ID\n        \n    Returns:\n        str: The full transcript text or an error message\n    \"\"\"\n    try:\n        # Get available transcripts\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        \n        # Try to get English transcript first, fall back to other languages if needed\n        try:\n            transcript = transcript_list.find_transcript(['en'])\n        except NoTranscriptFound:\n            # If no English transcript is found, use the first available one\n            transcript = transcript_list.find_transcript([])\n            \n            # If it's not in English, try to translate it\n            if transcript.language_code != 'en':\n                transcript = transcript.translate('en')\n        \n        # Fetch the transcript data\n        transcript_data = transcript.fetch()\n        \n        # Combine all transcript segments into a single text\n        full_transcript = \"\"\n        for segment in transcript_data:\n            # The segment is a dictionary with 'text', 'start', and 'duration' keys\n            if isinstance(segment, dict) and 'text' in segment:\n                text = segment['text']\n            else:\n                # Handle FetchedTranscriptSnippet objects\n                text = segment.text if hasattr(segment, 'text') else str(segment)\n                \n            # Add a space if the text doesn't end with punctuation\n            if text and text[-1] not in '.!?':\n                text += ' '\n            full_transcript += text\n        \n        return full_transcript.strip()\n        \n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n\n\nif __name__ == \"__main__\":\n    # Test the function with an example video ID\n    test_video_id = \"dQw4w9WgXcQ\"  # Rick Astley - Never Gonna Give You Up\n    transcript = extract_youtube_transcript(test_video_id)\n    \n    # Print the first 500 characters of the transcript\n    print(transcript[:500] + \"...\" if len(transcript) > 500 else transcript)\n\n\n--- File: 17 # src/utils/generate_html.py ---\n\"\"\"\nUtility function to generate HTML output for the YouTube video summary.\n\"\"\"\nimport html\n\ndef generate_html(summary_data):\n    \"\"\"\n    Generates an HTML page to visualize the YouTube video summary.\n    \n    Args:\n        summary_data (dict): A dictionary containing all the processed information\n            - video_id (str): YouTube video ID\n            - metadata (dict): Video metadata\n            - topics (list): List of extracted topics\n            - qa_pairs (dict): Q&A pairs organized by topic\n            - eli5_content (dict): Child-friendly explanations\n            \n    Returns:\n        str: HTML content as a string\n    \"\"\"\n    if not summary_data:\n        return \"<html><body><h1>Error: No summary data provided</h1></body></html>\"\n    \n    video_id = summary_data.get(\"video_id\", \"\")\n    metadata = summary_data.get(\"metadata\", {})\n    topics = summary_data.get(\"topics\", [])\n    qa_pairs = summary_data.get(\"qa_pairs\", {})\n    eli5_content = summary_data.get(\"eli5_content\", {})\n    \n    # Escape HTML special characters to prevent XSS\n    title = html.escape(metadata.get(\"title\", \"YouTube Video Summary\"))\n    channel = html.escape(metadata.get(\"channel_name\", \"Unknown Channel\"))\n    duration = html.escape(metadata.get(\"duration\", \"\"))\n    published_at = html.escape(metadata.get(\"published_at\", \"\"))\n    thumbnail_url = metadata.get(\"thumbnail_url\", \"\")\n    \n    # Generate the HTML content\n    html_content = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{title} - Summary</title>\n    <style>\n        :root {{\n            --primary-color: #ff5252;\n            --secondary-color: #3f51b5;\n            --background-color: #f9f9f9;\n            --card-color: #ffffff;\n            --text-color: #333333;\n            --border-radius: 10px;\n            --box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n        }}\n        \n        body {{\n            font-family: 'Comic Sans MS', 'Chalkboard SE', 'Arial', sans-serif;\n            background-color: var(--background-color);\n            color: var(--text-color);\n            line-height: 1.6;\n            margin: 0;\n            padding: 0;\n        }}\n        \n        .container {{\n            max-width: 1000px;\n            margin: 0 auto;\n            padding: 20px;\n        }}\n        \n        header {{\n            text-align: center;\n            margin-bottom: 30px;\n        }}\n        \n        h1 {{\n            color: var(--primary-color);\n            font-size: 2.5rem;\n            margin-bottom: 10px;\n        }}\n        \n        h2 {{\n            color: var(--secondary-color);\n            font-size: 1.8rem;\n            border-bottom: 2px solid var(--secondary-color);\n            padding-bottom: 10px;\n            margin-top: 30px;\n        }}\n        \n        h3 {{\n            color: var(--primary-color);\n            font-size: 1.5rem;\n        }}\n        \n        .video-info {{\n            display: flex;\n            align-items: center;\n            background-color: var(--card-color);\n            border-radius: var(--border-radius);\n            box-shadow: var(--box-shadow);\n            padding: 20px;\n            margin-bottom: 30px;\n        }}\n        \n        .video-thumbnail {{\n            flex: 0 0 280px;\n            margin-right: 20px;\n        }}\n        \n        .video-thumbnail img {{\n            width: 100%;\n            border-radius: var(--border-radius);\n        }}\n        \n        .video-details {{\n            flex: 1;\n        }}\n        \n        .topic-card {{\n            background-color: var(--card-color);\n            border-radius: var(--border-radius);\n            box-shadow: var(--box-shadow);\n            padding: 20px;\n            margin-bottom: 20px;\n        }}\n        \n        .qa-section {{\n            background-color: rgba(63, 81, 181, 0.1);\n            border-radius: var(--border-radius);\n            padding: 15px;\n            margin-top: 15px;\n        }}\n        \n        .question {{\n            font-weight: bold;\n            color: var(--secondary-color);\n            margin-bottom: 5px;\n        }}\n        \n        .answer {{\n            margin-bottom: 15px;\n            padding-left: 15px;\n            border-left: 3px solid var(--primary-color);\n        }}\n        \n        footer {{\n            text-align: center;\n            margin-top: 50px;\n            padding: 20px;\n            font-size: 0.9rem;\n            color: #666;\n        }}\n        \n        @media (max-width: 768px) {{\n            .video-info {{\n                flex-direction: column;\n            }}\n            \n            .video-thumbnail {{\n                margin-right: 0;\n                margin-bottom: 20px;\n                width: 100%;\n            }}\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <header>\n            <h1>\ud83c\udfac Kid-Friendly Video Summary \ud83c\udfac</h1>\n            <p>Here's a simple explanation of what this video is all about!</p>\n        </header>\n        \n        <div class=\"video-info\">\n            <div class=\"video-thumbnail\">\n                <img src=\"{thumbnail_url}\" alt=\"{title}\" onerror=\"this.src='https://via.placeholder.com/280x158?text=No+Thumbnail'\">\n                <p style=\"text-align: center; margin-top: 10px;\">\n                    <a href=\"https://www.youtube.com/watch?v={video_id}\" target=\"_blank\" style=\"text-decoration: none; color: var(--primary-color);\">\n                        Watch on YouTube\n                    </a>\n                </p>\n            </div>\n            <div class=\"video-details\">\n                <h2>{title}</h2>\n                <p><strong>Channel:</strong> {channel}</p>\n                <p><strong>Duration:</strong> {duration}</p>\n                <p><strong>Published:</strong> {published_at}</p>\n            </div>\n        </div>\n\"\"\"\n    \n    # Add topics and their content\n    for i, topic in enumerate(topics):\n        topic_escaped = html.escape(topic)\n        topic_id = f\"topic-{i+1}\"\n        \n        # Get ELI5 explanation for this topic\n        eli5_explanation = html.escape(eli5_content.get(topic, \"\"))\n        \n        # Get Q&A pairs for this topic\n        topic_qa_pairs = qa_pairs.get(topic, [])\n        \n        html_content += f\"\"\"\n        <h2 id=\"{topic_id}\">Topic {i+1}: {topic_escaped}</h2>\n        <div class=\"topic-card\">\n            <p>{eli5_explanation}</p>\n            \n            <div class=\"qa-section\">\n                <h3>Questions & Answers</h3>\n\"\"\"\n        \n        # Add Q&A pairs\n        if topic_qa_pairs:\n            for qa in topic_qa_pairs:\n                question = html.escape(qa.get(\"question\", \"\"))\n                answer = html.escape(qa.get(\"answer\", \"\"))\n                \n                html_content += f\"\"\"\n                <div class=\"question\">Q: {question}</div>\n                <div class=\"answer\">A: {answer}</div>\n\"\"\"\n        else:\n            html_content += \"\"\"\n                <p>No questions available for this topic.</p>\n\"\"\"\n        \n        html_content += \"\"\"\n            </div>\n        </div>\n\"\"\"\n    \n    # Close the HTML document\n    html_content += \"\"\"\n        <footer>\n            <p>This summary was created by YouTube Video Summarizer</p>\n        </footer>\n    </div>\n</body>\n</html>\n\"\"\"\n    \n    return html_content\n\n\nif __name__ == \"__main__\":\n    # Test the function with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    html_output = generate_html(test_data)\n    \n    # Save the HTML to a file for testing\n    with open(\"test_summary.html\", \"w\", encoding=\"utf-8\") as f:\n        f.write(html_output)\n    \n    print(\"HTML generated and saved to test_summary.html\")\n\n\n--- File: 18 # src/utils/logger.py ---\n\"\"\"\nLogging module for YouTube Video Summarizer.\n\"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom loguru import logger\n\ndef setup_logger(log_dir=\"logs\"):\n    \"\"\"\n    Set up the logger with appropriate configuration.\n    \n    Args:\n        log_dir (str): Directory to store log files\n        \n    Returns:\n        logger: Configured logger instance\n    \"\"\"\n    # Create logs directory if it doesn't exist\n    os.makedirs(log_dir, exist_ok=True)\n    \n    # Generate timestamp for the log file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(log_dir, f\"youtube_summarizer_{timestamp}.log\")\n    \n    # Remove any existing handlers\n    logger.remove()\n    \n    # Add console handler with color\n    logger.add(\n        sys.stdout,\n        format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\",\n        level=\"INFO\",\n        colorize=True\n    )\n    \n    # Add file handler\n    logger.add(\n        log_file,\n        format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}\",\n        level=\"DEBUG\",\n        rotation=\"20 MB\",\n        retention=\"1 week\"\n    )\n    \n    logger.info(f\"Logging initialized. Log file: {log_file}\")\n    return logger\n\n# Initialize logger\nlogger = setup_logger()\n\nif __name__ == \"__main__\":\n    # Test the logger\n    logger.debug(\"This is a debug message\")\n    logger.info(\"This is an info message\")\n    logger.warning(\"This is a warning message\")\n    logger.error(\"This is an error message\")\n    logger.critical(\"This is a critical message\")\n    \n    print(f\"Log file created in the logs directory\")\n\n\n--- File: 19 # src/utils/validate_youtube_url.py ---\n\"\"\"\nUtility function to validate YouTube URLs and extract video IDs.\n\"\"\"\nimport re\nfrom urllib.parse import urlparse, parse_qs\n\ndef validate_youtube_url(url):\n    \"\"\"\n    Validates if the given URL is a valid YouTube video URL and extracts the video ID.\n    \n    Args:\n        url (str): The YouTube URL to validate\n        \n    Returns:\n        tuple: (is_valid (bool), video_id (str))\n    \"\"\"\n    if not url:\n        return False, \"\"\n    \n    # Regular expression patterns for different YouTube URL formats\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})'\n    )\n    \n    youtube_match = re.match(youtube_regex, url)\n    \n    if youtube_match:\n        video_id = youtube_match.group(6)\n        return True, video_id\n    \n    # Handle youtu.be format\n    parsed_url = urlparse(url)\n    if parsed_url.netloc == 'youtu.be':\n        video_id = parsed_url.path.lstrip('/')\n        if len(video_id) == 11:\n            return True, video_id\n    \n    # Handle youtube.com/watch?v= format\n    if parsed_url.netloc in ('youtube.com', 'www.youtube.com'):\n        query_params = parse_qs(parsed_url.query)\n        if 'v' in query_params:\n            video_id = query_params['v'][0]\n            if len(video_id) == 11:\n                return True, video_id\n    \n    return False, \"\"\n\n\nif __name__ == \"__main__\":\n    # Test the function with some example URLs\n    test_urls = [\n        \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n        \"https://youtu.be/dQw4w9WgXcQ\",\n        \"https://youtube.com/embed/dQw4w9WgXcQ\",\n        \"https://www.youtube.com/shorts/dQw4w9WgXcQ\",\n        \"https://www.notayoutubeurl.com/watch?v=dQw4w9WgXcQ\",\n        \"invalid_url\"\n    ]\n    \n    for url in test_urls:\n        is_valid, video_id = validate_youtube_url(url)\n        print(f\"URL: {url}\")\n        print(f\"Valid: {is_valid}\")\n        print(f\"Video ID: {video_id}\")\n        print(\"-\" * 40)\n\n\nPlease provide:\n1. A high-level `summary` of the project's main purpose and functionality in a few beginner-friendly sentences. Use markdown formatting with **bold** and *italic* text to highlight important concepts.\n2. A list (`relationships`) describing the key interactions between these abstractions. For each relationship, specify:\n    - `from_abstraction`: Index of the source abstraction (e.g., `0 # AbstractionName1`)\n    - `to_abstraction`: Index of the target abstraction (e.g., `1 # AbstractionName2`)\n    - `label`: A brief label for the interaction **in just a few words** (e.g., \"Manages\", \"Inherits\", \"Uses\").\n    Ideally the relationship should be backed by one abstraction calling or passing parameters to another.\n    Simplify the relationship and exclude those non-important ones.\n\nIMPORTANT: Make sure EVERY abstraction is involved in at least ONE relationship (either as source or target). Each abstraction index must appear at least once across all relationships.\n\nFormat the output as YAML:\n\n```yaml\nsummary: |\n  A brief, simple explanation of the project.\n  Can span multiple lines with **bold** and *italic* for emphasis.\nrelationships:\n  - from_abstraction: 0 # AbstractionName1\n    to_abstraction: 1 # AbstractionName2\n    label: \"Manages\"\n  - from_abstraction: 2 # AbstractionName3\n    to_abstraction: 0 # AbstractionName1\n    label: \"Provides config\"\n  # ... other relationships\n```\n\nNow, provide the YAML output:\n": "```yaml\nsummary: |\n  This project acts like an automatic **report writer** for YouTube videos.\n  You give it a *YouTube video link*, and it watches the video (by reading its *transcript*) to understand the main points.\n  It uses **Artificial Intelligence (AI)** to identify key *topics*, create simple **question & answer pairs** about them, and even write an **\"Explain Like I'm 5\"** summary for each topic.\n  Finally, it neatly organizes all this information into a web page (**HTML report**) for easy reading.\nrelationships:\n  - from_abstraction: 0 # Pipeline Orchestration\n    to_abstraction: 1 # Node (Pipeline Step)\n    label: \"Executes Nodes\"\n  - from_abstraction: 0 # Pipeline Orchestration\n    to_abstraction: 2 # Shared Memory\n    label: \"Initializes & Passes Data via\"\n  - from_abstraction: 0 # Pipeline Orchestration\n    to_abstraction: 3 # Topic Processing Orchestrator\n    label: \"Calls Specific Node\"\n  - from_abstraction: 1 # Node (Pipeline Step)\n    to_abstraction: 2 # Shared Memory\n    label: \"Reads/Writes Data\"\n  - from_abstraction: 1 # Node (Pipeline Step)\n    to_abstraction: 4 # LLM Utility\n    label: \"Uses for AI Tasks\"\n  - from_abstraction: 1 # Node (Pipeline Step)\n    to_abstraction: 5 # YouTube Data Utilities\n    label: \"Uses for Video Data\"\n  - from_abstraction: 1 # Node (Pipeline Step)\n    to_abstraction: 6 # HTML Generation\n    label: \"Uses for Report Creation\"\n  - from_abstraction: 3 # Topic Processing Orchestrator\n    to_abstraction: 1 # Node (Pipeline Step)\n    label: \"Manages Worker Nodes\"\n  - from_abstraction: 0 # Pipeline Orchestration\n    to_abstraction: 7 # Logging\n    label: \"Uses for Overall Logging\"\n```", "\nGiven the following project abstractions and their relationships for the project ```` pf1 ````:\n\nAbstractions (Index # Name):\n- 0 # Pipeline Orchestration\n- 1 # Node (Pipeline Step)\n- 2 # Shared Memory\n- 3 # Topic Processing Orchestrator\n- 4 # LLM Utility\n- 5 # YouTube Data Utilities\n- 6 # HTML Generation\n- 7 # Logging\n\nContext about relationships and project summary:\nProject Summary:\nThis project acts like an automatic **report writer** for YouTube videos.\nYou give it a *YouTube video link*, and it watches the video (by reading its *transcript*) to understand the main points.\nIt uses **Artificial Intelligence (AI)** to identify key *topics*, create simple **question & answer pairs** about them, and even write an **\"Explain Like I'm 5\"** summary for each topic.\nFinally, it neatly organizes all this information into a web page (**HTML report**) for easy reading.\n\n\nRelationships (Indices refer to abstractions above):\n- From 0 (Pipeline Orchestration) to 1 (Node (Pipeline Step)): Executes Nodes\n- From 0 (Pipeline Orchestration) to 2 (Shared Memory): Initializes & Passes Data via\n- From 0 (Pipeline Orchestration) to 3 (Topic Processing Orchestrator): Calls Specific Node\n- From 1 (Node (Pipeline Step)) to 2 (Shared Memory): Reads/Writes Data\n- From 1 (Node (Pipeline Step)) to 4 (LLM Utility): Uses for AI Tasks\n- From 1 (Node (Pipeline Step)) to 5 (YouTube Data Utilities): Uses for Video Data\n- From 1 (Node (Pipeline Step)) to 6 (HTML Generation): Uses for Report Creation\n- From 3 (Topic Processing Orchestrator) to 1 (Node (Pipeline Step)): Manages Worker Nodes\n- From 0 (Pipeline Orchestration) to 7 (Logging): Uses for Overall Logging\n\n\nIf you are going to make a tutorial for ```` pf1 ````, what is the best order to explain these abstractions, from first to last?\nIdeally, first explain those that are the most important or foundational, perhaps user-facing concepts or entry points. Then move to more detailed, lower-level implementation details or supporting concepts.\n\nOutput the ordered list of abstraction indices, including the name in a comment for clarity. Use the format `idx # AbstractionName`.\n\n```yaml\n- 2 # FoundationalConcept\n- 0 # CoreClassA\n- 1 # CoreClassB (uses CoreClassA)\n- ...\n```\n\nNow, provide the YAML output:\n": "```yaml\n- 0 # Pipeline Orchestration\n- 1 # Node (Pipeline Step)\n- 2 # Shared Memory\n- 3 # Topic Processing Orchestrator\n- 5 # YouTube Data Utilities\n- 4 # LLM Utility\n- 6 # HTML Generation\n- 7 # Logging\n```", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"Pipeline Orchestration\". This is Chapter 1.\n\nConcept Details:\n- Description:\nThis manages the overall flow of the YouTube summarization process. It defines the sequence of steps (Nodes) that need to run to transform a YouTube URL into a summary.\nThink of it like a project manager for the summarization task, ensuring each specialist (Node) does their job in the right order, starting with the URL and ending with the HTML report.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\nThis is the first chapter.\n\nRelevant Code Snippets:\n--- File: src/main.py ---\n\"\"\"\nYouTube Video Summarizer - Main Application\n\"\"\"\nimport os\nimport sys\nimport argparse\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Add the project root to the path so we can import from src\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom src.nodes.input_processing_node import InputProcessingNode\nfrom src.nodes.content_extraction_node import ContentExtractionNode\nfrom src.nodes.topic_extraction_node import TopicExtractionNode\nfrom src.nodes.topic_orchestrator_node import TopicOrchestratorNode\nfrom src.nodes.html_generation_node import HTMLGenerationNode\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", enable_chunking=False, max_workers=3):\n    \"\"\"\n    Run the complete YouTube video summarization pipeline.\n    \n    Args:\n        youtube_url (str): URL of the YouTube video to summarize\n        output_dir (str): Directory to save the output HTML file\n        enable_chunking (bool): Whether to enable transcript chunking\n        max_workers (int): Maximum number of parallel workers for topic processing\n    \"\"\"\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"YouTube Video Summarizer\")\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n    logger.info(f\"Chunking: {'Enabled' if enable_chunking else 'Disabled'}\")\n    logger.info(f\"Parallel Workers: {max_workers}\")\n    logger.info(f\"{'='*60}\")\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": youtube_url}\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    logger.debug(f\"Output directory created/confirmed: {output_dir}\")\n    \n    # Configure chunking parameters if enabled\n    chunk_size = 4000 if enable_chunking else 0\n    overlap = 500 if enable_chunking else 0\n    \n    try:\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        input_node = InputProcessingNode(shared_memory)\n        shared_memory = input_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Input Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        logger.info(f\"Successfully processed video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        \n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Content Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        transcript_length = len(shared_memory.get('transcript', ''))\n        logger.info(f\"Successfully extracted transcript ({transcript_length} characters)\")\n        \n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, chunk_size=chunk_size, overlap=overlap)\n        shared_memory = topic_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        topics = shared_memory.get('topics', [])\n        logger.info(f\"Successfully extracted {len(topics)} topics\")\n        for i, topic in enumerate(topics):\n            logger.info(f\"  Topic {i+1}: {topic}\")\n        \n        # 4. Topic Processing Orchestrator Node\n        logger.info(\"[4/5] Starting Topic Processing...\")\n        orchestrator_node = TopicOrchestratorNode(shared_memory, max_workers=max_workers, questions_per_topic=3)\n        shared_memory = orchestrator_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        qa_pairs = shared_memory.get('qa_pairs', {})\n        eli5_content = shared_memory.get('eli5_content', {})\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        \n        logger.info(f\"Successfully processed {len(topics)} topics\")\n        logger.info(f\"Generated {total_qa_pairs} Q&A pairs and {len(eli5_content)} ELI5 explanations\")\n        \n        # 5. HTML Generation Node\n        logger.info(\"[5/5] Starting HTML Generation...\")\n        # Create a timestamped filename\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        video_id = shared_memory.get(\"video_id\", \"unknown\")\n        output_file = os.path.join(output_dir, f\"summary_{video_id}_{timestamp}.html\")\n        \n        html_node = HTMLGenerationNode(shared_memory, output_file)\n        shared_memory = html_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"HTML Generation failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        # Print summary\n        logger.info(f\"{'='*60}\")\n        logger.info(\"Summary Generation Complete!\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        logger.info(f\"Topics: {len(shared_memory.get('topics', []))}\")\n        logger.info(f\"Q&A Pairs: {sum(len(pairs) for pairs in shared_memory.get('qa_pairs', {}).values())}\")\n        logger.info(f\"Output File: {output_file}\")\n        logger.info(f\"{'='*60}\")\n        \n        return shared_memory\n        \n    except Exception as e:\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n\ndef main():\n    \"\"\"\n    Main entry point for the application.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"YouTube Video Summarizer\")\n    parser.add_argument(\"url\", nargs='?', default=None, help=\"YouTube video URL to summarize\")\n    parser.add_argument(\"--output\", \"-o\", default=\"output\", help=\"Output directory for HTML summary\")\n    parser.add_argument(\"--chunk\", action=\"store_true\", help=\"Enable transcript chunking for long videos\")\n    parser.add_argument(\"--workers\", \"-w\", type=int, default=3, help=\"Number of parallel workers for topic processing\")\n    \n    args = parser.parse_args()\n    \n    # If no URL is provided as a command-line argument, ask for user input\n    youtube_url = args.url\n    if not youtube_url:\n        youtube_url = input(\"Please enter a YouTube URL to summarize: \")\n    \n    logger.info(f\"Starting YouTube Video Summarizer with URL: {youtube_url}\")\n    \n    # Run the pipeline\n    run_pipeline(youtube_url, args.output, args.chunk, args.workers)\n\nif __name__ == \"__main__\":\n    main()\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 1: Pipeline Orchestration`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"Node (Pipeline Step)\". This is Chapter 2.\n\nConcept Details:\n- Description:\nA Node represents a single, self-contained step within the summarization pipeline. Each Node performs a specific task, such as extracting the transcript, identifying topics, or generating Q&A pairs. They inherit from a base class that defines a standard structure (`prep`, `exec`, `post`).\nIt's like a specialized worker on an assembly line. One worker validates the input, another extracts parts, another processes those parts, and a final one assembles the output.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n\nRelevant Code Snippets:\n--- File: src/nodes/base_node.py ---\n\"\"\"\nBase node class for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom abc import ABC, abstractmethod\nfrom src.utils.logger import logger\n\nclass BaseNode(ABC):\n    \"\"\"\n    Abstract base class for all nodes in the YouTube Video Summarizer.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary for data exchange between nodes\n        \"\"\"\n        self.shared_memory = shared_memory or {}\n        self.node_name = self.__class__.__name__\n        logger.debug(f\"{self.node_name} initialized\")\n    \n    @abstractmethod\n    def prep(self):\n        \"\"\"\n        Prepare for execution by reading necessary data from shared memory.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def exec(self):\n        \"\"\"\n        Execute the node's main functionality.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def post(self):\n        \"\"\"\n        Post-process and write results to shared memory.\n        \"\"\"\n        pass\n    \n    def run(self):\n        \"\"\"\n        Run the complete node workflow: prep, exec, post.\n        \n        Returns:\n            dict: The updated shared memory\n        \"\"\"\n        logger.debug(f\"{self.node_name} starting run\")\n        try:\n            self.prep()\n            if \"error\" not in self.shared_memory:\n                self.exec()\n            if \"error\" not in self.shared_memory:\n                self.post()\n            \n            if \"error\" in self.shared_memory:\n                logger.error(f\"{self.node_name} failed: {self.shared_memory['error']}\")\n            else:\n                logger.debug(f\"{self.node_name} completed successfully\")\n                \n        except Exception as e:\n            logger.exception(f\"Unexpected error in {self.node_name}: {str(e)}\")\n            self.shared_memory[\"error\"] = f\"{self.node_name} error: {str(e)}\"\n            \n        return self.shared_memory\n\n\n--- File: src/nodes/content_extraction_node.py ---\n\"\"\"\nContent Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\nfrom src.utils.logger import logger\n\nclass ContentExtractionNode(BaseNode):\n    \"\"\"\n    Node for extracting transcript from a YouTube video.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_id exists in shared memory.\n        \"\"\"\n        if \"video_id\" not in self.shared_memory:\n            error_msg = \"YouTube video ID not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Content Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(f\"Extracting transcript for video ID: {self.shared_memory['video_id']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute transcript extraction.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Extract the transcript\n        video_id = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Calling YouTube transcript API for video ID: {video_id}\")\n        transcript = extract_youtube_transcript(video_id)\n        \n        # Check if there was an error\n        if transcript.startswith(\"Error:\"):\n            logger.error(f\"Transcript extraction failed: {transcript}\")\n            self.shared_memory[\"error\"] = transcript\n            return\n        \n        self.shared_memory[\"transcript\"] = transcript\n        \n        # Log a preview of the transcript\n        preview_length = min(150, len(transcript))\n        logger.info(f\"Extracted transcript ({len(transcript)} characters)\")\n        logger.debug(f\"Transcript preview: {transcript[:preview_length]}...\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Content Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Failed to extract transcript\"\n            logger.error(f\"Error: {error_msg}\")\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Content Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample video ID\n    test_video_id = \"dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_id\": test_video_id}\n    \n    # Create and run the node\n    node = ContentExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    if \"error\" in updated_memory:\n        logger.error(f\"Error: {updated_memory['error']}\")\n    else:\n        transcript = updated_memory.get(\"transcript\", \"\")\n        logger.info(f\"Transcript length: {len(transcript)} characters\")\n        logger.info(f\"Transcript preview: {transcript[:200]}...\")\n\n\n--- File: src/nodes/eli5_transformation_node.py ---\n\"\"\"\nELI5 (Explain Like I'm 5) Transformation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass ELI5TransformationNode(BaseNode):\n    \"\"\"\n    Node for transforming content into child-friendly explanations (ELI5).\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n        self.eli5_content = {}\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and qa_pairs exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"qa_pairs\" not in self.shared_memory:\n            error_msg = \"Q&A pairs not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping ELI5 Transformation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        topics_count = len(self.shared_memory['topics'])\n        logger.info(f\"Transforming content into child-friendly explanations for {topics_count} topics\")\n        logger.debug(f\"Topics: {self.shared_memory['topics']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute ELI5 transformation for each topic and its Q&A pairs.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        topics = self.shared_memory[\"topics\"]\n        qa_pairs = self.shared_memory[\"qa_pairs\"]\n        \n        # Process each topic to create ELI5 explanations\n        for i, topic in enumerate(topics):\n            logger.info(f\"Creating ELI5 explanation for topic {i+1}/{len(topics)}: {topic}\")\n            \n            # Get Q&A pairs for this topic\n            topic_qa_pairs = qa_pairs.get(topic, [])\n            logger.debug(f\"Found {len(topic_qa_pairs)} Q&A pairs for topic '{topic}'\")\n            \n            # Combine Q&A pairs into a single text for context\n            qa_text = \"\"\n            for qa in topic_qa_pairs:\n                question = qa.get(\"question\", \"\")\n                answer = qa.get(\"answer\", \"\")\n                qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n            \n            # Create prompt for ELI5 transformation\n            prompt = textwrap.dedent(f\"\"\"\n            You are an expert at explaining complex topics to young children (5-7 years old).\n            \n            I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n            Your task is to:\n            1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n            2. Use simple words, short sentences, and concrete examples\n            3. Avoid jargon and technical terms\n            4. Use analogies to familiar concepts when possible\n            5. Keep the explanation under 200 words\n            6. Maintain the core information while simplifying the language\n            \n            Topic: {topic}\n            \n            Q&A Context:\n            {qa_text}\n            \n            Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n            \"\"\")\n            \n            # Call LLM to generate ELI5 explanation\n            try:\n                logger.debug(f\"Calling LLM for topic '{topic}' ELI5 transformation\")\n                response = call_llm(prompt, temperature=0.7, max_tokens=500)\n                logger.debug(f\"LLM response for topic '{topic}' ELI5: {response[:100]}...\")\n                \n                # Clean up the response\n                explanation = response.strip()\n                \n                # Store the ELI5 explanation\n                self.eli5_content[topic] = explanation\n                logger.info(f\"Generated ELI5 explanation for topic '{topic}' ({len(explanation)} characters)\")\n                \n            except Exception as e:\n                error_msg = f\"Error calling LLM for topic '{topic}': {str(e)}\"\n                logger.error(error_msg)\n                self.eli5_content[topic] = f\"Sorry, I couldn't create a simple explanation for {topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and store the ELI5 explanations in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in ELI5 Transformation Node: {self.shared_memory['error']}\")\n            return\n        \n        # Store the ELI5 explanations in shared memory\n        self.shared_memory[\"eli5_content\"] = self.eli5_content\n        \n        # Print a summary of the generated ELI5 explanations\n        logger.info(f\"Generated ELI5 explanations for {len(self.eli5_content)} topics\")\n        \n        for topic, explanation in self.eli5_content.items():\n            preview = explanation[:100] + \"...\" if len(explanation) > 100 else explanation\n            logger.info(f\"  Topic '{topic}':\")\n            logger.info(f\"    {preview}\")\n        \n        logger.info(\"ELI5 Transformation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and Q&A pairs\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_qa_pairs = {\n        \"Machine Learning Basics\": [\n            {\n                \"question\": \"What is machine learning?\",\n                \"answer\": \"Machine learning is a subset of artificial intelligence that focuses on building systems that learn from data. Unlike traditional programming where you explicitly program rules, in machine learning, you train models on data and they learn patterns.\"\n            },\n            {\n                \"question\": \"Why is machine learning important?\",\n                \"answer\": \"Machine learning is important because it allows computers to find insights and make predictions without being explicitly programmed. It can handle complex tasks that would be difficult to code manually.\"\n            }\n        ],\n        \"Types of Machine Learning\": [\n            {\n                \"question\": \"What is supervised learning?\",\n                \"answer\": \"Supervised learning is a type of machine learning where the model is trained on labeled data. For example, you might have images labeled as 'cat' or 'dog' and the model learns to distinguish between them.\"\n            },\n            {\n                \"question\": \"What is unsupervised learning?\",\n                \"answer\": \"Unsupervised learning is where the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task where the algorithm groups similar data points together.\"\n            }\n        ]\n    }\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"qa_pairs\": test_qa_pairs\n    }\n    \n    # Create and run the node\n    node = ELI5TransformationNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(explanation)\n\n\n--- File: src/nodes/html_generation_node.py ---\n\"\"\"\nHTML Generation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.generate_html import generate_html\nfrom src.utils.logger import logger\n\nclass HTMLGenerationNode(BaseNode):\n    \"\"\"\n    Node for generating HTML visualization of the video summary.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, output_path=None):\n        \"\"\"\n        Initialize the node with shared memory and output path.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            output_path (str): Path to save the HTML output file\n        \"\"\"\n        super().__init__(shared_memory)\n        self.output_path = output_path\n        logger.debug(f\"HTMLGenerationNode initialized with output_path={output_path}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if all required data exists in shared memory.\n        \"\"\"\n        required_keys = [\"video_id\", \"metadata\", \"topics\", \"qa_pairs\", \"eli5_content\"]\n        missing_keys = [key for key in required_keys if key not in self.shared_memory]\n        \n        if missing_keys:\n            error_msg = f\"Missing required data in shared memory: {', '.join(missing_keys)}\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping HTML Generation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Preparing to generate HTML summary\")\n        logger.debug(f\"Found {len(self.shared_memory['topics'])} topics and {len(self.shared_memory['eli5_content'])} ELI5 explanations\")\n    \n    def exec(self):\n        \"\"\"\n        Execute HTML generation using the data in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Prepare the summary data for HTML generation\n        summary_data = {\n            \"video_id\": self.shared_memory[\"video_id\"],\n            \"metadata\": self.shared_memory[\"metadata\"],\n            \"topics\": self.shared_memory[\"topics\"],\n            \"qa_pairs\": self.shared_memory[\"qa_pairs\"],\n            \"eli5_content\": self.shared_memory[\"eli5_content\"]\n        }\n        \n        # Generate the HTML content\n        logger.debug(\"Calling generate_html function\")\n        html_content = generate_html(summary_data)\n        \n        # Store the HTML content in shared memory\n        self.shared_memory[\"html_output\"] = html_content\n        logger.info(f\"Generated HTML content ({len(html_content)} characters)\")\n        \n        # Save the HTML to a file if output_path is provided\n        if self.output_path:\n            try:\n                # Create directory if it doesn't exist\n                os.makedirs(os.path.dirname(os.path.abspath(self.output_path)), exist_ok=True)\n                logger.debug(f\"Ensuring output directory exists: {os.path.dirname(os.path.abspath(self.output_path))}\")\n                \n                # Write the HTML content to the file\n                with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html_content)\n                \n                logger.info(f\"HTML summary saved to: {self.output_path}\")\n                \n            except Exception as e:\n                error_msg = f\"Error saving HTML file: {str(e)}\"\n                logger.error(error_msg)\n                self.shared_memory[\"error\"] = error_msg\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in HTML Generation Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"html_output\" not in self.shared_memory:\n            error_msg = \"Failed to generate HTML output\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        html_length = len(self.shared_memory[\"html_output\"])\n        logger.info(f\"Generated HTML summary ({html_length} characters)\")\n        logger.info(\"HTML Generation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    # Initialize shared memory\n    shared_memory = test_data\n    \n    # Create and run the node with a test output path\n    test_output_path = \"test_summary.html\"\n    node = HTMLGenerationNode(shared_memory, test_output_path)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"HTML output length: {len(updated_memory.get('html_output', ''))}\")\n    logger.info(f\"HTML file saved to: {test_output_path}\")\n\n\n--- File: src/nodes/input_processing_node.py ---\n\"\"\"\nInput Processing Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\nfrom src.utils.logger import logger\n\nclass InputProcessingNode(BaseNode):\n    \"\"\"\n    Node for validating YouTube URL and extracting video metadata.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, youtube_url=None):\n        \"\"\"\n        Initialize the node with shared memory and YouTube URL.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            youtube_url (str): YouTube URL to process\n        \"\"\"\n        super().__init__(shared_memory)\n        if youtube_url:\n            self.shared_memory[\"video_url\"] = youtube_url\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_url exists in shared memory.\n        \"\"\"\n        if \"video_url\" not in self.shared_memory:\n            error_msg = \"YouTube URL not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        logger.info(f\"Processing YouTube URL: {self.shared_memory['video_url']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute URL validation and metadata extraction.\n        \"\"\"\n        # Validate the YouTube URL\n        youtube_url = self.shared_memory[\"video_url\"]\n        is_valid, video_id = validate_youtube_url(youtube_url)\n        \n        if not is_valid:\n            error_msg = f\"Invalid YouTube URL: {youtube_url}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"video_id\"] = video_id\n        logger.info(f\"Valid YouTube video ID: {video_id}\")\n        \n        # Extract video metadata\n        logger.debug(f\"Extracting metadata for video ID: {video_id}\")\n        metadata = extract_youtube_metadata(video_id)\n        \n        if \"error\" in metadata:\n            error_msg = f\"Error extracting metadata: {metadata['error']}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"metadata\"] = metadata\n        logger.info(f\"Extracted metadata for video: {metadata.get('title', 'Unknown Title')}\")\n        logger.debug(f\"Video metadata: {metadata}\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Input Processing Node: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Input Processing Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample YouTube URL\n    test_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": test_url}\n    \n    # Create and run the node\n    node = InputProcessingNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Video ID: {updated_memory.get('video_id', 'Not found')}\")\n    logger.info(f\"Video Title: {updated_memory.get('metadata', {}).get('title', 'Not found')}\")\n    logger.info(f\"Channel: {updated_memory.get('metadata', {}).get('channel_name', 'Not found')}\")\n    logger.info(f\"Duration: {updated_memory.get('metadata', {}).get('duration', 'Not found')}\")\n\n\n--- File: src/nodes/topic_extraction_node.py ---\n\"\"\"\nTopic Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport concurrent.futures\nfrom typing import List, Dict, Any\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicExtractionNode(BaseNode):\n    \"\"\"\n    Node for identifying main topics from the video transcript.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, chunk_size=0, overlap=0, max_topics=5):\n        \"\"\"\n        Initialize the node with shared memory and processing parameters.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            chunk_size (int): Size of transcript chunks (0 = no chunking)\n            overlap (int): Overlap between chunks (when chunk_size > 0)\n            max_topics (int): Maximum number of topics to extract\n        \"\"\"\n        super().__init__(shared_memory)\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_topics = max_topics\n        self.chunks = []\n        self.chunk_topics = []\n        logger.debug(f\"TopicExtractionNode initialized with chunk_size={chunk_size}, overlap={overlap}, max_topics={max_topics}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if transcript exists in shared memory\n        and splitting it into chunks if chunk_size > 0.\n        \"\"\"\n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        transcript = self.shared_memory[\"transcript\"]\n        logger.debug(f\"Preparing to extract topics from transcript ({len(transcript)} characters)\")\n        \n        # Only chunk if chunk_size is specified\n        if self.chunk_size > 0:\n            # Split into overlapping chunks\n            start = 0\n            while start < len(transcript):\n                end = min(start + self.chunk_size, len(transcript))\n                self.chunks.append(transcript[start:end])\n                start = end - self.overlap\n            logger.info(f\"Split transcript into {len(self.chunks)} chunks for processing\")\n            logger.debug(f\"Chunk sizes: {[len(chunk) for chunk in self.chunks]}\")\n        else:\n            # Process as a single chunk\n            self.chunks = [transcript]\n            logger.info(\"Processing transcript as a single chunk\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic extraction for each chunk and combine results.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Use ThreadPoolExecutor for parallel processing of chunks\n        max_workers = min(len(self.chunks), 4)  # Limit concurrent API calls\n        logger.info(f\"Processing {len(self.chunks)} chunks with {max_workers} parallel workers\")\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all chunk processing tasks\n            future_to_chunk = {\n                executor.submit(self._process_chunk, i, chunk): i \n                for i, chunk in enumerate(self.chunks)\n            }\n            \n            # Collect results as they complete\n            for future in concurrent.futures.as_completed(future_to_chunk):\n                chunk_index = future_to_chunk[future]\n                try:\n                    topics = future.result()\n                    if topics:\n                        self.chunk_topics.append(topics)\n                        logger.info(f\"Completed processing for chunk {chunk_index+1}/{len(self.chunks)}\")\n                except Exception as e:\n                    logger.error(f\"Error processing chunk {chunk_index+1}: {str(e)}\")\n                    \n        # If we couldn't extract any topics, add some default ones\n        if not self.chunk_topics:\n            logger.warning(\"No topics extracted, using default topics\")\n            self.chunk_topics.append([\"Main Content\", \"Key Points\", \"Summary\"])\n    \n    def _process_chunk(self, chunk_index: int, chunk: str) -> List[str]:\n        \"\"\"\n        Process a single chunk to extract topics.\n        \n        Args:\n            chunk_index (int): Index of the chunk\n            chunk (str): Content of the chunk\n            \n        Returns:\n            List[str]: Extracted topics\n        \"\"\"\n        logger.info(f\"Processing chunk {chunk_index+1}/{len(self.chunks)}...\")\n        \n        # Create prompt for topic extraction\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content and identifying main topics.\n        \n        I'll provide you with a transcript from a YouTube video. Your task is to:\n        1. Identify the main topics discussed in this segment\n        2. List each topic as a short, clear phrase (3-7 words)\n        3. Provide at most {self.max_topics} topics\n        4. Focus on substantive content, not introductions or conclusions\n        \n        Here is the transcript segment:\n        \n        {chunk[:2000]}...\n        \n        Respond with ONLY a JSON array of topic strings. For example:\n        [\"Topic One\", \"Topic Two\", \"Topic Three\"]\n        \"\"\")\n        \n        # Call LLM to extract topics\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1} (timeout: 30s)...\")\n            response = call_llm(prompt, temperature=0.3, max_tokens=200, timeout=30)\n            \n            # Check if we got an error response\n            if response.startswith(\"Error:\"):\n                logger.warning(f\"LLM API error for chunk {chunk_index+1}: {response}\")\n                # Add some default topics if we can't get them from the LLM\n                if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                    logger.info(\"Using default topics due to LLM error\")\n                    return [\"Main Content\", \"Key Points\", \"Summary\"]\n                return []\n            \n            logger.info(f\"Received LLM response for chunk {chunk_index+1} ({len(response)} characters)\")\n            logger.debug(f\"LLM response preview: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                import json\n                topics = json.loads(cleaned_response)\n                if isinstance(topics, list):\n                    logger.info(f\"Extracted {len(topics)} topics from chunk {chunk_index+1}\")\n                    logger.debug(f\"Topics from chunk {chunk_index+1}: {topics}\")\n                    return topics\n                else:\n                    logger.warning(f\"Expected list but got {type(topics)} from LLM\")\n                    # Try to extract topics from text response\n                    return [cleaned_response]\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response}\")\n                # Try to extract topics from text response\n                lines = cleaned_response.split(\"\\n\")\n                potential_topics = [line.strip().strip('\",[]') for line in lines if line.strip()]\n                logger.info(f\"Extracted {len(potential_topics)} topics from non-JSON response\")\n                return potential_topics\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # Add some default topics if we can't get them from the LLM\n            if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                logger.info(\"Using default topics due to exception\")\n                return [\"Main Content\", \"Key Points\", \"Summary\"]\n            return []\n    \n    def post(self):\n        \"\"\"\n        Post-process by combining topics from all chunks and selecting the most relevant ones.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.chunk_topics:\n            error_msg = \"Failed to extract topics from transcript\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        # Flatten the list of topics from all chunks\n        all_topics = []\n        for topics in self.chunk_topics:\n            all_topics.extend(topics)\n        \n        logger.debug(f\"All extracted topics (before deduplication): {all_topics}\")\n        \n        # Count topic occurrences to find the most common ones\n        topic_counts = {}\n        for topic in all_topics:\n            topic_lower = topic.lower()\n            topic_counts[topic_lower] = topic_counts.get(topic_lower, 0) + 1\n        \n        logger.debug(f\"Topic frequency counts: {topic_counts}\")\n        \n        # Sort topics by frequency\n        sorted_topics = sorted(\n            [(count, topic) for topic, count in topic_counts.items()], \n            reverse=True\n        )\n        \n        # Select the top topics (using the original casing)\n        top_topics = []\n        seen_topics = set()\n        \n        for _, topic_lower in sorted_topics:\n            # Find the original casing version\n            original_casing = next(\n                (t for t in all_topics if t.lower() == topic_lower), \n                topic_lower.title()\n            )\n            \n            # Skip if we've already added this topic\n            if topic_lower in seen_topics:\n                continue\n                \n            top_topics.append(original_casing)\n            seen_topics.add(topic_lower)\n            \n            # Limit to max_topics\n            if len(top_topics) >= self.max_topics:\n                break\n        \n        # Store the final list of topics\n        self.shared_memory[\"topics\"] = top_topics\n        \n        logger.info(f\"Extracted {len(top_topics)} final topics:\")\n        for i, topic in enumerate(top_topics):\n            logger.info(f\"  {i+1}. {topic}\")\n        \n        logger.info(\"Topic Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with a sample transcript\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \n    Now let's discuss some applications of machine learning. In healthcare, machine learning \n    is used for disease diagnosis, drug discovery, and personalized medicine. In finance, \n    it's used for fraud detection, algorithmic trading, and credit scoring. In transportation, \n    self-driving cars rely heavily on machine learning algorithms.\n    \n    Ethical considerations in machine learning include bias in training data, model interpretability, \n    and privacy concerns. It's important to develop responsible AI systems that are fair and transparent.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\"transcript\": test_transcript}\n    \n    # Create and run the node\n    node = TopicExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Topics: {updated_memory.get('topics', [])}\")\n\n\n--- File: src/nodes/topic_orchestrator_node.py ---\n\"\"\"\nTopic Processing Orchestrator Node for YouTube Video Summarizer.\nImplements a Map-Reduce approach for parallel topic processing.\n\"\"\"\nimport sys\nimport os\nfrom typing import List, Dict, Any\nimport concurrent.futures\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.nodes.topic_processor_node import TopicProcessorNode\nfrom src.utils.logger import logger\n\nclass TopicOrchestratorNode(BaseNode):\n    \"\"\"\n    Node for orchestrating parallel processing of topics using Map-Reduce pattern.\n    Maps topics to individual processors and reduces the results.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, max_workers=3, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            max_workers (int): Maximum number of parallel workers for topic processing\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory)\n        self.max_workers = max_workers\n        self.questions_per_topic = questions_per_topic\n        self.topics = []\n        self.transcript = \"\"\n        self.topic_results = {}\n        logger.debug(f\"TopicOrchestratorNode initialized with max_workers={max_workers}, questions_per_topic={questions_per_topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and transcript exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Orchestration due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        \n        topics_count = len(self.topics)\n        # Adjust max_workers if there are fewer topics than workers\n        self.max_workers = min(self.max_workers, topics_count)\n        logger.info(f\"Preparing to process {topics_count} topics with {self.max_workers} parallel workers\")\n        logger.debug(f\"Topics to process: {self.topics}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute Map-Reduce processing for topics.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n        \n        # Map phase: Process all topics in parallel\n        self._map_phase()\n        \n        # Reduce phase: Combine results\n        self._reduce_phase()\n    \n    def _map_phase(self):\n        \"\"\"\n        Map phase: Process all topics in parallel using a thread pool.\n        \"\"\"\n        logger.info(f\"Starting Map phase with {self.max_workers} workers for {len(self.topics)} topics\")\n        \n        # Use ThreadPoolExecutor for parallel processing\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all topics for processing\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic \n                for topic in self.topics\n            }\n            \n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    result = future.result()\n                    self.topic_results[topic] = result\n                    logger.info(f\"Completed processing for topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {str(e)}\")\n                    # Create an empty result for failed topics\n                    self.topic_results[topic] = {\n                        \"topic\": topic,\n                        \"qa_pairs\": [],\n                        \"eli5_content\": f\"Error processing topic: {str(e)}\"\n                    }\n        \n        logger.info(f\"Map phase complete: Processed {len(self.topic_results)} topics\")\n    \n    def _process_topic(self, topic):\n        \"\"\"\n        Process a single topic using a TopicProcessorNode.\n        \n        Args:\n            topic (str): The topic to process\n            \n        Returns:\n            dict: The processing result\n        \"\"\"\n        logger.info(f\"Processing topic: {topic}\")\n        \n        # Create and run a TopicProcessorNode for this topic\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic\n        )\n        \n        return processor.run()[\"topic_results\"][topic]\n    \n    def _reduce_phase(self):\n        \"\"\"\n        Reduce phase: Combine results from all topic processors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        logger.info(f\"Starting Reduce phase with {len(self.topic_results)} topic results\")\n        \n        # Initialize the combined results\n        qa_pairs = {}\n        eli5_content = {}\n        \n        # Combine results from all topics\n        for topic, result in self.topic_results.items():\n            # Add Q&A pairs\n            qa_pairs[topic] = result.get(\"qa_pairs\", [])\n            \n            # Add ELI5 content\n            eli5_content[topic] = result.get(\"eli5_content\", \"\")\n        \n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        \n        # Log summary of combined results\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        logger.info(f\"Reduce phase complete: Combined {total_qa_pairs} Q&A pairs across {len(qa_pairs)} topics\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Orchestrator Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.topic_results:\n            error_msg = \"No topic results were generated\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Topic Orchestrator Node completed successfully\")\n        logger.info(f\"Processed {len(self.topic_results)} topics using Map-Reduce pattern\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and transcript\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"transcript\": test_transcript\n    }\n    \n    # Create and run the node\n    node = TopicOrchestratorNode(shared_memory, max_workers=2)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    \n    # Print Q&A pairs\n    qa_pairs = updated_memory.get(\"qa_pairs\", {})\n    for topic, pairs in qa_pairs.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(f\"Q&A Pairs: {len(pairs)}\")\n        for i, qa in enumerate(pairs):\n            logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n            logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    \n    # Print ELI5 content\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic ELI5: {topic}\")\n        logger.info(explanation[:200] + \"...\" if len(explanation) > 200 else explanation)\n\n\n--- File: src/nodes/topic_processor_node.py ---\n\"\"\"\nTopic Processor Node for YouTube Video Summarizer.\nProcesses a single topic with Q&A generation and ELI5 transformation.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicProcessorNode(BaseNode):\n    \"\"\"\n    Node for processing a single topic, including Q&A generation and ELI5 transformation.\n    This node is designed to be used as part of a Map-Reduce pattern.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, topic=None, transcript=None, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with topic and transcript.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary (optional)\n            topic (str): The topic to process\n            transcript (str): The video transcript\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory or {})\n        self.topic = topic\n        self.transcript = transcript\n        self.questions_per_topic = questions_per_topic\n        self.result = {\n            \"topic\": topic,\n            \"qa_pairs\": [],\n            \"eli5_content\": \"\"\n        }\n        logger.debug(f\"TopicProcessorNode initialized for topic: {topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topic and transcript are available.\n        \"\"\"\n        # If topic and transcript were not provided in constructor, try to get from shared memory\n        if self.topic is None and \"current_topic\" in self.shared_memory:\n            self.topic = self.shared_memory[\"current_topic\"]\n            \n        if self.transcript is None and \"transcript\" in self.shared_memory:\n            self.transcript = self.shared_memory[\"transcript\"]\n            \n        if self.topic is None:\n            error_msg = \"No topic provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        if self.transcript is None:\n            error_msg = \"No transcript provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        logger.info(f\"Processing topic: {self.topic}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic processing: generate Q&A pairs and ELI5 explanation.\n        \"\"\"\n        # Step 1: Generate Q&A pairs\n        self._generate_qa_pairs()\n        \n        # Step 2: Create ELI5 explanation\n        self._create_eli5_explanation()\n    \n    def _generate_qa_pairs(self):\n        \"\"\"\n        Generate Q&A pairs for the topic.\n        \"\"\"\n        logger.info(f\"Generating Q&A pairs for topic: {self.topic}\")\n        \n        # Create prompt for Q&A generation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at creating educational content for videos.\n        \n        I'll provide you with a transcript from a YouTube video and a specific topic from that video.\n        Your task is to:\n        1. Generate {self.questions_per_topic} insightful questions about this topic\n        2. Provide clear, accurate answers to each question based on the transcript\n        3. Make sure the questions cover different aspects of the topic\n        4. Ensure answers are based only on information in the transcript\n        \n        Topic: {self.topic}\n        \n        Transcript:\n        {self.transcript[:4000]}...\n        \n        Respond with ONLY a JSON array of question-answer objects. For example:\n        [\n            {{\n                \"question\": \"What is the main purpose of X?\",\n                \"answer\": \"According to the video, the main purpose of X is...\"\n            }},\n            {{\n                \"question\": \"How does Y relate to Z?\",\n                \"answer\": \"The video explains that Y and Z are connected through...\"\n            }}\n        ]\n        \"\"\")\n        \n        # Call LLM to generate Q&A pairs\n        try:\n            logger.debug(f\"Calling LLM for Q&A generation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=1000)\n            logger.debug(f\"LLM response for Q&A generation: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            elif cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                qa_pairs = json.loads(cleaned_response)\n                if isinstance(qa_pairs, list):\n                    self.result[\"qa_pairs\"] = qa_pairs\n                    logger.info(f\"Generated {len(qa_pairs)} Q&A pairs for topic '{self.topic}'\")\n                    for i, qa in enumerate(qa_pairs):\n                        logger.debug(f\"  Q{i+1}: {qa.get('question', '')}\")\n                        logger.debug(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n                else:\n                    logger.warning(f\"Expected list but got {type(qa_pairs)} from LLM\")\n                    self.result[\"qa_pairs\"] = []\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response[:200]}...\")\n                # Try to extract Q&A pairs from text response\n                extracted_qa = self._extract_qa_from_text(cleaned_response)\n                self.result[\"qa_pairs\"] = extracted_qa\n                logger.info(f\"Extracted {len(extracted_qa)} Q&A pairs from non-JSON response\")\n        except Exception as e:\n            logger.error(f\"Error generating Q&A pairs for topic '{self.topic}': {str(e)}\")\n            self.result[\"qa_pairs\"] = []\n    \n    def _extract_qa_from_text(self, text):\n        \"\"\"\n        Attempt to extract Q&A pairs from non-JSON text.\n        \n        Args:\n            text (str): Text containing Q&A pairs\n            \n        Returns:\n            list: List of Q&A pair dictionaries\n        \"\"\"\n        logger.debug(\"Attempting to extract Q&A pairs from non-JSON text\")\n        lines = text.split(\"\\n\")\n        qa_pairs = []\n        current_question = None\n        current_answer = \"\"\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line starts with Q: or Question:\n            if line.startswith((\"Q:\", \"Question:\")):\n                # If we have a previous question, save it\n                if current_question:\n                    qa_pairs.append({\n                        \"question\": current_question,\n                        \"answer\": current_answer.strip()\n                    })\n                \n                # Extract new question\n                current_question = line.split(\":\", 1)[1].strip()\n                current_answer = \"\"\n            # Check if line starts with A: or Answer:\n            elif line.startswith((\"A:\", \"Answer:\")) and current_question:\n                current_answer += line.split(\":\", 1)[1].strip() + \" \"\n            # Otherwise, add to current answer if we have a question\n            elif current_question:\n                current_answer += line + \" \"\n        \n        # Add the last Q&A pair if exists\n        if current_question:\n            qa_pairs.append({\n                \"question\": current_question,\n                \"answer\": current_answer.strip()\n            })\n            \n        logger.debug(f\"Extracted {len(qa_pairs)} Q&A pairs from text\")\n        return qa_pairs\n    \n    def _create_eli5_explanation(self):\n        \"\"\"\n        Create an ELI5 (Explain Like I'm 5) explanation for the topic.\n        \"\"\"\n        logger.info(f\"Creating ELI5 explanation for topic: {self.topic}\")\n        \n        # Get Q&A pairs for context\n        qa_text = \"\"\n        for qa in self.result[\"qa_pairs\"]:\n            question = qa.get(\"question\", \"\")\n            answer = qa.get(\"answer\", \"\")\n            qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n        \n        # Create prompt for ELI5 transformation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at explaining complex topics to young children (5-7 years old).\n        \n        I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n        Your task is to:\n        1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n        2. Use simple words, short sentences, and concrete examples\n        3. Avoid jargon and technical terms\n        4. Use analogies to familiar concepts when possible\n        5. Keep the explanation under 200 words\n        6. Maintain the core information while simplifying the language\n        \n        Topic: {self.topic}\n        \n        Q&A Context:\n        {qa_text}\n        \n        Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n        \"\"\")\n        \n        # Call LLM to generate ELI5 explanation\n        try:\n            logger.debug(f\"Calling LLM for ELI5 transformation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=500)\n            logger.debug(f\"LLM response for ELI5 transformation: {response[:100]}...\")\n            \n            # Clean up the response\n            explanation = response.strip()\n            \n            # Store the ELI5 explanation\n            self.result[\"eli5_content\"] = explanation\n            logger.info(f\"Generated ELI5 explanation for topic '{self.topic}' ({len(explanation)} characters)\")\n            \n        except Exception as e:\n            error_msg = f\"Error creating ELI5 explanation for topic '{self.topic}': {str(e)}\"\n            logger.error(error_msg)\n            self.result[\"eli5_content\"] = f\"Sorry, I couldn't create a simple explanation for {self.topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and return the results.\n        \"\"\"\n        # Store the results in shared memory if needed\n        if \"topic_results\" not in self.shared_memory:\n            self.shared_memory[\"topic_results\"] = {}\n            \n        self.shared_memory[\"topic_results\"][self.topic] = self.result\n        \n        logger.info(f\"Topic processing completed for '{self.topic}'\")\n        logger.debug(f\"Generated {len(self.result['qa_pairs'])} Q&A pairs and {len(self.result['eli5_content'])} characters of ELI5 content\")\n        \n        return self.result\n\n\nif __name__ == \"__main__\":\n    # Test with a sample topic and transcript\n    test_topic = \"Machine Learning Basics\"\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Create and run the node\n    node = TopicProcessorNode(topic=test_topic, transcript=test_transcript)\n    result = node.run()\n    \n    # Print the results\n    logger.info(\"\\nTopic Processing Results:\")\n    logger.info(f\"Topic: {result['topic']}\")\n    logger.info(f\"Q&A Pairs: {len(result['qa_pairs'])}\")\n    for i, qa in enumerate(result['qa_pairs']):\n        logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n        logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    logger.info(f\"ELI5 Explanation: {result['eli5_content']}\")\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 2: Node (Pipeline Step)`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"Shared Memory\". This is Chapter 3.\n\nConcept Details:\n- Description:\nThis is the mechanism used to pass data between different Nodes in the pipeline. It's essentially a Python dictionary where each Node can read the results from previous Nodes and write its own results for subsequent Nodes to use.\nImagine a shared whiteboard where each worker writes down their results (like video ID, transcript, topics) so the next worker in line knows what to work with.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n---\n# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!\n\nRelevant Code Snippets:\n--- File: src/main.py ---\n\"\"\"\nYouTube Video Summarizer - Main Application\n\"\"\"\nimport os\nimport sys\nimport argparse\nfrom datetime import datetime\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Add the project root to the path so we can import from src\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n\nfrom src.nodes.input_processing_node import InputProcessingNode\nfrom src.nodes.content_extraction_node import ContentExtractionNode\nfrom src.nodes.topic_extraction_node import TopicExtractionNode\nfrom src.nodes.topic_orchestrator_node import TopicOrchestratorNode\nfrom src.nodes.html_generation_node import HTMLGenerationNode\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", enable_chunking=False, max_workers=3):\n    \"\"\"\n    Run the complete YouTube video summarization pipeline.\n    \n    Args:\n        youtube_url (str): URL of the YouTube video to summarize\n        output_dir (str): Directory to save the output HTML file\n        enable_chunking (bool): Whether to enable transcript chunking\n        max_workers (int): Maximum number of parallel workers for topic processing\n    \"\"\"\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"YouTube Video Summarizer\")\n    logger.info(f\"{'='*60}\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n    logger.info(f\"Chunking: {'Enabled' if enable_chunking else 'Disabled'}\")\n    logger.info(f\"Parallel Workers: {max_workers}\")\n    logger.info(f\"{'='*60}\")\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": youtube_url}\n    \n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n    logger.debug(f\"Output directory created/confirmed: {output_dir}\")\n    \n    # Configure chunking parameters if enabled\n    chunk_size = 4000 if enable_chunking else 0\n    overlap = 500 if enable_chunking else 0\n    \n    try:\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        input_node = InputProcessingNode(shared_memory)\n        shared_memory = input_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Input Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        logger.info(f\"Successfully processed video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        \n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Content Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        transcript_length = len(shared_memory.get('transcript', ''))\n        logger.info(f\"Successfully extracted transcript ({transcript_length} characters)\")\n        \n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, chunk_size=chunk_size, overlap=overlap)\n        shared_memory = topic_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Extraction failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        topics = shared_memory.get('topics', [])\n        logger.info(f\"Successfully extracted {len(topics)} topics\")\n        for i, topic in enumerate(topics):\n            logger.info(f\"  Topic {i+1}: {topic}\")\n        \n        # 4. Topic Processing Orchestrator Node\n        logger.info(\"[4/5] Starting Topic Processing...\")\n        orchestrator_node = TopicOrchestratorNode(shared_memory, max_workers=max_workers, questions_per_topic=3)\n        shared_memory = orchestrator_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"Topic Processing failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        qa_pairs = shared_memory.get('qa_pairs', {})\n        eli5_content = shared_memory.get('eli5_content', {})\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        \n        logger.info(f\"Successfully processed {len(topics)} topics\")\n        logger.info(f\"Generated {total_qa_pairs} Q&A pairs and {len(eli5_content)} ELI5 explanations\")\n        \n        # 5. HTML Generation Node\n        logger.info(\"[5/5] Starting HTML Generation...\")\n        # Create a timestamped filename\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        video_id = shared_memory.get(\"video_id\", \"unknown\")\n        output_file = os.path.join(output_dir, f\"summary_{video_id}_{timestamp}.html\")\n        \n        html_node = HTMLGenerationNode(shared_memory, output_file)\n        shared_memory = html_node.run()\n        \n        # Check for errors\n        if \"error\" in shared_memory:\n            logger.error(f\"HTML Generation failed: {shared_memory['error']}\")\n            return shared_memory\n        \n        # Print summary\n        logger.info(f\"{'='*60}\")\n        logger.info(\"Summary Generation Complete!\")\n        logger.info(f\"{'='*60}\")\n        logger.info(f\"Video: {shared_memory.get('metadata', {}).get('title', 'Unknown')}\")\n        logger.info(f\"Topics: {len(shared_memory.get('topics', []))}\")\n        logger.info(f\"Q&A Pairs: {sum(len(pairs) for pairs in shared_memory.get('qa_pairs', {}).values())}\")\n        logger.info(f\"Output File: {output_file}\")\n        logger.info(f\"{'='*60}\")\n        \n        return shared_memory\n        \n    except Exception as e:\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n\ndef main():\n    \"\"\"\n    Main entry point for the application.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"YouTube Video Summarizer\")\n    parser.add_argument(\"url\", nargs='?', default=None, help=\"YouTube video URL to summarize\")\n    parser.add_argument(\"--output\", \"-o\", default=\"output\", help=\"Output directory for HTML summary\")\n    parser.add_argument(\"--chunk\", action=\"store_true\", help=\"Enable transcript chunking for long videos\")\n    parser.add_argument(\"--workers\", \"-w\", type=int, default=3, help=\"Number of parallel workers for topic processing\")\n    \n    args = parser.parse_args()\n    \n    # If no URL is provided as a command-line argument, ask for user input\n    youtube_url = args.url\n    if not youtube_url:\n        youtube_url = input(\"Please enter a YouTube URL to summarize: \")\n    \n    logger.info(f\"Starting YouTube Video Summarizer with URL: {youtube_url}\")\n    \n    # Run the pipeline\n    run_pipeline(youtube_url, args.output, args.chunk, args.workers)\n\nif __name__ == \"__main__\":\n    main()\n\n\n--- File: src/nodes/base_node.py ---\n\"\"\"\nBase node class for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom abc import ABC, abstractmethod\nfrom src.utils.logger import logger\n\nclass BaseNode(ABC):\n    \"\"\"\n    Abstract base class for all nodes in the YouTube Video Summarizer.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary for data exchange between nodes\n        \"\"\"\n        self.shared_memory = shared_memory or {}\n        self.node_name = self.__class__.__name__\n        logger.debug(f\"{self.node_name} initialized\")\n    \n    @abstractmethod\n    def prep(self):\n        \"\"\"\n        Prepare for execution by reading necessary data from shared memory.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def exec(self):\n        \"\"\"\n        Execute the node's main functionality.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def post(self):\n        \"\"\"\n        Post-process and write results to shared memory.\n        \"\"\"\n        pass\n    \n    def run(self):\n        \"\"\"\n        Run the complete node workflow: prep, exec, post.\n        \n        Returns:\n            dict: The updated shared memory\n        \"\"\"\n        logger.debug(f\"{self.node_name} starting run\")\n        try:\n            self.prep()\n            if \"error\" not in self.shared_memory:\n                self.exec()\n            if \"error\" not in self.shared_memory:\n                self.post()\n            \n            if \"error\" in self.shared_memory:\n                logger.error(f\"{self.node_name} failed: {self.shared_memory['error']}\")\n            else:\n                logger.debug(f\"{self.node_name} completed successfully\")\n                \n        except Exception as e:\n            logger.exception(f\"Unexpected error in {self.node_name}: {str(e)}\")\n            self.shared_memory[\"error\"] = f\"{self.node_name} error: {str(e)}\"\n            \n        return self.shared_memory\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 3: Shared Memory`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 3: Shared Memory\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we saw our pipeline manager (`run_pipeline`) coordinating the different steps. In [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), we met the specialized workers (Nodes) that perform each task, like getting the video transcript or finding topics.\n\nBut how does the \"transcript extractor\" Node hand over the transcript it found to the \"topic finder\" Node? They need a way to pass information along the assembly line. That's where **Shared Memory** comes in!\n\n## The Problem: Passing Notes Between Workers\n\nImagine our assembly line again.\n1.  Worker A gets the video ID.\n2.  Worker B needs that video ID to fetch the transcript.\n3.  Worker C needs the transcript to find the main topics.\n\nHow does Worker A tell Worker B the video ID? How does Worker B give the transcript to Worker C? They can't just shout across the factory floor! They need a reliable way to share their results.\n\n## Our Solution: The Shared Whiteboard (A Python Dictionary)\n\nIn `pf1`, the **Shared Memory** is our solution. Think of it like a **shared whiteboard** or a **central notebook** that every worker (Node) in the pipeline can access.\n\n*   When a Node finishes its job, it writes its results onto the whiteboard (e.g., \"Video ID: xyz123\", \"Transcript: Hello world...\").\n*   When the next Node starts its job, it looks at the whiteboard to find the information it needs (e.g., it reads the \"Transcript\" section).\n\nTechnically, this \"whiteboard\" is just a standard **Python dictionary**. If you're new to Python dictionaries, they simply store information as **key-value pairs**.\n\n```python\n# A simple Python dictionary example\nmy_info = {\n  \"name\": \"Alex\",\n  \"task\": \"Write chapter\",\n  \"tools\": [\"laptop\", \"keyboard\"]\n}\n\n# Accessing information using the key\nprint(my_info[\"name\"])  # Output: Alex\n\n# Adding new information\nmy_info[\"status\"] = \"In progress\"\nprint(my_info)\n# Output: {'name': 'Alex', 'task': 'Write chapter', 'tools': ['laptop', 'keyboard'], 'status': 'In progress'}\n```\n\nOur `shared_memory` works exactly like this! It's a dictionary where keys are strings describing the data (like `\"video_url\"`, `\"video_id\"`, `\"transcript\"`, `\"topics\"`), and the values are the actual data (the URL string, the ID string, the long transcript text, a list of topic strings, etc.).\n\n## How Shared Memory Flows Through the Pipeline\n\n1.  **Initialization:** The pipeline orchestrator (`run_pipeline` in `src/main.py`) creates the `shared_memory` dictionary when the pipeline starts. Initially, it usually just contains the input needed for the very first step, like the YouTube video URL.\n\n    ```python\n    # From src/main.py (simplified)\n    def run_pipeline(youtube_url, ...):\n        # Create the shared memory dictionary\n        shared_memory = {\"video_url\": youtube_url}\n        logger.info(\"Shared memory initialized.\")\n        # ... rest of the pipeline ...\n    ```\n    At this point, `shared_memory` looks like: `{\"video_url\": \"https://youtube.com/watch?v=some_id\"}`\n\n2.  **Passing to Nodes:** The orchestrator passes this `shared_memory` dictionary to the first Node when it calls the Node's `run()` method.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... inside run_pipeline ...\n    \n    # 1. Input Processing Node\n    logger.info(\"Calling InputProcessingNode...\")\n    input_node = InputProcessingNode(shared_memory) # Give the Node access\n    shared_memory = input_node.run() # Node runs and returns the UPDATED dictionary\n    ```\n\n3.  **Nodes Read and Write:** Inside the Node's `run()` method, the `prep`, `exec`, and `post` methods interact with the dictionary:\n    *   **`prep()` reads:** It looks up the keys it needs (e.g., the `InputProcessingNode` reads `\"video_url\"`).\n    *   **`post()` writes:** After `exec()` does the work, `post()` adds the Node's results back into the *same* dictionary using new keys (e.g., the `InputProcessingNode` adds `\"video_id\"` and `\"metadata\"`).\n\n    ```python\n    # Inside InputProcessingNode (simplified)\n    \n    def prep(self):\n        # Read the URL provided by the orchestrator\n        self.url = self.shared_memory[\"video_url\"]\n        logger.debug(f\"Input node read URL: {self.url}\")\n\n    def exec(self):\n        # Do work: Get video ID and metadata from the URL\n        # (Simplified - actual logic calls YouTube utilities)\n        self.video_id_result = \"xyz123\"\n        self.metadata_result = {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n        logger.debug(\"Input node got video ID and metadata.\")\n\n    def post(self):\n        # Write the results back to the dictionary\n        self.shared_memory[\"video_id\"] = self.video_id_result\n        self.shared_memory[\"metadata\"] = self.metadata_result\n        logger.debug(\"Input node wrote video_id and metadata to shared memory.\")\n    ```\n\n4.  **Passing Updated Memory:** When the Node's `run()` method finishes, it returns the modified `shared_memory` dictionary back to the orchestrator. The orchestrator then passes this *updated* dictionary to the *next* Node in the sequence.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... after InputProcessingNode runs ...\n    # shared_memory now contains:\n    # {\n    #   \"video_url\": \"https://youtube.com/watch?v=some_id\",\n    #   \"video_id\": \"xyz123\",\n    #   \"metadata\": {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n    # }\n    \n    # 2. Content Extraction Node\n    logger.info(\"Calling ContentExtractionNode...\")\n    content_node = ContentExtractionNode(shared_memory) # Pass the UPDATED memory\n    shared_memory = content_node.run() # Get the next update\n    ```\n\n5.  **Chain Reaction:** This process repeats for every Node in the pipeline. Each Node reads the data left by previous Nodes and adds its own results.\n\n    *   `ContentExtractionNode` reads `\"video_id\"`, does its work, and adds `\"transcript\"`.\n    *   `TopicExtractionNode` reads `\"transcript\"`, does its work, and adds `\"topics\"`.\n    *   ...and so on.\n\nThe `shared_memory` dictionary grows as it passes through the pipeline, accumulating all the results.\n\n## Under the Hood: Passing the Dictionary Around\n\nIs this some magic \"shared\" space in the computer's memory? Not really! It's much simpler in our case.\n\nPython dictionaries are \"mutable\" objects. This means when you pass a dictionary to a function (or a Node's method), you're passing a reference to the *exact same* dictionary object in memory. If the function modifies the dictionary (like adding a new key), the original dictionary that was passed in also changes.\n\nSo, the orchestrator creates *one* dictionary object. It passes a reference to this object to Node 1. Node 1 modifies it directly. The orchestrator then passes the reference to the *same, now modified* object to Node 2, and so on.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline (Orchestrator)\n    participant SMDict as Shared Memory (Python Dictionary)\n    participant NodeA as InputProcessingNode\n    participant NodeB as ContentExtractionNode\n\n    Orch->>SMDict: Create { \"video_url\": \"...\" }\n    Orch->>NodeA: run(reference to SMDict)\n    NodeA->>NodeA: prep() [reads \"video_url\" from SMDict]\n    NodeA->>NodeA: exec() [gets video_id, metadata]\n    NodeA->>SMDict: post() [adds \"video_id\", \"metadata\"]\n    NodeA-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now modified\n\n    Orch->>NodeB: run(reference to SMDict)\n    NodeB->>NodeB: prep() [reads \"video_id\" from SMDict]\n    NodeB->>NodeB: exec() [gets transcript]\n    NodeB->>SMDict: post() [adds \"transcript\"]\n    NodeB-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now further modified\n```\n\nIt's like passing the *same physical notebook* from person to person. Each person adds their notes to it before passing it on.\n\n## Example: Reading and Writing\n\nLet's see how the `ContentExtractionNode` might read the `\"video_id\"` written by `InputProcessingNode` and then write the `\"transcript\"`.\n\n```python\n# Inside ContentExtractionNode (simplified)\n\ndef prep(self):\n    # Read the video_id written by the previous node\n    if \"video_id\" in self.shared_memory:\n        self.video_id_to_process = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Content node read video_id: {self.video_id_to_process}\")\n    else:\n        # Handle missing data (important!)\n        logger.error(\"Error: video_id not found in shared memory!\")\n        self.shared_memory[\"error\"] = \"Missing video_id for transcript extraction\"\n\ndef exec(self):\n    # Only run if prep was successful (no error)\n    if \"error\" not in self.shared_memory:\n        # Use self.video_id_to_process to get the transcript\n        # (Simplified - actual logic calls YouTube utilities)\n        self.transcript_result = \"This is the transcript text...\"\n        logger.debug(\"Content node got the transcript.\")\n\ndef post(self):\n    # Only run if prep and exec were successful\n    if \"error\" not in self.shared_memory:\n        # Write the transcript result to shared memory\n        self.shared_memory[\"transcript\"] = self.transcript_result\n        logger.debug(\"Content node wrote transcript to shared memory.\")\n```\nThis Node first checks if the key `\"video_id\"` exists in the `shared_memory` it received (this is crucial error handling!). If it exists, it uses that ID in its `exec` step. Finally, in `post`, it adds its own result under the key `\"transcript\"`. The next Node can now read `\"transcript\"`.\n\n## Why Use a Dictionary?\n\n*   **Simplicity:** It's a fundamental Python data structure, easy to understand and use.\n*   **Flexibility:** Dictionaries can hold various data types (strings, numbers, lists, even other dictionaries), which is perfect for the different kinds of results our Nodes produce.\n*   **Clarity:** Using meaningful keys (like `\"transcript\"`, `\"topics\"`) makes it clear what data is being passed around.\n\n## Conclusion\n\nWe've learned about **Shared Memory**, the crucial mechanism that allows our pipeline Nodes to communicate and share data.\n\n*   It's implemented as a simple **Python dictionary**.\n*   The pipeline orchestrator ([Chapter 1](01_pipeline_orchestration.md)) creates it and passes it sequentially to each Node ([Chapter 2](02_node__pipeline_step_.md)).\n*   Nodes **read** the inputs they need from the dictionary using keys (usually in `prep`).\n*   Nodes **write** their results back into the dictionary using new keys (usually in `post`).\n*   This allows data like the video ID, transcript, and topics to flow smoothly from one step to the next.\n\nUnderstanding Shared Memory is key to seeing how the different parts of our `pf1` project connect and build upon each other's work.\n\nNow that we understand the basic building blocks (Orchestration, Nodes, Shared Memory), we can look at more specialized components. Next, we'll explore a Node that acts as a mini-orchestrator itself, managing the detailed processing of topics.\n\nLet's dive into the [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"Topic Processing Orchestrator\". This is Chapter 4.\n\nConcept Details:\n- Description:\nThis Node manages the processing of multiple topics identified in the video. It uses a parallel approach (Map-Reduce style) by assigning each topic to a dedicated `TopicProcessorNode` to generate Q&A and ELI5 explanations simultaneously, making the process faster.\nIt's like a team lead who divides a big research task (all topics) among team members (TopicProcessorNodes), lets them work in parallel, and then collects and combines their findings.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n---\n# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!\n---\n# Chapter 3: Shared Memory\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we saw our pipeline manager (`run_pipeline`) coordinating the different steps. In [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), we met the specialized workers (Nodes) that perform each task, like getting the video transcript or finding topics.\n\nBut how does the \"transcript extractor\" Node hand over the transcript it found to the \"topic finder\" Node? They need a way to pass information along the assembly line. That's where **Shared Memory** comes in!\n\n## The Problem: Passing Notes Between Workers\n\nImagine our assembly line again.\n1.  Worker A gets the video ID.\n2.  Worker B needs that video ID to fetch the transcript.\n3.  Worker C needs the transcript to find the main topics.\n\nHow does Worker A tell Worker B the video ID? How does Worker B give the transcript to Worker C? They can't just shout across the factory floor! They need a reliable way to share their results.\n\n## Our Solution: The Shared Whiteboard (A Python Dictionary)\n\nIn `pf1`, the **Shared Memory** is our solution. Think of it like a **shared whiteboard** or a **central notebook** that every worker (Node) in the pipeline can access.\n\n*   When a Node finishes its job, it writes its results onto the whiteboard (e.g., \"Video ID: xyz123\", \"Transcript: Hello world...\").\n*   When the next Node starts its job, it looks at the whiteboard to find the information it needs (e.g., it reads the \"Transcript\" section).\n\nTechnically, this \"whiteboard\" is just a standard **Python dictionary**. If you're new to Python dictionaries, they simply store information as **key-value pairs**.\n\n```python\n# A simple Python dictionary example\nmy_info = {\n  \"name\": \"Alex\",\n  \"task\": \"Write chapter\",\n  \"tools\": [\"laptop\", \"keyboard\"]\n}\n\n# Accessing information using the key\nprint(my_info[\"name\"])  # Output: Alex\n\n# Adding new information\nmy_info[\"status\"] = \"In progress\"\nprint(my_info)\n# Output: {'name': 'Alex', 'task': 'Write chapter', 'tools': ['laptop', 'keyboard'], 'status': 'In progress'}\n```\n\nOur `shared_memory` works exactly like this! It's a dictionary where keys are strings describing the data (like `\"video_url\"`, `\"video_id\"`, `\"transcript\"`, `\"topics\"`), and the values are the actual data (the URL string, the ID string, the long transcript text, a list of topic strings, etc.).\n\n## How Shared Memory Flows Through the Pipeline\n\n1.  **Initialization:** The pipeline orchestrator (`run_pipeline` in `src/main.py`) creates the `shared_memory` dictionary when the pipeline starts. Initially, it usually just contains the input needed for the very first step, like the YouTube video URL.\n\n    ```python\n    # From src/main.py (simplified)\n    def run_pipeline(youtube_url, ...):\n        # Create the shared memory dictionary\n        shared_memory = {\"video_url\": youtube_url}\n        logger.info(\"Shared memory initialized.\")\n        # ... rest of the pipeline ...\n    ```\n    At this point, `shared_memory` looks like: `{\"video_url\": \"https://youtube.com/watch?v=some_id\"}`\n\n2.  **Passing to Nodes:** The orchestrator passes this `shared_memory` dictionary to the first Node when it calls the Node's `run()` method.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... inside run_pipeline ...\n    \n    # 1. Input Processing Node\n    logger.info(\"Calling InputProcessingNode...\")\n    input_node = InputProcessingNode(shared_memory) # Give the Node access\n    shared_memory = input_node.run() # Node runs and returns the UPDATED dictionary\n    ```\n\n3.  **Nodes Read and Write:** Inside the Node's `run()` method, the `prep`, `exec`, and `post` methods interact with the dictionary:\n    *   **`prep()` reads:** It looks up the keys it needs (e.g., the `InputProcessingNode` reads `\"video_url\"`).\n    *   **`post()` writes:** After `exec()` does the work, `post()` adds the Node's results back into the *same* dictionary using new keys (e.g., the `InputProcessingNode` adds `\"video_id\"` and `\"metadata\"`).\n\n    ```python\n    # Inside InputProcessingNode (simplified)\n    \n    def prep(self):\n        # Read the URL provided by the orchestrator\n        self.url = self.shared_memory[\"video_url\"]\n        logger.debug(f\"Input node read URL: {self.url}\")\n\n    def exec(self):\n        # Do work: Get video ID and metadata from the URL\n        # (Simplified - actual logic calls YouTube utilities)\n        self.video_id_result = \"xyz123\"\n        self.metadata_result = {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n        logger.debug(\"Input node got video ID and metadata.\")\n\n    def post(self):\n        # Write the results back to the dictionary\n        self.shared_memory[\"video_id\"] = self.video_id_result\n        self.shared_memory[\"metadata\"] = self.metadata_result\n        logger.debug(\"Input node wrote video_id and metadata to shared memory.\")\n    ```\n\n4.  **Passing Updated Memory:** When the Node's `run()` method finishes, it returns the modified `shared_memory` dictionary back to the orchestrator. The orchestrator then passes this *updated* dictionary to the *next* Node in the sequence.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... after InputProcessingNode runs ...\n    # shared_memory now contains:\n    # {\n    #   \"video_url\": \"https://youtube.com/watch?v=some_id\",\n    #   \"video_id\": \"xyz123\",\n    #   \"metadata\": {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n    # }\n    \n    # 2. Content Extraction Node\n    logger.info(\"Calling ContentExtractionNode...\")\n    content_node = ContentExtractionNode(shared_memory) # Pass the UPDATED memory\n    shared_memory = content_node.run() # Get the next update\n    ```\n\n5.  **Chain Reaction:** This process repeats for every Node in the pipeline. Each Node reads the data left by previous Nodes and adds its own results.\n\n    *   `ContentExtractionNode` reads `\"video_id\"`, does its work, and adds `\"transcript\"`.\n    *   `TopicExtractionNode` reads `\"transcript\"`, does its work, and adds `\"topics\"`.\n    *   ...and so on.\n\nThe `shared_memory` dictionary grows as it passes through the pipeline, accumulating all the results.\n\n## Under the Hood: Passing the Dictionary Around\n\nIs this some magic \"shared\" space in the computer's memory? Not really! It's much simpler in our case.\n\nPython dictionaries are \"mutable\" objects. This means when you pass a dictionary to a function (or a Node's method), you're passing a reference to the *exact same* dictionary object in memory. If the function modifies the dictionary (like adding a new key), the original dictionary that was passed in also changes.\n\nSo, the orchestrator creates *one* dictionary object. It passes a reference to this object to Node 1. Node 1 modifies it directly. The orchestrator then passes the reference to the *same, now modified* object to Node 2, and so on.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline (Orchestrator)\n    participant SMDict as Shared Memory (Python Dictionary)\n    participant NodeA as InputProcessingNode\n    participant NodeB as ContentExtractionNode\n\n    Orch->>SMDict: Create { \"video_url\": \"...\" }\n    Orch->>NodeA: run(reference to SMDict)\n    NodeA->>NodeA: prep() [reads \"video_url\" from SMDict]\n    NodeA->>NodeA: exec() [gets video_id, metadata]\n    NodeA->>SMDict: post() [adds \"video_id\", \"metadata\"]\n    NodeA-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now modified\n\n    Orch->>NodeB: run(reference to SMDict)\n    NodeB->>NodeB: prep() [reads \"video_id\" from SMDict]\n    NodeB->>NodeB: exec() [gets transcript]\n    NodeB->>SMDict: post() [adds \"transcript\"]\n    NodeB-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now further modified\n```\n\nIt's like passing the *same physical notebook* from person to person. Each person adds their notes to it before passing it on.\n\n## Example: Reading and Writing\n\nLet's see how the `ContentExtractionNode` might read the `\"video_id\"` written by `InputProcessingNode` and then write the `\"transcript\"`.\n\n```python\n# Inside ContentExtractionNode (simplified)\n\ndef prep(self):\n    # Read the video_id written by the previous node\n    if \"video_id\" in self.shared_memory:\n        self.video_id_to_process = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Content node read video_id: {self.video_id_to_process}\")\n    else:\n        # Handle missing data (important!)\n        logger.error(\"Error: video_id not found in shared memory!\")\n        self.shared_memory[\"error\"] = \"Missing video_id for transcript extraction\"\n\ndef exec(self):\n    # Only run if prep was successful (no error)\n    if \"error\" not in self.shared_memory:\n        # Use self.video_id_to_process to get the transcript\n        # (Simplified - actual logic calls YouTube utilities)\n        self.transcript_result = \"This is the transcript text...\"\n        logger.debug(\"Content node got the transcript.\")\n\ndef post(self):\n    # Only run if prep and exec were successful\n    if \"error\" not in self.shared_memory:\n        # Write the transcript result to shared memory\n        self.shared_memory[\"transcript\"] = self.transcript_result\n        logger.debug(\"Content node wrote transcript to shared memory.\")\n```\nThis Node first checks if the key `\"video_id\"` exists in the `shared_memory` it received (this is crucial error handling!). If it exists, it uses that ID in its `exec` step. Finally, in `post`, it adds its own result under the key `\"transcript\"`. The next Node can now read `\"transcript\"`.\n\n## Why Use a Dictionary?\n\n*   **Simplicity:** It's a fundamental Python data structure, easy to understand and use.\n*   **Flexibility:** Dictionaries can hold various data types (strings, numbers, lists, even other dictionaries), which is perfect for the different kinds of results our Nodes produce.\n*   **Clarity:** Using meaningful keys (like `\"transcript\"`, `\"topics\"`) makes it clear what data is being passed around.\n\n## Conclusion\n\nWe've learned about **Shared Memory**, the crucial mechanism that allows our pipeline Nodes to communicate and share data.\n\n*   It's implemented as a simple **Python dictionary**.\n*   The pipeline orchestrator ([Chapter 1](01_pipeline_orchestration.md)) creates it and passes it sequentially to each Node ([Chapter 2](02_node__pipeline_step_.md)).\n*   Nodes **read** the inputs they need from the dictionary using keys (usually in `prep`).\n*   Nodes **write** their results back into the dictionary using new keys (usually in `post`).\n*   This allows data like the video ID, transcript, and topics to flow smoothly from one step to the next.\n\nUnderstanding Shared Memory is key to seeing how the different parts of our `pf1` project connect and build upon each other's work.\n\nNow that we understand the basic building blocks (Orchestration, Nodes, Shared Memory), we can look at more specialized components. Next, we'll explore a Node that acts as a mini-orchestrator itself, managing the detailed processing of topics.\n\nLet's dive into the [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)!\n\nRelevant Code Snippets:\n--- File: src/nodes/topic_orchestrator_node.py ---\n\"\"\"\nTopic Processing Orchestrator Node for YouTube Video Summarizer.\nImplements a Map-Reduce approach for parallel topic processing.\n\"\"\"\nimport sys\nimport os\nfrom typing import List, Dict, Any\nimport concurrent.futures\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.nodes.topic_processor_node import TopicProcessorNode\nfrom src.utils.logger import logger\n\nclass TopicOrchestratorNode(BaseNode):\n    \"\"\"\n    Node for orchestrating parallel processing of topics using Map-Reduce pattern.\n    Maps topics to individual processors and reduces the results.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, max_workers=3, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            max_workers (int): Maximum number of parallel workers for topic processing\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory)\n        self.max_workers = max_workers\n        self.questions_per_topic = questions_per_topic\n        self.topics = []\n        self.transcript = \"\"\n        self.topic_results = {}\n        logger.debug(f\"TopicOrchestratorNode initialized with max_workers={max_workers}, questions_per_topic={questions_per_topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topics and transcript exist in shared memory.\n        \"\"\"\n        if \"topics\" not in self.shared_memory:\n            error_msg = \"Topics not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Orchestration due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        \n        topics_count = len(self.topics)\n        # Adjust max_workers if there are fewer topics than workers\n        self.max_workers = min(self.max_workers, topics_count)\n        logger.info(f\"Preparing to process {topics_count} topics with {self.max_workers} parallel workers\")\n        logger.debug(f\"Topics to process: {self.topics}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute Map-Reduce processing for topics.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n        \n        # Map phase: Process all topics in parallel\n        self._map_phase()\n        \n        # Reduce phase: Combine results\n        self._reduce_phase()\n    \n    def _map_phase(self):\n        \"\"\"\n        Map phase: Process all topics in parallel using a thread pool.\n        \"\"\"\n        logger.info(f\"Starting Map phase with {self.max_workers} workers for {len(self.topics)} topics\")\n        \n        # Use ThreadPoolExecutor for parallel processing\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all topics for processing\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic \n                for topic in self.topics\n            }\n            \n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    result = future.result()\n                    self.topic_results[topic] = result\n                    logger.info(f\"Completed processing for topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {str(e)}\")\n                    # Create an empty result for failed topics\n                    self.topic_results[topic] = {\n                        \"topic\": topic,\n                        \"qa_pairs\": [],\n                        \"eli5_content\": f\"Error processing topic: {str(e)}\"\n                    }\n        \n        logger.info(f\"Map phase complete: Processed {len(self.topic_results)} topics\")\n    \n    def _process_topic(self, topic):\n        \"\"\"\n        Process a single topic using a TopicProcessorNode.\n        \n        Args:\n            topic (str): The topic to process\n            \n        Returns:\n            dict: The processing result\n        \"\"\"\n        logger.info(f\"Processing topic: {topic}\")\n        \n        # Create and run a TopicProcessorNode for this topic\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic\n        )\n        \n        return processor.run()[\"topic_results\"][topic]\n    \n    def _reduce_phase(self):\n        \"\"\"\n        Reduce phase: Combine results from all topic processors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        logger.info(f\"Starting Reduce phase with {len(self.topic_results)} topic results\")\n        \n        # Initialize the combined results\n        qa_pairs = {}\n        eli5_content = {}\n        \n        # Combine results from all topics\n        for topic, result in self.topic_results.items():\n            # Add Q&A pairs\n            qa_pairs[topic] = result.get(\"qa_pairs\", [])\n            \n            # Add ELI5 content\n            eli5_content[topic] = result.get(\"eli5_content\", \"\")\n        \n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        \n        # Log summary of combined results\n        total_qa_pairs = sum(len(pairs) for pairs in qa_pairs.values())\n        logger.info(f\"Reduce phase complete: Combined {total_qa_pairs} Q&A pairs across {len(qa_pairs)} topics\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Orchestrator Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.topic_results:\n            error_msg = \"No topic results were generated\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Topic Orchestrator Node completed successfully\")\n        logger.info(f\"Processed {len(self.topic_results)} topics using Map-Reduce pattern\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample topics and transcript\n    test_topics = [\"Machine Learning Basics\", \"Types of Machine Learning\"]\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\n        \"topics\": test_topics,\n        \"transcript\": test_transcript\n    }\n    \n    # Create and run the node\n    node = TopicOrchestratorNode(shared_memory, max_workers=2)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    \n    # Print Q&A pairs\n    qa_pairs = updated_memory.get(\"qa_pairs\", {})\n    for topic, pairs in qa_pairs.items():\n        logger.info(f\"\\nTopic: {topic}\")\n        logger.info(f\"Q&A Pairs: {len(pairs)}\")\n        for i, qa in enumerate(pairs):\n            logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n            logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    \n    # Print ELI5 content\n    eli5_content = updated_memory.get(\"eli5_content\", {})\n    for topic, explanation in eli5_content.items():\n        logger.info(f\"\\nTopic ELI5: {topic}\")\n        logger.info(explanation[:200] + \"...\" if len(explanation) > 200 else explanation)\n\n\n--- File: src/nodes/topic_processor_node.py ---\n\"\"\"\nTopic Processor Node for YouTube Video Summarizer.\nProcesses a single topic with Q&A generation and ELI5 transformation.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicProcessorNode(BaseNode):\n    \"\"\"\n    Node for processing a single topic, including Q&A generation and ELI5 transformation.\n    This node is designed to be used as part of a Map-Reduce pattern.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, topic=None, transcript=None, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with topic and transcript.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary (optional)\n            topic (str): The topic to process\n            transcript (str): The video transcript\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory or {})\n        self.topic = topic\n        self.transcript = transcript\n        self.questions_per_topic = questions_per_topic\n        self.result = {\n            \"topic\": topic,\n            \"qa_pairs\": [],\n            \"eli5_content\": \"\"\n        }\n        logger.debug(f\"TopicProcessorNode initialized for topic: {topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topic and transcript are available.\n        \"\"\"\n        # If topic and transcript were not provided in constructor, try to get from shared memory\n        if self.topic is None and \"current_topic\" in self.shared_memory:\n            self.topic = self.shared_memory[\"current_topic\"]\n            \n        if self.transcript is None and \"transcript\" in self.shared_memory:\n            self.transcript = self.shared_memory[\"transcript\"]\n            \n        if self.topic is None:\n            error_msg = \"No topic provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        if self.transcript is None:\n            error_msg = \"No transcript provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        logger.info(f\"Processing topic: {self.topic}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic processing: generate Q&A pairs and ELI5 explanation.\n        \"\"\"\n        # Step 1: Generate Q&A pairs\n        self._generate_qa_pairs()\n        \n        # Step 2: Create ELI5 explanation\n        self._create_eli5_explanation()\n    \n    def _generate_qa_pairs(self):\n        \"\"\"\n        Generate Q&A pairs for the topic.\n        \"\"\"\n        logger.info(f\"Generating Q&A pairs for topic: {self.topic}\")\n        \n        # Create prompt for Q&A generation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at creating educational content for videos.\n        \n        I'll provide you with a transcript from a YouTube video and a specific topic from that video.\n        Your task is to:\n        1. Generate {self.questions_per_topic} insightful questions about this topic\n        2. Provide clear, accurate answers to each question based on the transcript\n        3. Make sure the questions cover different aspects of the topic\n        4. Ensure answers are based only on information in the transcript\n        \n        Topic: {self.topic}\n        \n        Transcript:\n        {self.transcript[:4000]}...\n        \n        Respond with ONLY a JSON array of question-answer objects. For example:\n        [\n            {{\n                \"question\": \"What is the main purpose of X?\",\n                \"answer\": \"According to the video, the main purpose of X is...\"\n            }},\n            {{\n                \"question\": \"How does Y relate to Z?\",\n                \"answer\": \"The video explains that Y and Z are connected through...\"\n            }}\n        ]\n        \"\"\")\n        \n        # Call LLM to generate Q&A pairs\n        try:\n            logger.debug(f\"Calling LLM for Q&A generation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=1000)\n            logger.debug(f\"LLM response for Q&A generation: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            elif cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                qa_pairs = json.loads(cleaned_response)\n                if isinstance(qa_pairs, list):\n                    self.result[\"qa_pairs\"] = qa_pairs\n                    logger.info(f\"Generated {len(qa_pairs)} Q&A pairs for topic '{self.topic}'\")\n                    for i, qa in enumerate(qa_pairs):\n                        logger.debug(f\"  Q{i+1}: {qa.get('question', '')}\")\n                        logger.debug(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n                else:\n                    logger.warning(f\"Expected list but got {type(qa_pairs)} from LLM\")\n                    self.result[\"qa_pairs\"] = []\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response[:200]}...\")\n                # Try to extract Q&A pairs from text response\n                extracted_qa = self._extract_qa_from_text(cleaned_response)\n                self.result[\"qa_pairs\"] = extracted_qa\n                logger.info(f\"Extracted {len(extracted_qa)} Q&A pairs from non-JSON response\")\n        except Exception as e:\n            logger.error(f\"Error generating Q&A pairs for topic '{self.topic}': {str(e)}\")\n            self.result[\"qa_pairs\"] = []\n    \n    def _extract_qa_from_text(self, text):\n        \"\"\"\n        Attempt to extract Q&A pairs from non-JSON text.\n        \n        Args:\n            text (str): Text containing Q&A pairs\n            \n        Returns:\n            list: List of Q&A pair dictionaries\n        \"\"\"\n        logger.debug(\"Attempting to extract Q&A pairs from non-JSON text\")\n        lines = text.split(\"\\n\")\n        qa_pairs = []\n        current_question = None\n        current_answer = \"\"\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line starts with Q: or Question:\n            if line.startswith((\"Q:\", \"Question:\")):\n                # If we have a previous question, save it\n                if current_question:\n                    qa_pairs.append({\n                        \"question\": current_question,\n                        \"answer\": current_answer.strip()\n                    })\n                \n                # Extract new question\n                current_question = line.split(\":\", 1)[1].strip()\n                current_answer = \"\"\n            # Check if line starts with A: or Answer:\n            elif line.startswith((\"A:\", \"Answer:\")) and current_question:\n                current_answer += line.split(\":\", 1)[1].strip() + \" \"\n            # Otherwise, add to current answer if we have a question\n            elif current_question:\n                current_answer += line + \" \"\n        \n        # Add the last Q&A pair if exists\n        if current_question:\n            qa_pairs.append({\n                \"question\": current_question,\n                \"answer\": current_answer.strip()\n            })\n            \n        logger.debug(f\"Extracted {len(qa_pairs)} Q&A pairs from text\")\n        return qa_pairs\n    \n    def _create_eli5_explanation(self):\n        \"\"\"\n        Create an ELI5 (Explain Like I'm 5) explanation for the topic.\n        \"\"\"\n        logger.info(f\"Creating ELI5 explanation for topic: {self.topic}\")\n        \n        # Get Q&A pairs for context\n        qa_text = \"\"\n        for qa in self.result[\"qa_pairs\"]:\n            question = qa.get(\"question\", \"\")\n            answer = qa.get(\"answer\", \"\")\n            qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n        \n        # Create prompt for ELI5 transformation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at explaining complex topics to young children (5-7 years old).\n        \n        I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n        Your task is to:\n        1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n        2. Use simple words, short sentences, and concrete examples\n        3. Avoid jargon and technical terms\n        4. Use analogies to familiar concepts when possible\n        5. Keep the explanation under 200 words\n        6. Maintain the core information while simplifying the language\n        \n        Topic: {self.topic}\n        \n        Q&A Context:\n        {qa_text}\n        \n        Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n        \"\"\")\n        \n        # Call LLM to generate ELI5 explanation\n        try:\n            logger.debug(f\"Calling LLM for ELI5 transformation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=500)\n            logger.debug(f\"LLM response for ELI5 transformation: {response[:100]}...\")\n            \n            # Clean up the response\n            explanation = response.strip()\n            \n            # Store the ELI5 explanation\n            self.result[\"eli5_content\"] = explanation\n            logger.info(f\"Generated ELI5 explanation for topic '{self.topic}' ({len(explanation)} characters)\")\n            \n        except Exception as e:\n            error_msg = f\"Error creating ELI5 explanation for topic '{self.topic}': {str(e)}\"\n            logger.error(error_msg)\n            self.result[\"eli5_content\"] = f\"Sorry, I couldn't create a simple explanation for {self.topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and return the results.\n        \"\"\"\n        # Store the results in shared memory if needed\n        if \"topic_results\" not in self.shared_memory:\n            self.shared_memory[\"topic_results\"] = {}\n            \n        self.shared_memory[\"topic_results\"][self.topic] = self.result\n        \n        logger.info(f\"Topic processing completed for '{self.topic}'\")\n        logger.debug(f\"Generated {len(self.result['qa_pairs'])} Q&A pairs and {len(self.result['eli5_content'])} characters of ELI5 content\")\n        \n        return self.result\n\n\nif __name__ == \"__main__\":\n    # Test with a sample topic and transcript\n    test_topic = \"Machine Learning Basics\"\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Create and run the node\n    node = TopicProcessorNode(topic=test_topic, transcript=test_transcript)\n    result = node.run()\n    \n    # Print the results\n    logger.info(\"\\nTopic Processing Results:\")\n    logger.info(f\"Topic: {result['topic']}\")\n    logger.info(f\"Q&A Pairs: {len(result['qa_pairs'])}\")\n    for i, qa in enumerate(result['qa_pairs']):\n        logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n        logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    logger.info(f\"ELI5 Explanation: {result['eli5_content']}\")\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 4: Topic Processing Orchestrator`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 4: Topic Processing Orchestrator\n\nWelcome back! In [Chapter 3: Shared Memory](03_shared_memory.md), we learned how different steps (Nodes) in our pipeline share information using a central dictionary called `shared_memory`. We saw how the `TopicExtractionNode` adds a list of topics it found into this shared space.\n\nNow, imagine our video analyzer found 10 different topics in a long video! For each topic, we want to:\n1.  Generate some Questions and Answers (Q&A).\n2.  Create a super simple \"Explain Like I'm 5\" (ELI5) summary.\n\nIf we did this one topic at a time, it could take a while, especially since generating Q&A and ELI5 often involves asking an AI model (like the ones we'll see in [Chapter 6: LLM Utility](06_llm_utility.md)), which can be slow. How can we speed this up?\n\n## The Problem: Analyzing Many Topics Takes Time\n\nLet's say analyzing one topic takes 30 seconds. If we have 10 topics, doing them one after another would take 300 seconds (5 minutes!). That's a long coffee break!\n\n```\nTopic 1 Analysis --> Wait 30s --> Topic 2 Analysis --> Wait 30s --> ... --> Topic 10 Analysis --> Wait 30s\nTotal Time = 10 * 30s = 300s\n```\n\nWe need a smarter way to handle this.\n\n## The Solution: Divide and Conquer with Parallelism!\n\nThink about a big research project given to a team. Does the team lead make everyone wait while one person researches the first part, then the next person researches the second part? No! The lead divides the work: \"Alice, you research Topic A. Bob, you research Topic B. Charlie, you research Topic C. Go!\" Everyone works *at the same time* (in parallel). When they're done, the lead collects all their findings.\n\nThis is exactly what the **Topic Processing Orchestrator** (`TopicOrchestratorNode`) does! It acts like that efficient team lead for analyzing topics.\n\n*   **The Big Task:** Analyze all topics found in the video.\n*   **The Team Lead:** `TopicOrchestratorNode`.\n*   **The Team Members:** Specialized workers called `TopicProcessorNode` (we'll see these workers in action soon).\n*   **The Assignment:** The `TopicOrchestratorNode` gives *each* topic to a *separate* `TopicProcessorNode`.\n*   **Working in Parallel:** It lets all the `TopicProcessorNode`s work simultaneously to generate Q&A and ELI5 for their assigned topic.\n*   **Collecting Results:** Once all workers are finished, the `TopicOrchestratorNode` gathers their results and puts them together.\n\nIf analyzing one topic still takes 30 seconds, but we have, say, 3 workers (or more!) operating in parallel, we can analyze 3 topics in roughly 30 seconds. If we have enough workers for all 10 topics, the total time might be closer to just 30 seconds (plus a little overhead for managing them), instead of 300!\n\n```\n                                Topic 1 Analysis (Worker 1) \\\nSimultaneously Start -->        Topic 2 Analysis (Worker 2)  --> Wait ~30s --> All Done!\n                                Topic 3 Analysis (Worker 3) /\n                                ... and so on ...\n```\n\nThis parallel approach is sometimes called **Map-Reduce**:\n1.  **Map:** Assign (map) each piece of work (topic) to a worker (`TopicProcessorNode`).\n2.  **Reduce:** Collect (reduce) the results from all workers into a final combined output.\n\n## How the Topic Orchestrator Fits In\n\nThe `TopicOrchestratorNode` is just another [Node (Pipeline Step)](02_node__pipeline_step_.md) in our main pipeline, run by the [Pipeline Orchestration](01_pipeline_orchestration.md) (`run_pipeline` function).\n\nHere's what it does in the pipeline flow:\n\n1.  **Input:** It reads the `topics` (a list of strings) and the full `transcript` (a long string) from the [Shared Memory](03_shared_memory.md), put there by previous Nodes.\n2.  **Processing (Parallel):** It performs the \"Map-Reduce\" strategy described above, using multiple `TopicProcessorNode` instances internally.\n3.  **Output:** It writes the collected results back into [Shared Memory](03_shared_memory.md). The results are typically stored in dictionaries keyed by topic, for example:\n    *   `\"qa_pairs\"`: A dictionary where keys are topics and values are lists of Q&A for that topic.\n    *   `\"eli5_content\"`: A dictionary where keys are topics and values are the ELI5 explanations.\n    *   `\"topic_results\"`: A combined dictionary holding both Q&A and ELI5 for each topic.\n\n## Using the Topic Orchestrator Node\n\nWe don't usually call the `TopicOrchestratorNode` directly. The main `run_pipeline` function in `src/main.py` calls it after the topics have been extracted.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory contains 'topics' and 'transcript')\n\n# 4. Topic Orchestration Node\nlogger.info(\"[4/5] Starting Topic Processing Orchestration...\")\n# Create the orchestrator node, giving it shared memory\n# We can configure how many 'workers' (max_workers) it can use\ntopic_orchestrator = TopicOrchestratorNode(shared_memory, max_workers=3)\n\n# Run the node - this triggers the parallel processing\nshared_memory = topic_orchestrator.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"Topic Orchestration failed...\")\n    return shared_memory\n\nlogger.info(\"Successfully processed all topics.\")\n\n# Now, shared_memory contains the new keys like 'qa_pairs' and 'eli5_content'\n# Example: shared_memory['qa_pairs']['Machine Learning Basics'] might be a list of Q&A\n# Example: shared_memory['eli5_content']['Machine Learning Basics'] might be the ELI5 text\n\n# ... (Next node, like HTML Generation, can use these results) ...\n```\n\nIn this example:\n*   We create `TopicOrchestratorNode`, passing the current `shared_memory` (which must contain `\"topics\"` and `\"transcript\"`).\n*   We also tell it it can use up to `max_workers=3` parallel workers.\n*   Calling `topic_orchestrator.run()` kicks off the parallel processing.\n*   After it finishes, the `shared_memory` dictionary is updated with the results (`\"qa_pairs\"`, `\"eli5_content\"`, etc.).\n\n## Under the Hood: How Parallelism Works\n\nLet's visualize how the `TopicOrchestratorNode` (TON) manages the `TopicProcessorNode` (TPN) workers.\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline\n    participant TON as TopicOrchestratorNode\n    participant SM as Shared Memory\n    participant TPN1 as TopicProcessorNode (Worker 1)\n    participant TPN2 as TopicProcessorNode (Worker 2)\n\n    RP->>TON: run(SM) [SM has topics: T1, T2]\n    TON->>SM: prep() [Reads topics=[T1, T2], transcript]\n    Note over TON, TPN2: exec() starts Map Phase (using Thread Pool)\n    TON->>TPN1: Create & submit _process_topic(T1)\n    TON->>TPN2: Create & submit _process_topic(T2)\n    Note over TPN1, TPN2: TPN1 and TPN2 run in parallel\n    TPN1->>TPN1: Runs its own prep/exec/post (gets Q&A/ELI5 for T1)\n    TPN2->>TPN2: Runs its own prep/exec/post (gets Q&A/ELI5 for T2)\n    TPN1-->>TON: Returns result for T1\n    TPN2-->>TON: Returns result for T2\n    Note over TON: Map Phase ends, Reduce Phase starts\n    TON->>TON: _reduce_phase() [Collects results for T1, T2]\n    TON->>SM: post() [Writes combined 'qa_pairs', 'eli5_content' to SM]\n    TON-->>RP: Return updated SM\n```\n\nThis diagram shows:\n1.  `run_pipeline` calls the `TopicOrchestratorNode` (`TON`).\n2.  `TON` reads the list of topics (e.g., T1, T2) from `Shared Memory`.\n3.  `TON` starts its `exec` phase, creating and launching worker `TopicProcessorNode`s (`TPN1`, `TPN2`) in parallel, one for each topic.\n4.  Each `TPN` does its work (generating Q&A/ELI5 for its specific topic).\n5.  `TON` waits for all `TPN` workers to finish and collects their individual results.\n6.  `TON` combines these results and writes them back to `Shared Memory`.\n\n## Diving into the Code (`src/nodes/topic_orchestrator_node.py`)\n\nLet's peek inside the `TopicOrchestratorNode`. Remember, it follows the standard [Node (Pipeline Step)](02_node__pipeline_step_.md) structure: `prep`, `exec`, `post`.\n\n**1. Preparation (`prep`)**\n\nThe `prep` method gets the necessary inputs from `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\nclass TopicOrchestratorNode(BaseNode):\n    # ... (init method stores max_workers) ...\n\n    def prep(self):\n        logger.info(\"TopicOrchestrator: Preparing...\")\n        # Check if needed data exists in shared memory\n        if \"topics\" not in self.shared_memory:\n            raise ValueError(\"Topics not found in shared memory\")\n        if \"transcript\" not in self.shared_memory:\n            raise ValueError(\"Transcript not found in shared memory\")\n\n        # Get the topics and transcript\n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        logger.info(f\"Found {len(self.topics)} topics to process.\")\n```\nThis part just checks if the `\"topics\"` list and `\"transcript\"` string are available in the shared memory and stores them within the node instance for later use.\n\n**2. Execution (`exec`)**\n\nThe `exec` method orchestrates the Map and Reduce phases.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def exec(self):\n        logger.info(\"TopicOrchestrator: Executing...\")\n        # Map phase: Process topics in parallel\n        self._map_phase()\n\n        # Reduce phase: Combine the results\n        self._reduce_phase()\n```\nIt simply calls two helper methods: `_map_phase` to run the workers and `_reduce_phase` to collect the results.\n\n**3. Map Phase (`_map_phase`)**\n\nThis is where the parallelism happens using Python's `concurrent.futures.ThreadPoolExecutor`. Think of the executor as a manager for a pool of worker threads.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nimport concurrent.futures # Tool for running things in parallel\n\n    def _map_phase(self):\n        logger.info(f\"Starting Map phase with {self.max_workers} workers...\")\n        self.topic_results = {} # Dictionary to store results\n\n        # Create a pool of workers\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Give each topic to a worker by submitting the _process_topic task\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic\n                for topic in self.topics\n            }\n\n            # Wait for workers to finish and collect results\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    # Get the result from the finished worker\n                    result = future.result()\n                    self.topic_results[topic] = result # Store result\n                    logger.info(f\"Finished processing topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {e}\")\n                    # Store an error message for this topic\n                    self.topic_results[topic] = {\"error\": str(e)}\n```\n*   It creates a `ThreadPoolExecutor` which manages the worker threads.\n*   `executor.submit(self._process_topic, topic)` tells the executor: \"Hey, run the `_process_topic` function with this `topic` as input, using one of your available workers.\" This happens for *all* topics.\n*   `concurrent.futures.as_completed(...)` waits for any worker to finish, gets its result (`future.result()`), and stores it in `self.topic_results` dictionary, keyed by the topic name.\n\n**4. Processing a Single Topic (`_process_topic`)**\n\nThis helper function is what each worker thread actually runs. It creates and runs the specialized `TopicProcessorNode` for a single topic.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nfrom src.nodes.topic_processor_node import TopicProcessorNode # The worker node\n\n    def _process_topic(self, topic):\n        logger.info(f\"Worker starting on topic: {topic}\")\n        # Create the specialist node for *this specific topic*\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic # Configurable option\n        )\n        # Run the specialist node and get its results\n        # The result is a dictionary with 'qa_pairs' and 'eli5_content' for this topic\n        result_dict = processor.run()\n        # We only need the part specific to this topic from the processor's output\n        return result_dict[\"topic_results\"][topic]\n\n```\nThis function takes one `topic`, creates a `TopicProcessorNode` specifically for it (giving it the topic and the full transcript), runs that node, and returns the Q&A/ELI5 results just for that single topic.\n\n**5. Reduce Phase (`_reduce_phase`)**\n\nAfter the `_map_phase` has collected results from all workers into `self.topic_results`, the `_reduce_phase` combines them into the final structure needed in `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def _reduce_phase(self):\n        logger.info(\"Starting Reduce phase...\")\n        qa_pairs = {}\n        eli5_content = {}\n\n        # Go through results collected from workers\n        for topic, result in self.topic_results.items():\n            if \"error\" not in result: # Skip topics that had errors\n                qa_pairs[topic] = result.get(\"qa_pairs\", [])\n                eli5_content[topic] = result.get(\"eli5_content\", \"\")\n\n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        # Also store the raw results per topic\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        logger.info(\"Reduce phase complete. Results added to shared memory.\")\n```\nThis function iterates through the `self.topic_results` gathered during the map phase. It creates two dictionaries, `qa_pairs` and `eli5_content`, where the keys are the topic strings. It then populates these dictionaries with the results from each worker and finally adds these combined dictionaries to the main `shared_memory`.\n\n**6. Post-Processing (`post`)**\n\nThe `post` method mainly does final checks and logging.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def post(self):\n        # Basic check if any results were generated\n        if not self.topic_results:\n             self.shared_memory[\"error\"] = \"No topic results were generated\"\n             logger.error(self.shared_memory[\"error\"])\n             return # Stop if no results\n\n        logger.info(\"Topic Orchestrator Node completed successfully.\")\n```\n\nAnd that's how the `TopicOrchestratorNode` uses parallelism (Map-Reduce) to speed up the processing of multiple topics by delegating the work to `TopicProcessorNode` instances!\n\n## Conclusion\n\nIn this chapter, we explored the `TopicOrchestratorNode`, a smart manager within our pipeline. We learned:\n\n*   **Problem:** Analyzing many topics one-by-one is slow.\n*   **Solution:** Use parallelism with a **Map-Reduce** approach.\n*   **TopicOrchestratorNode:** Acts as a team lead, assigning each topic to a worker (`TopicProcessorNode`).\n*   **Parallel Execution:** Workers run simultaneously using threads, significantly speeding up the process.\n*   **Map Phase:** Assigns tasks to workers and runs them.\n*   **Reduce Phase:** Collects results from all workers and combines them.\n*   **Integration:** It reads `topics` and `transcript` from [Shared Memory](03_shared_memory.md) and writes back `qa_pairs` and `eli5_content`.\n\nThis node demonstrates a more advanced pattern within our pipeline architecture, showing how we can optimize for performance.\n\nSo far, we've seen how the pipeline flows, how individual nodes work, how they share data, and how we can use parallelism. But where does data like the transcript or video title actually come from? Next, we'll look at the tools used to interact with YouTube itself.\n\nLet's move on to [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"YouTube Data Utilities\". This is Chapter 5.\n\nConcept Details:\n- Description:\nThese are helper functions specifically designed to interact with YouTube. One utility extracts metadata (like title, channel, duration), another extracts the video's transcript (spoken words), and another validates the input URL.\nThink of these as specialized tools in a toolbox for YouTube: one tool fetches the video's label information, another gets the script, and a third checks if the link is correct.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n---\n# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!\n---\n# Chapter 3: Shared Memory\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we saw our pipeline manager (`run_pipeline`) coordinating the different steps. In [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), we met the specialized workers (Nodes) that perform each task, like getting the video transcript or finding topics.\n\nBut how does the \"transcript extractor\" Node hand over the transcript it found to the \"topic finder\" Node? They need a way to pass information along the assembly line. That's where **Shared Memory** comes in!\n\n## The Problem: Passing Notes Between Workers\n\nImagine our assembly line again.\n1.  Worker A gets the video ID.\n2.  Worker B needs that video ID to fetch the transcript.\n3.  Worker C needs the transcript to find the main topics.\n\nHow does Worker A tell Worker B the video ID? How does Worker B give the transcript to Worker C? They can't just shout across the factory floor! They need a reliable way to share their results.\n\n## Our Solution: The Shared Whiteboard (A Python Dictionary)\n\nIn `pf1`, the **Shared Memory** is our solution. Think of it like a **shared whiteboard** or a **central notebook** that every worker (Node) in the pipeline can access.\n\n*   When a Node finishes its job, it writes its results onto the whiteboard (e.g., \"Video ID: xyz123\", \"Transcript: Hello world...\").\n*   When the next Node starts its job, it looks at the whiteboard to find the information it needs (e.g., it reads the \"Transcript\" section).\n\nTechnically, this \"whiteboard\" is just a standard **Python dictionary**. If you're new to Python dictionaries, they simply store information as **key-value pairs**.\n\n```python\n# A simple Python dictionary example\nmy_info = {\n  \"name\": \"Alex\",\n  \"task\": \"Write chapter\",\n  \"tools\": [\"laptop\", \"keyboard\"]\n}\n\n# Accessing information using the key\nprint(my_info[\"name\"])  # Output: Alex\n\n# Adding new information\nmy_info[\"status\"] = \"In progress\"\nprint(my_info)\n# Output: {'name': 'Alex', 'task': 'Write chapter', 'tools': ['laptop', 'keyboard'], 'status': 'In progress'}\n```\n\nOur `shared_memory` works exactly like this! It's a dictionary where keys are strings describing the data (like `\"video_url\"`, `\"video_id\"`, `\"transcript\"`, `\"topics\"`), and the values are the actual data (the URL string, the ID string, the long transcript text, a list of topic strings, etc.).\n\n## How Shared Memory Flows Through the Pipeline\n\n1.  **Initialization:** The pipeline orchestrator (`run_pipeline` in `src/main.py`) creates the `shared_memory` dictionary when the pipeline starts. Initially, it usually just contains the input needed for the very first step, like the YouTube video URL.\n\n    ```python\n    # From src/main.py (simplified)\n    def run_pipeline(youtube_url, ...):\n        # Create the shared memory dictionary\n        shared_memory = {\"video_url\": youtube_url}\n        logger.info(\"Shared memory initialized.\")\n        # ... rest of the pipeline ...\n    ```\n    At this point, `shared_memory` looks like: `{\"video_url\": \"https://youtube.com/watch?v=some_id\"}`\n\n2.  **Passing to Nodes:** The orchestrator passes this `shared_memory` dictionary to the first Node when it calls the Node's `run()` method.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... inside run_pipeline ...\n    \n    # 1. Input Processing Node\n    logger.info(\"Calling InputProcessingNode...\")\n    input_node = InputProcessingNode(shared_memory) # Give the Node access\n    shared_memory = input_node.run() # Node runs and returns the UPDATED dictionary\n    ```\n\n3.  **Nodes Read and Write:** Inside the Node's `run()` method, the `prep`, `exec`, and `post` methods interact with the dictionary:\n    *   **`prep()` reads:** It looks up the keys it needs (e.g., the `InputProcessingNode` reads `\"video_url\"`).\n    *   **`post()` writes:** After `exec()` does the work, `post()` adds the Node's results back into the *same* dictionary using new keys (e.g., the `InputProcessingNode` adds `\"video_id\"` and `\"metadata\"`).\n\n    ```python\n    # Inside InputProcessingNode (simplified)\n    \n    def prep(self):\n        # Read the URL provided by the orchestrator\n        self.url = self.shared_memory[\"video_url\"]\n        logger.debug(f\"Input node read URL: {self.url}\")\n\n    def exec(self):\n        # Do work: Get video ID and metadata from the URL\n        # (Simplified - actual logic calls YouTube utilities)\n        self.video_id_result = \"xyz123\"\n        self.metadata_result = {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n        logger.debug(\"Input node got video ID and metadata.\")\n\n    def post(self):\n        # Write the results back to the dictionary\n        self.shared_memory[\"video_id\"] = self.video_id_result\n        self.shared_memory[\"metadata\"] = self.metadata_result\n        logger.debug(\"Input node wrote video_id and metadata to shared memory.\")\n    ```\n\n4.  **Passing Updated Memory:** When the Node's `run()` method finishes, it returns the modified `shared_memory` dictionary back to the orchestrator. The orchestrator then passes this *updated* dictionary to the *next* Node in the sequence.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... after InputProcessingNode runs ...\n    # shared_memory now contains:\n    # {\n    #   \"video_url\": \"https://youtube.com/watch?v=some_id\",\n    #   \"video_id\": \"xyz123\",\n    #   \"metadata\": {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n    # }\n    \n    # 2. Content Extraction Node\n    logger.info(\"Calling ContentExtractionNode...\")\n    content_node = ContentExtractionNode(shared_memory) # Pass the UPDATED memory\n    shared_memory = content_node.run() # Get the next update\n    ```\n\n5.  **Chain Reaction:** This process repeats for every Node in the pipeline. Each Node reads the data left by previous Nodes and adds its own results.\n\n    *   `ContentExtractionNode` reads `\"video_id\"`, does its work, and adds `\"transcript\"`.\n    *   `TopicExtractionNode` reads `\"transcript\"`, does its work, and adds `\"topics\"`.\n    *   ...and so on.\n\nThe `shared_memory` dictionary grows as it passes through the pipeline, accumulating all the results.\n\n## Under the Hood: Passing the Dictionary Around\n\nIs this some magic \"shared\" space in the computer's memory? Not really! It's much simpler in our case.\n\nPython dictionaries are \"mutable\" objects. This means when you pass a dictionary to a function (or a Node's method), you're passing a reference to the *exact same* dictionary object in memory. If the function modifies the dictionary (like adding a new key), the original dictionary that was passed in also changes.\n\nSo, the orchestrator creates *one* dictionary object. It passes a reference to this object to Node 1. Node 1 modifies it directly. The orchestrator then passes the reference to the *same, now modified* object to Node 2, and so on.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline (Orchestrator)\n    participant SMDict as Shared Memory (Python Dictionary)\n    participant NodeA as InputProcessingNode\n    participant NodeB as ContentExtractionNode\n\n    Orch->>SMDict: Create { \"video_url\": \"...\" }\n    Orch->>NodeA: run(reference to SMDict)\n    NodeA->>NodeA: prep() [reads \"video_url\" from SMDict]\n    NodeA->>NodeA: exec() [gets video_id, metadata]\n    NodeA->>SMDict: post() [adds \"video_id\", \"metadata\"]\n    NodeA-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now modified\n\n    Orch->>NodeB: run(reference to SMDict)\n    NodeB->>NodeB: prep() [reads \"video_id\" from SMDict]\n    NodeB->>NodeB: exec() [gets transcript]\n    NodeB->>SMDict: post() [adds \"transcript\"]\n    NodeB-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now further modified\n```\n\nIt's like passing the *same physical notebook* from person to person. Each person adds their notes to it before passing it on.\n\n## Example: Reading and Writing\n\nLet's see how the `ContentExtractionNode` might read the `\"video_id\"` written by `InputProcessingNode` and then write the `\"transcript\"`.\n\n```python\n# Inside ContentExtractionNode (simplified)\n\ndef prep(self):\n    # Read the video_id written by the previous node\n    if \"video_id\" in self.shared_memory:\n        self.video_id_to_process = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Content node read video_id: {self.video_id_to_process}\")\n    else:\n        # Handle missing data (important!)\n        logger.error(\"Error: video_id not found in shared memory!\")\n        self.shared_memory[\"error\"] = \"Missing video_id for transcript extraction\"\n\ndef exec(self):\n    # Only run if prep was successful (no error)\n    if \"error\" not in self.shared_memory:\n        # Use self.video_id_to_process to get the transcript\n        # (Simplified - actual logic calls YouTube utilities)\n        self.transcript_result = \"This is the transcript text...\"\n        logger.debug(\"Content node got the transcript.\")\n\ndef post(self):\n    # Only run if prep and exec were successful\n    if \"error\" not in self.shared_memory:\n        # Write the transcript result to shared memory\n        self.shared_memory[\"transcript\"] = self.transcript_result\n        logger.debug(\"Content node wrote transcript to shared memory.\")\n```\nThis Node first checks if the key `\"video_id\"` exists in the `shared_memory` it received (this is crucial error handling!). If it exists, it uses that ID in its `exec` step. Finally, in `post`, it adds its own result under the key `\"transcript\"`. The next Node can now read `\"transcript\"`.\n\n## Why Use a Dictionary?\n\n*   **Simplicity:** It's a fundamental Python data structure, easy to understand and use.\n*   **Flexibility:** Dictionaries can hold various data types (strings, numbers, lists, even other dictionaries), which is perfect for the different kinds of results our Nodes produce.\n*   **Clarity:** Using meaningful keys (like `\"transcript\"`, `\"topics\"`) makes it clear what data is being passed around.\n\n## Conclusion\n\nWe've learned about **Shared Memory**, the crucial mechanism that allows our pipeline Nodes to communicate and share data.\n\n*   It's implemented as a simple **Python dictionary**.\n*   The pipeline orchestrator ([Chapter 1](01_pipeline_orchestration.md)) creates it and passes it sequentially to each Node ([Chapter 2](02_node__pipeline_step_.md)).\n*   Nodes **read** the inputs they need from the dictionary using keys (usually in `prep`).\n*   Nodes **write** their results back into the dictionary using new keys (usually in `post`).\n*   This allows data like the video ID, transcript, and topics to flow smoothly from one step to the next.\n\nUnderstanding Shared Memory is key to seeing how the different parts of our `pf1` project connect and build upon each other's work.\n\nNow that we understand the basic building blocks (Orchestration, Nodes, Shared Memory), we can look at more specialized components. Next, we'll explore a Node that acts as a mini-orchestrator itself, managing the detailed processing of topics.\n\nLet's dive into the [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)!\n---\n# Chapter 4: Topic Processing Orchestrator\n\nWelcome back! In [Chapter 3: Shared Memory](03_shared_memory.md), we learned how different steps (Nodes) in our pipeline share information using a central dictionary called `shared_memory`. We saw how the `TopicExtractionNode` adds a list of topics it found into this shared space.\n\nNow, imagine our video analyzer found 10 different topics in a long video! For each topic, we want to:\n1.  Generate some Questions and Answers (Q&A).\n2.  Create a super simple \"Explain Like I'm 5\" (ELI5) summary.\n\nIf we did this one topic at a time, it could take a while, especially since generating Q&A and ELI5 often involves asking an AI model (like the ones we'll see in [Chapter 6: LLM Utility](06_llm_utility.md)), which can be slow. How can we speed this up?\n\n## The Problem: Analyzing Many Topics Takes Time\n\nLet's say analyzing one topic takes 30 seconds. If we have 10 topics, doing them one after another would take 300 seconds (5 minutes!). That's a long coffee break!\n\n```\nTopic 1 Analysis --> Wait 30s --> Topic 2 Analysis --> Wait 30s --> ... --> Topic 10 Analysis --> Wait 30s\nTotal Time = 10 * 30s = 300s\n```\n\nWe need a smarter way to handle this.\n\n## The Solution: Divide and Conquer with Parallelism!\n\nThink about a big research project given to a team. Does the team lead make everyone wait while one person researches the first part, then the next person researches the second part? No! The lead divides the work: \"Alice, you research Topic A. Bob, you research Topic B. Charlie, you research Topic C. Go!\" Everyone works *at the same time* (in parallel). When they're done, the lead collects all their findings.\n\nThis is exactly what the **Topic Processing Orchestrator** (`TopicOrchestratorNode`) does! It acts like that efficient team lead for analyzing topics.\n\n*   **The Big Task:** Analyze all topics found in the video.\n*   **The Team Lead:** `TopicOrchestratorNode`.\n*   **The Team Members:** Specialized workers called `TopicProcessorNode` (we'll see these workers in action soon).\n*   **The Assignment:** The `TopicOrchestratorNode` gives *each* topic to a *separate* `TopicProcessorNode`.\n*   **Working in Parallel:** It lets all the `TopicProcessorNode`s work simultaneously to generate Q&A and ELI5 for their assigned topic.\n*   **Collecting Results:** Once all workers are finished, the `TopicOrchestratorNode` gathers their results and puts them together.\n\nIf analyzing one topic still takes 30 seconds, but we have, say, 3 workers (or more!) operating in parallel, we can analyze 3 topics in roughly 30 seconds. If we have enough workers for all 10 topics, the total time might be closer to just 30 seconds (plus a little overhead for managing them), instead of 300!\n\n```\n                                Topic 1 Analysis (Worker 1) \\\nSimultaneously Start -->        Topic 2 Analysis (Worker 2)  --> Wait ~30s --> All Done!\n                                Topic 3 Analysis (Worker 3) /\n                                ... and so on ...\n```\n\nThis parallel approach is sometimes called **Map-Reduce**:\n1.  **Map:** Assign (map) each piece of work (topic) to a worker (`TopicProcessorNode`).\n2.  **Reduce:** Collect (reduce) the results from all workers into a final combined output.\n\n## How the Topic Orchestrator Fits In\n\nThe `TopicOrchestratorNode` is just another [Node (Pipeline Step)](02_node__pipeline_step_.md) in our main pipeline, run by the [Pipeline Orchestration](01_pipeline_orchestration.md) (`run_pipeline` function).\n\nHere's what it does in the pipeline flow:\n\n1.  **Input:** It reads the `topics` (a list of strings) and the full `transcript` (a long string) from the [Shared Memory](03_shared_memory.md), put there by previous Nodes.\n2.  **Processing (Parallel):** It performs the \"Map-Reduce\" strategy described above, using multiple `TopicProcessorNode` instances internally.\n3.  **Output:** It writes the collected results back into [Shared Memory](03_shared_memory.md). The results are typically stored in dictionaries keyed by topic, for example:\n    *   `\"qa_pairs\"`: A dictionary where keys are topics and values are lists of Q&A for that topic.\n    *   `\"eli5_content\"`: A dictionary where keys are topics and values are the ELI5 explanations.\n    *   `\"topic_results\"`: A combined dictionary holding both Q&A and ELI5 for each topic.\n\n## Using the Topic Orchestrator Node\n\nWe don't usually call the `TopicOrchestratorNode` directly. The main `run_pipeline` function in `src/main.py` calls it after the topics have been extracted.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory contains 'topics' and 'transcript')\n\n# 4. Topic Orchestration Node\nlogger.info(\"[4/5] Starting Topic Processing Orchestration...\")\n# Create the orchestrator node, giving it shared memory\n# We can configure how many 'workers' (max_workers) it can use\ntopic_orchestrator = TopicOrchestratorNode(shared_memory, max_workers=3)\n\n# Run the node - this triggers the parallel processing\nshared_memory = topic_orchestrator.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"Topic Orchestration failed...\")\n    return shared_memory\n\nlogger.info(\"Successfully processed all topics.\")\n\n# Now, shared_memory contains the new keys like 'qa_pairs' and 'eli5_content'\n# Example: shared_memory['qa_pairs']['Machine Learning Basics'] might be a list of Q&A\n# Example: shared_memory['eli5_content']['Machine Learning Basics'] might be the ELI5 text\n\n# ... (Next node, like HTML Generation, can use these results) ...\n```\n\nIn this example:\n*   We create `TopicOrchestratorNode`, passing the current `shared_memory` (which must contain `\"topics\"` and `\"transcript\"`).\n*   We also tell it it can use up to `max_workers=3` parallel workers.\n*   Calling `topic_orchestrator.run()` kicks off the parallel processing.\n*   After it finishes, the `shared_memory` dictionary is updated with the results (`\"qa_pairs\"`, `\"eli5_content\"`, etc.).\n\n## Under the Hood: How Parallelism Works\n\nLet's visualize how the `TopicOrchestratorNode` (TON) manages the `TopicProcessorNode` (TPN) workers.\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline\n    participant TON as TopicOrchestratorNode\n    participant SM as Shared Memory\n    participant TPN1 as TopicProcessorNode (Worker 1)\n    participant TPN2 as TopicProcessorNode (Worker 2)\n\n    RP->>TON: run(SM) [SM has topics: T1, T2]\n    TON->>SM: prep() [Reads topics=[T1, T2], transcript]\n    Note over TON, TPN2: exec() starts Map Phase (using Thread Pool)\n    TON->>TPN1: Create & submit _process_topic(T1)\n    TON->>TPN2: Create & submit _process_topic(T2)\n    Note over TPN1, TPN2: TPN1 and TPN2 run in parallel\n    TPN1->>TPN1: Runs its own prep/exec/post (gets Q&A/ELI5 for T1)\n    TPN2->>TPN2: Runs its own prep/exec/post (gets Q&A/ELI5 for T2)\n    TPN1-->>TON: Returns result for T1\n    TPN2-->>TON: Returns result for T2\n    Note over TON: Map Phase ends, Reduce Phase starts\n    TON->>TON: _reduce_phase() [Collects results for T1, T2]\n    TON->>SM: post() [Writes combined 'qa_pairs', 'eli5_content' to SM]\n    TON-->>RP: Return updated SM\n```\n\nThis diagram shows:\n1.  `run_pipeline` calls the `TopicOrchestratorNode` (`TON`).\n2.  `TON` reads the list of topics (e.g., T1, T2) from `Shared Memory`.\n3.  `TON` starts its `exec` phase, creating and launching worker `TopicProcessorNode`s (`TPN1`, `TPN2`) in parallel, one for each topic.\n4.  Each `TPN` does its work (generating Q&A/ELI5 for its specific topic).\n5.  `TON` waits for all `TPN` workers to finish and collects their individual results.\n6.  `TON` combines these results and writes them back to `Shared Memory`.\n\n## Diving into the Code (`src/nodes/topic_orchestrator_node.py`)\n\nLet's peek inside the `TopicOrchestratorNode`. Remember, it follows the standard [Node (Pipeline Step)](02_node__pipeline_step_.md) structure: `prep`, `exec`, `post`.\n\n**1. Preparation (`prep`)**\n\nThe `prep` method gets the necessary inputs from `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\nclass TopicOrchestratorNode(BaseNode):\n    # ... (init method stores max_workers) ...\n\n    def prep(self):\n        logger.info(\"TopicOrchestrator: Preparing...\")\n        # Check if needed data exists in shared memory\n        if \"topics\" not in self.shared_memory:\n            raise ValueError(\"Topics not found in shared memory\")\n        if \"transcript\" not in self.shared_memory:\n            raise ValueError(\"Transcript not found in shared memory\")\n\n        # Get the topics and transcript\n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        logger.info(f\"Found {len(self.topics)} topics to process.\")\n```\nThis part just checks if the `\"topics\"` list and `\"transcript\"` string are available in the shared memory and stores them within the node instance for later use.\n\n**2. Execution (`exec`)**\n\nThe `exec` method orchestrates the Map and Reduce phases.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def exec(self):\n        logger.info(\"TopicOrchestrator: Executing...\")\n        # Map phase: Process topics in parallel\n        self._map_phase()\n\n        # Reduce phase: Combine the results\n        self._reduce_phase()\n```\nIt simply calls two helper methods: `_map_phase` to run the workers and `_reduce_phase` to collect the results.\n\n**3. Map Phase (`_map_phase`)**\n\nThis is where the parallelism happens using Python's `concurrent.futures.ThreadPoolExecutor`. Think of the executor as a manager for a pool of worker threads.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nimport concurrent.futures # Tool for running things in parallel\n\n    def _map_phase(self):\n        logger.info(f\"Starting Map phase with {self.max_workers} workers...\")\n        self.topic_results = {} # Dictionary to store results\n\n        # Create a pool of workers\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Give each topic to a worker by submitting the _process_topic task\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic\n                for topic in self.topics\n            }\n\n            # Wait for workers to finish and collect results\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    # Get the result from the finished worker\n                    result = future.result()\n                    self.topic_results[topic] = result # Store result\n                    logger.info(f\"Finished processing topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {e}\")\n                    # Store an error message for this topic\n                    self.topic_results[topic] = {\"error\": str(e)}\n```\n*   It creates a `ThreadPoolExecutor` which manages the worker threads.\n*   `executor.submit(self._process_topic, topic)` tells the executor: \"Hey, run the `_process_topic` function with this `topic` as input, using one of your available workers.\" This happens for *all* topics.\n*   `concurrent.futures.as_completed(...)` waits for any worker to finish, gets its result (`future.result()`), and stores it in `self.topic_results` dictionary, keyed by the topic name.\n\n**4. Processing a Single Topic (`_process_topic`)**\n\nThis helper function is what each worker thread actually runs. It creates and runs the specialized `TopicProcessorNode` for a single topic.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nfrom src.nodes.topic_processor_node import TopicProcessorNode # The worker node\n\n    def _process_topic(self, topic):\n        logger.info(f\"Worker starting on topic: {topic}\")\n        # Create the specialist node for *this specific topic*\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic # Configurable option\n        )\n        # Run the specialist node and get its results\n        # The result is a dictionary with 'qa_pairs' and 'eli5_content' for this topic\n        result_dict = processor.run()\n        # We only need the part specific to this topic from the processor's output\n        return result_dict[\"topic_results\"][topic]\n\n```\nThis function takes one `topic`, creates a `TopicProcessorNode` specifically for it (giving it the topic and the full transcript), runs that node, and returns the Q&A/ELI5 results just for that single topic.\n\n**5. Reduce Phase (`_reduce_phase`)**\n\nAfter the `_map_phase` has collected results from all workers into `self.topic_results`, the `_reduce_phase` combines them into the final structure needed in `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def _reduce_phase(self):\n        logger.info(\"Starting Reduce phase...\")\n        qa_pairs = {}\n        eli5_content = {}\n\n        # Go through results collected from workers\n        for topic, result in self.topic_results.items():\n            if \"error\" not in result: # Skip topics that had errors\n                qa_pairs[topic] = result.get(\"qa_pairs\", [])\n                eli5_content[topic] = result.get(\"eli5_content\", \"\")\n\n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        # Also store the raw results per topic\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        logger.info(\"Reduce phase complete. Results added to shared memory.\")\n```\nThis function iterates through the `self.topic_results` gathered during the map phase. It creates two dictionaries, `qa_pairs` and `eli5_content`, where the keys are the topic strings. It then populates these dictionaries with the results from each worker and finally adds these combined dictionaries to the main `shared_memory`.\n\n**6. Post-Processing (`post`)**\n\nThe `post` method mainly does final checks and logging.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def post(self):\n        # Basic check if any results were generated\n        if not self.topic_results:\n             self.shared_memory[\"error\"] = \"No topic results were generated\"\n             logger.error(self.shared_memory[\"error\"])\n             return # Stop if no results\n\n        logger.info(\"Topic Orchestrator Node completed successfully.\")\n```\n\nAnd that's how the `TopicOrchestratorNode` uses parallelism (Map-Reduce) to speed up the processing of multiple topics by delegating the work to `TopicProcessorNode` instances!\n\n## Conclusion\n\nIn this chapter, we explored the `TopicOrchestratorNode`, a smart manager within our pipeline. We learned:\n\n*   **Problem:** Analyzing many topics one-by-one is slow.\n*   **Solution:** Use parallelism with a **Map-Reduce** approach.\n*   **TopicOrchestratorNode:** Acts as a team lead, assigning each topic to a worker (`TopicProcessorNode`).\n*   **Parallel Execution:** Workers run simultaneously using threads, significantly speeding up the process.\n*   **Map Phase:** Assigns tasks to workers and runs them.\n*   **Reduce Phase:** Collects results from all workers and combines them.\n*   **Integration:** It reads `topics` and `transcript` from [Shared Memory](03_shared_memory.md) and writes back `qa_pairs` and `eli5_content`.\n\nThis node demonstrates a more advanced pattern within our pipeline architecture, showing how we can optimize for performance.\n\nSo far, we've seen how the pipeline flows, how individual nodes work, how they share data, and how we can use parallelism. But where does data like the transcript or video title actually come from? Next, we'll look at the tools used to interact with YouTube itself.\n\nLet's move on to [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)!\n\nRelevant Code Snippets:\n--- File: src/nodes/content_extraction_node.py ---\n\"\"\"\nContent Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\nfrom src.utils.logger import logger\n\nclass ContentExtractionNode(BaseNode):\n    \"\"\"\n    Node for extracting transcript from a YouTube video.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None):\n        \"\"\"\n        Initialize the node with shared memory.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n        \"\"\"\n        super().__init__(shared_memory)\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_id exists in shared memory.\n        \"\"\"\n        if \"video_id\" not in self.shared_memory:\n            error_msg = \"YouTube video ID not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Content Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(f\"Extracting transcript for video ID: {self.shared_memory['video_id']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute transcript extraction.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Extract the transcript\n        video_id = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Calling YouTube transcript API for video ID: {video_id}\")\n        transcript = extract_youtube_transcript(video_id)\n        \n        # Check if there was an error\n        if transcript.startswith(\"Error:\"):\n            logger.error(f\"Transcript extraction failed: {transcript}\")\n            self.shared_memory[\"error\"] = transcript\n            return\n        \n        self.shared_memory[\"transcript\"] = transcript\n        \n        # Log a preview of the transcript\n        preview_length = min(150, len(transcript))\n        logger.info(f\"Extracted transcript ({len(transcript)} characters)\")\n        logger.debug(f\"Transcript preview: {transcript[:preview_length]}...\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Content Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Failed to extract transcript\"\n            logger.error(f\"Error: {error_msg}\")\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        logger.info(\"Content Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample video ID\n    test_video_id = \"dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_id\": test_video_id}\n    \n    # Create and run the node\n    node = ContentExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    if \"error\" in updated_memory:\n        logger.error(f\"Error: {updated_memory['error']}\")\n    else:\n        transcript = updated_memory.get(\"transcript\", \"\")\n        logger.info(f\"Transcript length: {len(transcript)} characters\")\n        logger.info(f\"Transcript preview: {transcript[:200]}...\")\n\n\n--- File: src/nodes/input_processing_node.py ---\n\"\"\"\nInput Processing Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\nfrom src.utils.logger import logger\n\nclass InputProcessingNode(BaseNode):\n    \"\"\"\n    Node for validating YouTube URL and extracting video metadata.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, youtube_url=None):\n        \"\"\"\n        Initialize the node with shared memory and YouTube URL.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            youtube_url (str): YouTube URL to process\n        \"\"\"\n        super().__init__(shared_memory)\n        if youtube_url:\n            self.shared_memory[\"video_url\"] = youtube_url\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if video_url exists in shared memory.\n        \"\"\"\n        if \"video_url\" not in self.shared_memory:\n            error_msg = \"YouTube URL not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        logger.info(f\"Processing YouTube URL: {self.shared_memory['video_url']}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute URL validation and metadata extraction.\n        \"\"\"\n        # Validate the YouTube URL\n        youtube_url = self.shared_memory[\"video_url\"]\n        is_valid, video_id = validate_youtube_url(youtube_url)\n        \n        if not is_valid:\n            error_msg = f\"Invalid YouTube URL: {youtube_url}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"video_id\"] = video_id\n        logger.info(f\"Valid YouTube video ID: {video_id}\")\n        \n        # Extract video metadata\n        logger.debug(f\"Extracting metadata for video ID: {video_id}\")\n        metadata = extract_youtube_metadata(video_id)\n        \n        if \"error\" in metadata:\n            error_msg = f\"Error extracting metadata: {metadata['error']}\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        self.shared_memory[\"metadata\"] = metadata\n        logger.info(f\"Extracted metadata for video: {metadata.get('title', 'Unknown Title')}\")\n        logger.debug(f\"Video metadata: {metadata}\")\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Input Processing Node: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Input Processing Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test the node with a sample YouTube URL\n    test_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n    \n    # Initialize shared memory\n    shared_memory = {\"video_url\": test_url}\n    \n    # Create and run the node\n    node = InputProcessingNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Video ID: {updated_memory.get('video_id', 'Not found')}\")\n    logger.info(f\"Video Title: {updated_memory.get('metadata', {}).get('title', 'Not found')}\")\n    logger.info(f\"Channel: {updated_memory.get('metadata', {}).get('channel_name', 'Not found')}\")\n    logger.info(f\"Duration: {updated_memory.get('metadata', {}).get('duration', 'Not found')}\")\n\n\n--- File: src/utils/extract_youtube_metadata.py ---\n\"\"\"\nUtility function to extract metadata from YouTube videos.\n\"\"\"\nimport requests\nimport json\nimport os\nfrom datetime import datetime\n\ndef extract_youtube_metadata(video_id):\n    \"\"\"\n    Extracts metadata from a YouTube video using the YouTube Data API.\n    \n    Args:\n        video_id (str): The YouTube video ID\n        \n    Returns:\n        dict: A dictionary containing video metadata (title, channel, duration, etc.)\n    \"\"\"\n    # In a production environment, you would use an API key from environment variables\n    # For this example, we'll use a placeholder\n    # You would need to get your own API key from the Google Developer Console\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    \n    # Base URL for YouTube Data API v3\n    base_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    \n    # Parameters for the API request\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\",\n        \"id\": video_id,\n        \"key\": api_key\n    }\n    \n    try:\n        # Make the API request\n        response = requests.get(base_url, params=params)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Check if the video exists\n        if not data.get(\"items\"):\n            return {\n                \"error\": \"Video not found or API key invalid\",\n                \"video_id\": video_id\n            }\n        \n        # Extract relevant metadata\n        video_data = data[\"items\"][0]\n        snippet = video_data.get(\"snippet\", {})\n        content_details = video_data.get(\"contentDetails\", {})\n        statistics = video_data.get(\"statistics\", {})\n        \n        # Format the duration (ISO 8601 format to human-readable)\n        duration_iso = content_details.get(\"duration\", \"PT0S\")\n        # Simple conversion for common formats (not handling all ISO 8601 cases)\n        hours = 0\n        minutes = 0\n        seconds = 0\n        \n        if \"H\" in duration_iso:\n            hours = int(duration_iso.split(\"H\")[0].split(\"PT\")[1])\n            duration_iso = duration_iso.split(\"H\")[1]\n        elif \"PT\" in duration_iso:\n            duration_iso = duration_iso.split(\"PT\")[1]\n            \n        if \"M\" in duration_iso:\n            minutes = int(duration_iso.split(\"M\")[0])\n            duration_iso = duration_iso.split(\"M\")[1]\n            \n        if \"S\" in duration_iso:\n            seconds = int(duration_iso.split(\"S\")[0])\n        \n        duration_str = \"\"\n        if hours > 0:\n            duration_str += f\"{hours} hour{'s' if hours > 1 else ''} \"\n        if minutes > 0:\n            duration_str += f\"{minutes} minute{'s' if minutes > 1 else ''} \"\n        if seconds > 0 or (hours == 0 and minutes == 0):\n            duration_str += f\"{seconds} second{'s' if seconds > 1 else ''}\"\n        \n        # Format the published date\n        published_at = snippet.get(\"publishedAt\", \"\")\n        if published_at:\n            try:\n                published_date = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n                published_at = published_date.strftime(\"%B %d, %Y\")\n            except ValueError:\n                pass\n        \n        # Construct the metadata dictionary\n        metadata = {\n            \"video_id\": video_id,\n            \"title\": snippet.get(\"title\", \"Unknown Title\"),\n            \"description\": snippet.get(\"description\", \"\"),\n            \"channel_name\": snippet.get(\"channelTitle\", \"Unknown Channel\"),\n            \"channel_id\": snippet.get(\"channelId\", \"\"),\n            \"published_at\": published_at,\n            \"duration\": duration_str,\n            \"duration_seconds\": hours * 3600 + minutes * 60 + seconds,\n            \"view_count\": int(statistics.get(\"viewCount\", 0)),\n            \"like_count\": int(statistics.get(\"likeCount\", 0)),\n            \"comment_count\": int(statistics.get(\"commentCount\", 0)),\n            \"thumbnail_url\": snippet.get(\"thumbnails\", {}).get(\"high\", {}).get(\"url\", \"\"),\n            \"tags\": snippet.get(\"tags\", []),\n            \"category_id\": snippet.get(\"categoryId\", \"\"),\n        }\n        \n        return metadata\n        \n    except requests.exceptions.RequestException as e:\n        # Handle request errors\n        return {\n            \"error\": f\"API request error: {str(e)}\",\n            \"video_id\": video_id\n        }\n    except (KeyError, ValueError, json.JSONDecodeError) as e:\n        # Handle parsing errors\n        return {\n            \"error\": f\"Data parsing error: {str(e)}\",\n            \"video_id\": video_id\n        }\n\n\nif __name__ == \"__main__\":\n    # Test the function with an example video ID\n    test_video_id = \"dQw4w9WgXcQ\"  # Rick Astley - Never Gonna Give You Up\n    metadata = extract_youtube_metadata(test_video_id)\n    \n    # Print the metadata in a readable format\n    print(json.dumps(metadata, indent=2))\n\n\n--- File: src/utils/extract_youtube_transcript.py ---\n\"\"\"\nUtility function to extract transcript from YouTube videos.\n\"\"\"\nimport os\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    \"\"\"\n    Extracts the transcript/captions from a YouTube video.\n    \n    Args:\n        video_id (str): The YouTube video ID\n        \n    Returns:\n        str: The full transcript text or an error message\n    \"\"\"\n    try:\n        # Get available transcripts\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        \n        # Try to get English transcript first, fall back to other languages if needed\n        try:\n            transcript = transcript_list.find_transcript(['en'])\n        except NoTranscriptFound:\n            # If no English transcript is found, use the first available one\n            transcript = transcript_list.find_transcript([])\n            \n            # If it's not in English, try to translate it\n            if transcript.language_code != 'en':\n                transcript = transcript.translate('en')\n        \n        # Fetch the transcript data\n        transcript_data = transcript.fetch()\n        \n        # Combine all transcript segments into a single text\n        full_transcript = \"\"\n        for segment in transcript_data:\n            # The segment is a dictionary with 'text', 'start', and 'duration' keys\n            if isinstance(segment, dict) and 'text' in segment:\n                text = segment['text']\n            else:\n                # Handle FetchedTranscriptSnippet objects\n                text = segment.text if hasattr(segment, 'text') else str(segment)\n                \n            # Add a space if the text doesn't end with punctuation\n            if text and text[-1] not in '.!?':\n                text += ' '\n            full_transcript += text\n        \n        return full_transcript.strip()\n        \n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n\n\nif __name__ == \"__main__\":\n    # Test the function with an example video ID\n    test_video_id = \"dQw4w9WgXcQ\"  # Rick Astley - Never Gonna Give You Up\n    transcript = extract_youtube_transcript(test_video_id)\n    \n    # Print the first 500 characters of the transcript\n    print(transcript[:500] + \"...\" if len(transcript) > 500 else transcript)\n\n\n--- File: src/utils/validate_youtube_url.py ---\n\"\"\"\nUtility function to validate YouTube URLs and extract video IDs.\n\"\"\"\nimport re\nfrom urllib.parse import urlparse, parse_qs\n\ndef validate_youtube_url(url):\n    \"\"\"\n    Validates if the given URL is a valid YouTube video URL and extracts the video ID.\n    \n    Args:\n        url (str): The YouTube URL to validate\n        \n    Returns:\n        tuple: (is_valid (bool), video_id (str))\n    \"\"\"\n    if not url:\n        return False, \"\"\n    \n    # Regular expression patterns for different YouTube URL formats\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})'\n    )\n    \n    youtube_match = re.match(youtube_regex, url)\n    \n    if youtube_match:\n        video_id = youtube_match.group(6)\n        return True, video_id\n    \n    # Handle youtu.be format\n    parsed_url = urlparse(url)\n    if parsed_url.netloc == 'youtu.be':\n        video_id = parsed_url.path.lstrip('/')\n        if len(video_id) == 11:\n            return True, video_id\n    \n    # Handle youtube.com/watch?v= format\n    if parsed_url.netloc in ('youtube.com', 'www.youtube.com'):\n        query_params = parse_qs(parsed_url.query)\n        if 'v' in query_params:\n            video_id = query_params['v'][0]\n            if len(video_id) == 11:\n                return True, video_id\n    \n    return False, \"\"\n\n\nif __name__ == \"__main__\":\n    # Test the function with some example URLs\n    test_urls = [\n        \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\",\n        \"https://youtu.be/dQw4w9WgXcQ\",\n        \"https://youtube.com/embed/dQw4w9WgXcQ\",\n        \"https://www.youtube.com/shorts/dQw4w9WgXcQ\",\n        \"https://www.notayoutubeurl.com/watch?v=dQw4w9WgXcQ\",\n        \"invalid_url\"\n    ]\n    \n    for url in test_urls:\n        is_valid, video_id = validate_youtube_url(url)\n        print(f\"URL: {url}\")\n        print(f\"Valid: {is_valid}\")\n        print(f\"Video ID: {video_id}\")\n        print(\"-\" * 40)\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 5: YouTube Data Utilities`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 5: YouTube Data Utilities\n\nWelcome back! In the previous chapter, [Topic Processing Orchestrator](04_topic_processing_orchestrator.md), we saw how our pipeline cleverly analyzes multiple topics from a video transcript in parallel. We also saw in earlier chapters how Nodes like `InputProcessingNode` and `ContentExtractionNode` magically seem to get information *about* the video (like its title) and its transcript (the spoken words).\n\nBut where does this information actually *come from*? How does our program talk to YouTube? That's where our **YouTube Data Utilities** come in!\n\n## The Problem: Getting Data *From* YouTube\n\nOur summarizer project starts with just one thing: a YouTube video URL (like `https://www.youtube.com/watch?v=some_video_id`). To do anything useful, we need to extract specific pieces of information directly related to that URL:\n\n1.  **Is this even a real YouTube link?** We need to check if the URL points to a valid YouTube video before we try to process it.\n2.  **What is this video about?** We need basic details like the video's title, the channel that uploaded it, and maybe how long it is (its duration). We call this \"metadata\".\n3.  **What is being said in the video?** For topic analysis and summarization, we crucially need the spoken words, known as the transcript or captions.\n\nSimply having the URL isn't enough. We need tools to interact with YouTube (or YouTube data sources) to fetch this information.\n\n## Our Solution: Specialized YouTube Tools (Utilities)\n\nImagine you have a toolbox. You don't just have one giant \"Do-Everything\" tool. You have specific tools for specific jobs: a screwdriver for screws, a wrench for bolts, a hammer for nails.\n\nOur **YouTube Data Utilities** are like these specialized tools in our project's toolbox. They are *not* full pipeline [Nodes (Pipeline Step)](02_node__pipeline_step_.md) themselves. Instead, they are smaller, focused helper functions that our Nodes *use* to perform specific tasks related to YouTube.\n\nWe have three main utility tools in our `src/utils/` directory:\n\n1.  **The Link Checker (`validate_youtube_url`)**: Checks if a given text string looks like a valid YouTube URL and extracts the unique 11-character video ID.\n2.  **The Info Fetcher (`extract_youtube_metadata`)**: Takes a video ID and fetches details like the title, channel name, duration, etc., by talking to YouTube's official Data API.\n3.  **The Script Grabber (`extract_youtube_transcript`)**: Takes a video ID and retrieves the transcript (the spoken words) for that video using a specialized library.\n\nThese utilities live in their own files (like `validate_youtube_url.py`, `extract_youtube_metadata.py`, `extract_youtube_transcript.py`) inside the `src/utils/` folder, keeping our toolbox organized.\n\n## How Nodes Use These Utilities\n\nOur pipeline Nodes, which we learned about in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), act like the workers on the assembly line. When a worker needs to perform a YouTube-specific action, they reach into the toolbox and use the appropriate utility. This usually happens inside the Node's `exec()` method.\n\n**Example 1: Input Processing Node using the Link Checker and Info Fetcher**\n\nThe `InputProcessingNode`'s job is to take the initial URL, validate it, and get basic video info. Here's a simplified look at how its `exec` method might use our utilities:\n\n```python\n# Simplified from src/nodes/input_processing_node.py\n\n# Import the utility functions (tools)\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\n# ... other imports ...\n\nclass InputProcessingNode(BaseNode):\n    # ... (prep method gets the URL from shared_memory) ...\n\n    def exec(self):\n        # Get the URL we stored during prep\n        youtube_url = self.shared_memory[\"video_url\"]\n\n        # Use the 'Link Checker' utility\n        is_valid, video_id = validate_youtube_url(youtube_url)\n\n        if not is_valid:\n            # If the link is bad, report an error and stop\n            self.shared_memory[\"error\"] = \"Invalid YouTube URL\"\n            return\n\n        # Store the extracted video ID in shared memory\n        self.shared_memory[\"video_id\"] = video_id\n\n        # Use the 'Info Fetcher' utility with the video ID\n        metadata = extract_youtube_metadata(video_id)\n\n        # Check if fetching metadata failed\n        if \"error\" in metadata:\n            self.shared_memory[\"error\"] = metadata[\"error\"]\n            return\n\n        # Store the fetched metadata in shared memory\n        self.shared_memory[\"metadata\"] = metadata\n\n    # ... (post method logs success or failure) ...\n\n```\n\nIn this snippet:\n1.  The Node calls `validate_youtube_url` with the input URL.\n2.  If valid, it gets back `True` and the `video_id`. It stores the `video_id` in [Shared Memory](03_shared_memory.md).\n3.  It then calls `extract_youtube_metadata` with the `video_id`.\n4.  If successful, it gets back a dictionary of metadata (title, channel, etc.) and stores it in [Shared Memory](03_shared_memory.md).\n\n**Example 2: Content Extraction Node using the Script Grabber**\n\nThe `ContentExtractionNode`'s job is to get the transcript. It expects the `video_id` to already be in [Shared Memory](03_shared_memory.md) (put there by the `InputProcessingNode`).\n\n```python\n# Simplified from src/nodes/content_extraction_node.py\n\n# Import the utility function (tool)\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\n# ... other imports ...\n\nclass ContentExtractionNode(BaseNode):\n    # ... (prep method gets video_id from shared_memory) ...\n\n    def exec(self):\n        # Get the video_id we stored during prep\n        video_id = self.shared_memory[\"video_id\"]\n\n        # Use the 'Script Grabber' utility\n        transcript_text = extract_youtube_transcript(video_id)\n\n        # Check if fetching the transcript failed\n        if transcript_text.startswith(\"Error:\"):\n            self.shared_memory[\"error\"] = transcript_text\n            return\n\n        # Store the fetched transcript in shared memory\n        self.shared_memory[\"transcript\"] = transcript_text\n\n    # ... (post method logs success or failure) ...\n```\n\nHere:\n1.  The Node gets the `video_id` from [Shared Memory](03_shared_memory.md).\n2.  It calls `extract_youtube_transcript` with the `video_id`.\n3.  If successful, it gets back the transcript text as a string and stores it in [Shared Memory](03_shared_memory.md) under the key `\"transcript\"`.\n\nThese examples show how Nodes delegate specific YouTube interactions to our utility functions, keeping the Node's code focused on its main workflow (`prep`, `exec`, `post`).\n\n## Under the Hood: How the Utilities Work\n\nLet's briefly peek inside the toolbox and see how these tools operate.\n\n**1. Link Checker (`validate_youtube_url`)**\n\nThis utility doesn't actually connect to the internet. It just looks at the structure of the URL string you give it. It uses pattern matching (specifically, \"regular expressions\") to see if the URL *looks like* common YouTube video links (e.g., `youtube.com/watch?v=...` or `youtu.be/...`). If it finds a match, it extracts the 11-character part that is the video ID.\n\n```python\n# Simplified from src/utils/validate_youtube_url.py\nimport re # Python's regular expression module\n\ndef validate_youtube_url(url):\n    if not url: return False, \"\"\n\n    # A pattern to match YouTube URLs and capture the ID\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})' # Capture 11 chars\n    )\n    match = re.match(youtube_regex, url)\n\n    if match:\n        video_id = match.group(6) # Get the captured ID part\n        return True, video_id\n    else:\n        # (Also checks other formats like youtu.be/...)\n        return False, \"\"\n```\n\n**2. Info Fetcher (`extract_youtube_metadata`)**\n\nThis utility *does* connect to the internet. It talks to the official **YouTube Data API**, which is a service provided by Google/YouTube for developers to get video information programmatically.\n\n*   **Needs an API Key:** To use this API, you usually need a special key (like a password) to identify yourself. Our utility looks for this key (you'll need to get one from Google and configure it for the project).\n*   **Makes a Request:** It sends the `video_id` and the API key to a specific Google web address.\n*   **Gets a Response:** Google sends back structured data (in JSON format) containing the video's title, description, channel, duration, view count, etc.\n*   **Parses the Response:** The utility extracts the useful bits from the JSON response and returns them as a Python dictionary.\n\n```mermaid\nsequenceDiagram\n    participant Node as InputProcessingNode\n    participant MetadataUtil as extract_youtube_metadata()\n    participant YT_API as YouTube Data API (Google)\n\n    Node->>MetadataUtil: extract_youtube_metadata(\"some_video_id\")\n    Note over MetadataUtil: Needs API Key!\n    MetadataUtil->>YT_API: GET /youtube/v3/videos?id=some_video_id&key=API_KEY&part=snippet...\n    YT_API-->>MetadataUtil: Return JSON data { \"items\": [ { \"snippet\": { \"title\": \"...\", ... } } ] }\n    MetadataUtil->>MetadataUtil: Parse JSON, extract title, channel, etc.\n    MetadataUtil-->>Node: Return metadata_dictionary\n```\n\n```python\n# Simplified from src/utils/extract_youtube_metadata.py\nimport requests # Library to make web requests\nimport os\n\ndef extract_youtube_metadata(video_id):\n    # Get API key (needs to be set up separately)\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    if api_key == \"YOUR_API_KEY\":\n        return {\"error\": \"YouTube API Key not configured\"}\n\n    api_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\", # Ask for specific info\n        \"id\": video_id,\n        \"key\": api_key\n    }\n\n    try:\n        response = requests.get(api_url, params=params)\n        response.raise_for_status() # Check for web errors (like 404)\n        data = response.json() # Parse the JSON response\n\n        if not data.get(\"items\"):\n             return {\"error\": \"Video not found or API key invalid\"}\n\n        # ... (code to extract title, channel, duration etc. from data) ...\n        metadata = { \"title\": \"...\", \"channel_name\": \"...\", ... }\n        return metadata\n\n    except Exception as e:\n        return {\"error\": f\"API request error: {str(e)}\"}\n```\n\n**3. Script Grabber (`extract_youtube_transcript`)**\n\nThis utility uses a third-party Python library called `youtube_transcript_api`. This library knows how to find and download the captions or transcripts that YouTube provides for many videos.\n\n*   **Uses the Library:** It calls functions from the `youtube_transcript_api` library, passing the `video_id`.\n*   **Library Handles Complexity:** The library deals with finding available languages, downloading the transcript data (which includes text and timing information), and potentially translating it if needed.\n*   **Returns Text:** Our utility simplifies the result, combining all the text pieces into one single string and returning it. It also handles cases where transcripts are disabled or not found.\n\n```python\n# Simplified from src/utils/extract_youtube_transcript.py\n\n# Import the specialized library\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    try:\n        # Ask the library to find and fetch the transcript (prefer English)\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        transcript = transcript_list.find_transcript(['en'])\n        transcript_data = transcript.fetch()\n\n        # Combine the text parts into one string\n        full_transcript = \"\"\n        for segment in transcript_data:\n            full_transcript += segment['text'] + \" \"\n\n        return full_transcript.strip()\n\n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n```\n\n## Conclusion\n\nIn this chapter, we opened up the toolbox and looked at our **YouTube Data Utilities**. These are essential helper functions that allow our project to interact directly with YouTube data.\n\n*   They are **specialized tools** for specific tasks: validating URLs, fetching metadata, and grabbing transcripts.\n*   They are **not Nodes**, but rather **used by Nodes** (like `InputProcessingNode` and `ContentExtractionNode`) within their `exec` methods.\n*   They live in the `src/utils/` directory, keeping our code organized.\n*   They handle the details of pattern matching (`validate_youtube_url`), talking to the YouTube Data API (`extract_youtube_metadata`), and using libraries to get transcripts (`extract_youtube_transcript`).\n\nThese utilities provide the raw materials (metadata, transcript) that the rest of our pipeline ([Pipeline Orchestration](01_pipeline_orchestration.md)) processes to create the final summary.\n\nNow that we have the video's transcript, how do we actually analyze it to find topics, generate Q&A, or create ELI5 summaries? This often involves using powerful Artificial Intelligence models. Let's explore the utility designed to help us interact with these AI models next.\n\nReady to see how we talk to the AI? Let's move on to [Chapter 6: LLM Utility](06_llm_utility.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"LLM Utility\". This is Chapter 6.\n\nConcept Details:\n- Description:\nThis utility function provides a standardized way to communicate with a Large Language Model (like GPT-4). Nodes use this function to send prompts (instructions and context) to the AI and receive generated content, such as topics, answers to questions, or simplified explanations.\nIt acts like a dedicated phone line to a very knowledgeable expert (the LLM). You dial the number, ask your question (prompt), and get an answer back.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n---\n# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!\n---\n# Chapter 3: Shared Memory\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we saw our pipeline manager (`run_pipeline`) coordinating the different steps. In [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), we met the specialized workers (Nodes) that perform each task, like getting the video transcript or finding topics.\n\nBut how does the \"transcript extractor\" Node hand over the transcript it found to the \"topic finder\" Node? They need a way to pass information along the assembly line. That's where **Shared Memory** comes in!\n\n## The Problem: Passing Notes Between Workers\n\nImagine our assembly line again.\n1.  Worker A gets the video ID.\n2.  Worker B needs that video ID to fetch the transcript.\n3.  Worker C needs the transcript to find the main topics.\n\nHow does Worker A tell Worker B the video ID? How does Worker B give the transcript to Worker C? They can't just shout across the factory floor! They need a reliable way to share their results.\n\n## Our Solution: The Shared Whiteboard (A Python Dictionary)\n\nIn `pf1`, the **Shared Memory** is our solution. Think of it like a **shared whiteboard** or a **central notebook** that every worker (Node) in the pipeline can access.\n\n*   When a Node finishes its job, it writes its results onto the whiteboard (e.g., \"Video ID: xyz123\", \"Transcript: Hello world...\").\n*   When the next Node starts its job, it looks at the whiteboard to find the information it needs (e.g., it reads the \"Transcript\" section).\n\nTechnically, this \"whiteboard\" is just a standard **Python dictionary**. If you're new to Python dictionaries, they simply store information as **key-value pairs**.\n\n```python\n# A simple Python dictionary example\nmy_info = {\n  \"name\": \"Alex\",\n  \"task\": \"Write chapter\",\n  \"tools\": [\"laptop\", \"keyboard\"]\n}\n\n# Accessing information using the key\nprint(my_info[\"name\"])  # Output: Alex\n\n# Adding new information\nmy_info[\"status\"] = \"In progress\"\nprint(my_info)\n# Output: {'name': 'Alex', 'task': 'Write chapter', 'tools': ['laptop', 'keyboard'], 'status': 'In progress'}\n```\n\nOur `shared_memory` works exactly like this! It's a dictionary where keys are strings describing the data (like `\"video_url\"`, `\"video_id\"`, `\"transcript\"`, `\"topics\"`), and the values are the actual data (the URL string, the ID string, the long transcript text, a list of topic strings, etc.).\n\n## How Shared Memory Flows Through the Pipeline\n\n1.  **Initialization:** The pipeline orchestrator (`run_pipeline` in `src/main.py`) creates the `shared_memory` dictionary when the pipeline starts. Initially, it usually just contains the input needed for the very first step, like the YouTube video URL.\n\n    ```python\n    # From src/main.py (simplified)\n    def run_pipeline(youtube_url, ...):\n        # Create the shared memory dictionary\n        shared_memory = {\"video_url\": youtube_url}\n        logger.info(\"Shared memory initialized.\")\n        # ... rest of the pipeline ...\n    ```\n    At this point, `shared_memory` looks like: `{\"video_url\": \"https://youtube.com/watch?v=some_id\"}`\n\n2.  **Passing to Nodes:** The orchestrator passes this `shared_memory` dictionary to the first Node when it calls the Node's `run()` method.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... inside run_pipeline ...\n    \n    # 1. Input Processing Node\n    logger.info(\"Calling InputProcessingNode...\")\n    input_node = InputProcessingNode(shared_memory) # Give the Node access\n    shared_memory = input_node.run() # Node runs and returns the UPDATED dictionary\n    ```\n\n3.  **Nodes Read and Write:** Inside the Node's `run()` method, the `prep`, `exec`, and `post` methods interact with the dictionary:\n    *   **`prep()` reads:** It looks up the keys it needs (e.g., the `InputProcessingNode` reads `\"video_url\"`).\n    *   **`post()` writes:** After `exec()` does the work, `post()` adds the Node's results back into the *same* dictionary using new keys (e.g., the `InputProcessingNode` adds `\"video_id\"` and `\"metadata\"`).\n\n    ```python\n    # Inside InputProcessingNode (simplified)\n    \n    def prep(self):\n        # Read the URL provided by the orchestrator\n        self.url = self.shared_memory[\"video_url\"]\n        logger.debug(f\"Input node read URL: {self.url}\")\n\n    def exec(self):\n        # Do work: Get video ID and metadata from the URL\n        # (Simplified - actual logic calls YouTube utilities)\n        self.video_id_result = \"xyz123\"\n        self.metadata_result = {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n        logger.debug(\"Input node got video ID and metadata.\")\n\n    def post(self):\n        # Write the results back to the dictionary\n        self.shared_memory[\"video_id\"] = self.video_id_result\n        self.shared_memory[\"metadata\"] = self.metadata_result\n        logger.debug(\"Input node wrote video_id and metadata to shared memory.\")\n    ```\n\n4.  **Passing Updated Memory:** When the Node's `run()` method finishes, it returns the modified `shared_memory` dictionary back to the orchestrator. The orchestrator then passes this *updated* dictionary to the *next* Node in the sequence.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... after InputProcessingNode runs ...\n    # shared_memory now contains:\n    # {\n    #   \"video_url\": \"https://youtube.com/watch?v=some_id\",\n    #   \"video_id\": \"xyz123\",\n    #   \"metadata\": {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n    # }\n    \n    # 2. Content Extraction Node\n    logger.info(\"Calling ContentExtractionNode...\")\n    content_node = ContentExtractionNode(shared_memory) # Pass the UPDATED memory\n    shared_memory = content_node.run() # Get the next update\n    ```\n\n5.  **Chain Reaction:** This process repeats for every Node in the pipeline. Each Node reads the data left by previous Nodes and adds its own results.\n\n    *   `ContentExtractionNode` reads `\"video_id\"`, does its work, and adds `\"transcript\"`.\n    *   `TopicExtractionNode` reads `\"transcript\"`, does its work, and adds `\"topics\"`.\n    *   ...and so on.\n\nThe `shared_memory` dictionary grows as it passes through the pipeline, accumulating all the results.\n\n## Under the Hood: Passing the Dictionary Around\n\nIs this some magic \"shared\" space in the computer's memory? Not really! It's much simpler in our case.\n\nPython dictionaries are \"mutable\" objects. This means when you pass a dictionary to a function (or a Node's method), you're passing a reference to the *exact same* dictionary object in memory. If the function modifies the dictionary (like adding a new key), the original dictionary that was passed in also changes.\n\nSo, the orchestrator creates *one* dictionary object. It passes a reference to this object to Node 1. Node 1 modifies it directly. The orchestrator then passes the reference to the *same, now modified* object to Node 2, and so on.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline (Orchestrator)\n    participant SMDict as Shared Memory (Python Dictionary)\n    participant NodeA as InputProcessingNode\n    participant NodeB as ContentExtractionNode\n\n    Orch->>SMDict: Create { \"video_url\": \"...\" }\n    Orch->>NodeA: run(reference to SMDict)\n    NodeA->>NodeA: prep() [reads \"video_url\" from SMDict]\n    NodeA->>NodeA: exec() [gets video_id, metadata]\n    NodeA->>SMDict: post() [adds \"video_id\", \"metadata\"]\n    NodeA-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now modified\n\n    Orch->>NodeB: run(reference to SMDict)\n    NodeB->>NodeB: prep() [reads \"video_id\" from SMDict]\n    NodeB->>NodeB: exec() [gets transcript]\n    NodeB->>SMDict: post() [adds \"transcript\"]\n    NodeB-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now further modified\n```\n\nIt's like passing the *same physical notebook* from person to person. Each person adds their notes to it before passing it on.\n\n## Example: Reading and Writing\n\nLet's see how the `ContentExtractionNode` might read the `\"video_id\"` written by `InputProcessingNode` and then write the `\"transcript\"`.\n\n```python\n# Inside ContentExtractionNode (simplified)\n\ndef prep(self):\n    # Read the video_id written by the previous node\n    if \"video_id\" in self.shared_memory:\n        self.video_id_to_process = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Content node read video_id: {self.video_id_to_process}\")\n    else:\n        # Handle missing data (important!)\n        logger.error(\"Error: video_id not found in shared memory!\")\n        self.shared_memory[\"error\"] = \"Missing video_id for transcript extraction\"\n\ndef exec(self):\n    # Only run if prep was successful (no error)\n    if \"error\" not in self.shared_memory:\n        # Use self.video_id_to_process to get the transcript\n        # (Simplified - actual logic calls YouTube utilities)\n        self.transcript_result = \"This is the transcript text...\"\n        logger.debug(\"Content node got the transcript.\")\n\ndef post(self):\n    # Only run if prep and exec were successful\n    if \"error\" not in self.shared_memory:\n        # Write the transcript result to shared memory\n        self.shared_memory[\"transcript\"] = self.transcript_result\n        logger.debug(\"Content node wrote transcript to shared memory.\")\n```\nThis Node first checks if the key `\"video_id\"` exists in the `shared_memory` it received (this is crucial error handling!). If it exists, it uses that ID in its `exec` step. Finally, in `post`, it adds its own result under the key `\"transcript\"`. The next Node can now read `\"transcript\"`.\n\n## Why Use a Dictionary?\n\n*   **Simplicity:** It's a fundamental Python data structure, easy to understand and use.\n*   **Flexibility:** Dictionaries can hold various data types (strings, numbers, lists, even other dictionaries), which is perfect for the different kinds of results our Nodes produce.\n*   **Clarity:** Using meaningful keys (like `\"transcript\"`, `\"topics\"`) makes it clear what data is being passed around.\n\n## Conclusion\n\nWe've learned about **Shared Memory**, the crucial mechanism that allows our pipeline Nodes to communicate and share data.\n\n*   It's implemented as a simple **Python dictionary**.\n*   The pipeline orchestrator ([Chapter 1](01_pipeline_orchestration.md)) creates it and passes it sequentially to each Node ([Chapter 2](02_node__pipeline_step_.md)).\n*   Nodes **read** the inputs they need from the dictionary using keys (usually in `prep`).\n*   Nodes **write** their results back into the dictionary using new keys (usually in `post`).\n*   This allows data like the video ID, transcript, and topics to flow smoothly from one step to the next.\n\nUnderstanding Shared Memory is key to seeing how the different parts of our `pf1` project connect and build upon each other's work.\n\nNow that we understand the basic building blocks (Orchestration, Nodes, Shared Memory), we can look at more specialized components. Next, we'll explore a Node that acts as a mini-orchestrator itself, managing the detailed processing of topics.\n\nLet's dive into the [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)!\n---\n# Chapter 4: Topic Processing Orchestrator\n\nWelcome back! In [Chapter 3: Shared Memory](03_shared_memory.md), we learned how different steps (Nodes) in our pipeline share information using a central dictionary called `shared_memory`. We saw how the `TopicExtractionNode` adds a list of topics it found into this shared space.\n\nNow, imagine our video analyzer found 10 different topics in a long video! For each topic, we want to:\n1.  Generate some Questions and Answers (Q&A).\n2.  Create a super simple \"Explain Like I'm 5\" (ELI5) summary.\n\nIf we did this one topic at a time, it could take a while, especially since generating Q&A and ELI5 often involves asking an AI model (like the ones we'll see in [Chapter 6: LLM Utility](06_llm_utility.md)), which can be slow. How can we speed this up?\n\n## The Problem: Analyzing Many Topics Takes Time\n\nLet's say analyzing one topic takes 30 seconds. If we have 10 topics, doing them one after another would take 300 seconds (5 minutes!). That's a long coffee break!\n\n```\nTopic 1 Analysis --> Wait 30s --> Topic 2 Analysis --> Wait 30s --> ... --> Topic 10 Analysis --> Wait 30s\nTotal Time = 10 * 30s = 300s\n```\n\nWe need a smarter way to handle this.\n\n## The Solution: Divide and Conquer with Parallelism!\n\nThink about a big research project given to a team. Does the team lead make everyone wait while one person researches the first part, then the next person researches the second part? No! The lead divides the work: \"Alice, you research Topic A. Bob, you research Topic B. Charlie, you research Topic C. Go!\" Everyone works *at the same time* (in parallel). When they're done, the lead collects all their findings.\n\nThis is exactly what the **Topic Processing Orchestrator** (`TopicOrchestratorNode`) does! It acts like that efficient team lead for analyzing topics.\n\n*   **The Big Task:** Analyze all topics found in the video.\n*   **The Team Lead:** `TopicOrchestratorNode`.\n*   **The Team Members:** Specialized workers called `TopicProcessorNode` (we'll see these workers in action soon).\n*   **The Assignment:** The `TopicOrchestratorNode` gives *each* topic to a *separate* `TopicProcessorNode`.\n*   **Working in Parallel:** It lets all the `TopicProcessorNode`s work simultaneously to generate Q&A and ELI5 for their assigned topic.\n*   **Collecting Results:** Once all workers are finished, the `TopicOrchestratorNode` gathers their results and puts them together.\n\nIf analyzing one topic still takes 30 seconds, but we have, say, 3 workers (or more!) operating in parallel, we can analyze 3 topics in roughly 30 seconds. If we have enough workers for all 10 topics, the total time might be closer to just 30 seconds (plus a little overhead for managing them), instead of 300!\n\n```\n                                Topic 1 Analysis (Worker 1) \\\nSimultaneously Start -->        Topic 2 Analysis (Worker 2)  --> Wait ~30s --> All Done!\n                                Topic 3 Analysis (Worker 3) /\n                                ... and so on ...\n```\n\nThis parallel approach is sometimes called **Map-Reduce**:\n1.  **Map:** Assign (map) each piece of work (topic) to a worker (`TopicProcessorNode`).\n2.  **Reduce:** Collect (reduce) the results from all workers into a final combined output.\n\n## How the Topic Orchestrator Fits In\n\nThe `TopicOrchestratorNode` is just another [Node (Pipeline Step)](02_node__pipeline_step_.md) in our main pipeline, run by the [Pipeline Orchestration](01_pipeline_orchestration.md) (`run_pipeline` function).\n\nHere's what it does in the pipeline flow:\n\n1.  **Input:** It reads the `topics` (a list of strings) and the full `transcript` (a long string) from the [Shared Memory](03_shared_memory.md), put there by previous Nodes.\n2.  **Processing (Parallel):** It performs the \"Map-Reduce\" strategy described above, using multiple `TopicProcessorNode` instances internally.\n3.  **Output:** It writes the collected results back into [Shared Memory](03_shared_memory.md). The results are typically stored in dictionaries keyed by topic, for example:\n    *   `\"qa_pairs\"`: A dictionary where keys are topics and values are lists of Q&A for that topic.\n    *   `\"eli5_content\"`: A dictionary where keys are topics and values are the ELI5 explanations.\n    *   `\"topic_results\"`: A combined dictionary holding both Q&A and ELI5 for each topic.\n\n## Using the Topic Orchestrator Node\n\nWe don't usually call the `TopicOrchestratorNode` directly. The main `run_pipeline` function in `src/main.py` calls it after the topics have been extracted.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory contains 'topics' and 'transcript')\n\n# 4. Topic Orchestration Node\nlogger.info(\"[4/5] Starting Topic Processing Orchestration...\")\n# Create the orchestrator node, giving it shared memory\n# We can configure how many 'workers' (max_workers) it can use\ntopic_orchestrator = TopicOrchestratorNode(shared_memory, max_workers=3)\n\n# Run the node - this triggers the parallel processing\nshared_memory = topic_orchestrator.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"Topic Orchestration failed...\")\n    return shared_memory\n\nlogger.info(\"Successfully processed all topics.\")\n\n# Now, shared_memory contains the new keys like 'qa_pairs' and 'eli5_content'\n# Example: shared_memory['qa_pairs']['Machine Learning Basics'] might be a list of Q&A\n# Example: shared_memory['eli5_content']['Machine Learning Basics'] might be the ELI5 text\n\n# ... (Next node, like HTML Generation, can use these results) ...\n```\n\nIn this example:\n*   We create `TopicOrchestratorNode`, passing the current `shared_memory` (which must contain `\"topics\"` and `\"transcript\"`).\n*   We also tell it it can use up to `max_workers=3` parallel workers.\n*   Calling `topic_orchestrator.run()` kicks off the parallel processing.\n*   After it finishes, the `shared_memory` dictionary is updated with the results (`\"qa_pairs\"`, `\"eli5_content\"`, etc.).\n\n## Under the Hood: How Parallelism Works\n\nLet's visualize how the `TopicOrchestratorNode` (TON) manages the `TopicProcessorNode` (TPN) workers.\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline\n    participant TON as TopicOrchestratorNode\n    participant SM as Shared Memory\n    participant TPN1 as TopicProcessorNode (Worker 1)\n    participant TPN2 as TopicProcessorNode (Worker 2)\n\n    RP->>TON: run(SM) [SM has topics: T1, T2]\n    TON->>SM: prep() [Reads topics=[T1, T2], transcript]\n    Note over TON, TPN2: exec() starts Map Phase (using Thread Pool)\n    TON->>TPN1: Create & submit _process_topic(T1)\n    TON->>TPN2: Create & submit _process_topic(T2)\n    Note over TPN1, TPN2: TPN1 and TPN2 run in parallel\n    TPN1->>TPN1: Runs its own prep/exec/post (gets Q&A/ELI5 for T1)\n    TPN2->>TPN2: Runs its own prep/exec/post (gets Q&A/ELI5 for T2)\n    TPN1-->>TON: Returns result for T1\n    TPN2-->>TON: Returns result for T2\n    Note over TON: Map Phase ends, Reduce Phase starts\n    TON->>TON: _reduce_phase() [Collects results for T1, T2]\n    TON->>SM: post() [Writes combined 'qa_pairs', 'eli5_content' to SM]\n    TON-->>RP: Return updated SM\n```\n\nThis diagram shows:\n1.  `run_pipeline` calls the `TopicOrchestratorNode` (`TON`).\n2.  `TON` reads the list of topics (e.g., T1, T2) from `Shared Memory`.\n3.  `TON` starts its `exec` phase, creating and launching worker `TopicProcessorNode`s (`TPN1`, `TPN2`) in parallel, one for each topic.\n4.  Each `TPN` does its work (generating Q&A/ELI5 for its specific topic).\n5.  `TON` waits for all `TPN` workers to finish and collects their individual results.\n6.  `TON` combines these results and writes them back to `Shared Memory`.\n\n## Diving into the Code (`src/nodes/topic_orchestrator_node.py`)\n\nLet's peek inside the `TopicOrchestratorNode`. Remember, it follows the standard [Node (Pipeline Step)](02_node__pipeline_step_.md) structure: `prep`, `exec`, `post`.\n\n**1. Preparation (`prep`)**\n\nThe `prep` method gets the necessary inputs from `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\nclass TopicOrchestratorNode(BaseNode):\n    # ... (init method stores max_workers) ...\n\n    def prep(self):\n        logger.info(\"TopicOrchestrator: Preparing...\")\n        # Check if needed data exists in shared memory\n        if \"topics\" not in self.shared_memory:\n            raise ValueError(\"Topics not found in shared memory\")\n        if \"transcript\" not in self.shared_memory:\n            raise ValueError(\"Transcript not found in shared memory\")\n\n        # Get the topics and transcript\n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        logger.info(f\"Found {len(self.topics)} topics to process.\")\n```\nThis part just checks if the `\"topics\"` list and `\"transcript\"` string are available in the shared memory and stores them within the node instance for later use.\n\n**2. Execution (`exec`)**\n\nThe `exec` method orchestrates the Map and Reduce phases.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def exec(self):\n        logger.info(\"TopicOrchestrator: Executing...\")\n        # Map phase: Process topics in parallel\n        self._map_phase()\n\n        # Reduce phase: Combine the results\n        self._reduce_phase()\n```\nIt simply calls two helper methods: `_map_phase` to run the workers and `_reduce_phase` to collect the results.\n\n**3. Map Phase (`_map_phase`)**\n\nThis is where the parallelism happens using Python's `concurrent.futures.ThreadPoolExecutor`. Think of the executor as a manager for a pool of worker threads.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nimport concurrent.futures # Tool for running things in parallel\n\n    def _map_phase(self):\n        logger.info(f\"Starting Map phase with {self.max_workers} workers...\")\n        self.topic_results = {} # Dictionary to store results\n\n        # Create a pool of workers\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Give each topic to a worker by submitting the _process_topic task\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic\n                for topic in self.topics\n            }\n\n            # Wait for workers to finish and collect results\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    # Get the result from the finished worker\n                    result = future.result()\n                    self.topic_results[topic] = result # Store result\n                    logger.info(f\"Finished processing topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {e}\")\n                    # Store an error message for this topic\n                    self.topic_results[topic] = {\"error\": str(e)}\n```\n*   It creates a `ThreadPoolExecutor` which manages the worker threads.\n*   `executor.submit(self._process_topic, topic)` tells the executor: \"Hey, run the `_process_topic` function with this `topic` as input, using one of your available workers.\" This happens for *all* topics.\n*   `concurrent.futures.as_completed(...)` waits for any worker to finish, gets its result (`future.result()`), and stores it in `self.topic_results` dictionary, keyed by the topic name.\n\n**4. Processing a Single Topic (`_process_topic`)**\n\nThis helper function is what each worker thread actually runs. It creates and runs the specialized `TopicProcessorNode` for a single topic.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nfrom src.nodes.topic_processor_node import TopicProcessorNode # The worker node\n\n    def _process_topic(self, topic):\n        logger.info(f\"Worker starting on topic: {topic}\")\n        # Create the specialist node for *this specific topic*\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic # Configurable option\n        )\n        # Run the specialist node and get its results\n        # The result is a dictionary with 'qa_pairs' and 'eli5_content' for this topic\n        result_dict = processor.run()\n        # We only need the part specific to this topic from the processor's output\n        return result_dict[\"topic_results\"][topic]\n\n```\nThis function takes one `topic`, creates a `TopicProcessorNode` specifically for it (giving it the topic and the full transcript), runs that node, and returns the Q&A/ELI5 results just for that single topic.\n\n**5. Reduce Phase (`_reduce_phase`)**\n\nAfter the `_map_phase` has collected results from all workers into `self.topic_results`, the `_reduce_phase` combines them into the final structure needed in `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def _reduce_phase(self):\n        logger.info(\"Starting Reduce phase...\")\n        qa_pairs = {}\n        eli5_content = {}\n\n        # Go through results collected from workers\n        for topic, result in self.topic_results.items():\n            if \"error\" not in result: # Skip topics that had errors\n                qa_pairs[topic] = result.get(\"qa_pairs\", [])\n                eli5_content[topic] = result.get(\"eli5_content\", \"\")\n\n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        # Also store the raw results per topic\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        logger.info(\"Reduce phase complete. Results added to shared memory.\")\n```\nThis function iterates through the `self.topic_results` gathered during the map phase. It creates two dictionaries, `qa_pairs` and `eli5_content`, where the keys are the topic strings. It then populates these dictionaries with the results from each worker and finally adds these combined dictionaries to the main `shared_memory`.\n\n**6. Post-Processing (`post`)**\n\nThe `post` method mainly does final checks and logging.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def post(self):\n        # Basic check if any results were generated\n        if not self.topic_results:\n             self.shared_memory[\"error\"] = \"No topic results were generated\"\n             logger.error(self.shared_memory[\"error\"])\n             return # Stop if no results\n\n        logger.info(\"Topic Orchestrator Node completed successfully.\")\n```\n\nAnd that's how the `TopicOrchestratorNode` uses parallelism (Map-Reduce) to speed up the processing of multiple topics by delegating the work to `TopicProcessorNode` instances!\n\n## Conclusion\n\nIn this chapter, we explored the `TopicOrchestratorNode`, a smart manager within our pipeline. We learned:\n\n*   **Problem:** Analyzing many topics one-by-one is slow.\n*   **Solution:** Use parallelism with a **Map-Reduce** approach.\n*   **TopicOrchestratorNode:** Acts as a team lead, assigning each topic to a worker (`TopicProcessorNode`).\n*   **Parallel Execution:** Workers run simultaneously using threads, significantly speeding up the process.\n*   **Map Phase:** Assigns tasks to workers and runs them.\n*   **Reduce Phase:** Collects results from all workers and combines them.\n*   **Integration:** It reads `topics` and `transcript` from [Shared Memory](03_shared_memory.md) and writes back `qa_pairs` and `eli5_content`.\n\nThis node demonstrates a more advanced pattern within our pipeline architecture, showing how we can optimize for performance.\n\nSo far, we've seen how the pipeline flows, how individual nodes work, how they share data, and how we can use parallelism. But where does data like the transcript or video title actually come from? Next, we'll look at the tools used to interact with YouTube itself.\n\nLet's move on to [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)!\n---\n# Chapter 5: YouTube Data Utilities\n\nWelcome back! In the previous chapter, [Topic Processing Orchestrator](04_topic_processing_orchestrator.md), we saw how our pipeline cleverly analyzes multiple topics from a video transcript in parallel. We also saw in earlier chapters how Nodes like `InputProcessingNode` and `ContentExtractionNode` magically seem to get information *about* the video (like its title) and its transcript (the spoken words).\n\nBut where does this information actually *come from*? How does our program talk to YouTube? That's where our **YouTube Data Utilities** come in!\n\n## The Problem: Getting Data *From* YouTube\n\nOur summarizer project starts with just one thing: a YouTube video URL (like `https://www.youtube.com/watch?v=some_video_id`). To do anything useful, we need to extract specific pieces of information directly related to that URL:\n\n1.  **Is this even a real YouTube link?** We need to check if the URL points to a valid YouTube video before we try to process it.\n2.  **What is this video about?** We need basic details like the video's title, the channel that uploaded it, and maybe how long it is (its duration). We call this \"metadata\".\n3.  **What is being said in the video?** For topic analysis and summarization, we crucially need the spoken words, known as the transcript or captions.\n\nSimply having the URL isn't enough. We need tools to interact with YouTube (or YouTube data sources) to fetch this information.\n\n## Our Solution: Specialized YouTube Tools (Utilities)\n\nImagine you have a toolbox. You don't just have one giant \"Do-Everything\" tool. You have specific tools for specific jobs: a screwdriver for screws, a wrench for bolts, a hammer for nails.\n\nOur **YouTube Data Utilities** are like these specialized tools in our project's toolbox. They are *not* full pipeline [Nodes (Pipeline Step)](02_node__pipeline_step_.md) themselves. Instead, they are smaller, focused helper functions that our Nodes *use* to perform specific tasks related to YouTube.\n\nWe have three main utility tools in our `src/utils/` directory:\n\n1.  **The Link Checker (`validate_youtube_url`)**: Checks if a given text string looks like a valid YouTube URL and extracts the unique 11-character video ID.\n2.  **The Info Fetcher (`extract_youtube_metadata`)**: Takes a video ID and fetches details like the title, channel name, duration, etc., by talking to YouTube's official Data API.\n3.  **The Script Grabber (`extract_youtube_transcript`)**: Takes a video ID and retrieves the transcript (the spoken words) for that video using a specialized library.\n\nThese utilities live in their own files (like `validate_youtube_url.py`, `extract_youtube_metadata.py`, `extract_youtube_transcript.py`) inside the `src/utils/` folder, keeping our toolbox organized.\n\n## How Nodes Use These Utilities\n\nOur pipeline Nodes, which we learned about in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), act like the workers on the assembly line. When a worker needs to perform a YouTube-specific action, they reach into the toolbox and use the appropriate utility. This usually happens inside the Node's `exec()` method.\n\n**Example 1: Input Processing Node using the Link Checker and Info Fetcher**\n\nThe `InputProcessingNode`'s job is to take the initial URL, validate it, and get basic video info. Here's a simplified look at how its `exec` method might use our utilities:\n\n```python\n# Simplified from src/nodes/input_processing_node.py\n\n# Import the utility functions (tools)\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\n# ... other imports ...\n\nclass InputProcessingNode(BaseNode):\n    # ... (prep method gets the URL from shared_memory) ...\n\n    def exec(self):\n        # Get the URL we stored during prep\n        youtube_url = self.shared_memory[\"video_url\"]\n\n        # Use the 'Link Checker' utility\n        is_valid, video_id = validate_youtube_url(youtube_url)\n\n        if not is_valid:\n            # If the link is bad, report an error and stop\n            self.shared_memory[\"error\"] = \"Invalid YouTube URL\"\n            return\n\n        # Store the extracted video ID in shared memory\n        self.shared_memory[\"video_id\"] = video_id\n\n        # Use the 'Info Fetcher' utility with the video ID\n        metadata = extract_youtube_metadata(video_id)\n\n        # Check if fetching metadata failed\n        if \"error\" in metadata:\n            self.shared_memory[\"error\"] = metadata[\"error\"]\n            return\n\n        # Store the fetched metadata in shared memory\n        self.shared_memory[\"metadata\"] = metadata\n\n    # ... (post method logs success or failure) ...\n\n```\n\nIn this snippet:\n1.  The Node calls `validate_youtube_url` with the input URL.\n2.  If valid, it gets back `True` and the `video_id`. It stores the `video_id` in [Shared Memory](03_shared_memory.md).\n3.  It then calls `extract_youtube_metadata` with the `video_id`.\n4.  If successful, it gets back a dictionary of metadata (title, channel, etc.) and stores it in [Shared Memory](03_shared_memory.md).\n\n**Example 2: Content Extraction Node using the Script Grabber**\n\nThe `ContentExtractionNode`'s job is to get the transcript. It expects the `video_id` to already be in [Shared Memory](03_shared_memory.md) (put there by the `InputProcessingNode`).\n\n```python\n# Simplified from src/nodes/content_extraction_node.py\n\n# Import the utility function (tool)\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\n# ... other imports ...\n\nclass ContentExtractionNode(BaseNode):\n    # ... (prep method gets video_id from shared_memory) ...\n\n    def exec(self):\n        # Get the video_id we stored during prep\n        video_id = self.shared_memory[\"video_id\"]\n\n        # Use the 'Script Grabber' utility\n        transcript_text = extract_youtube_transcript(video_id)\n\n        # Check if fetching the transcript failed\n        if transcript_text.startswith(\"Error:\"):\n            self.shared_memory[\"error\"] = transcript_text\n            return\n\n        # Store the fetched transcript in shared memory\n        self.shared_memory[\"transcript\"] = transcript_text\n\n    # ... (post method logs success or failure) ...\n```\n\nHere:\n1.  The Node gets the `video_id` from [Shared Memory](03_shared_memory.md).\n2.  It calls `extract_youtube_transcript` with the `video_id`.\n3.  If successful, it gets back the transcript text as a string and stores it in [Shared Memory](03_shared_memory.md) under the key `\"transcript\"`.\n\nThese examples show how Nodes delegate specific YouTube interactions to our utility functions, keeping the Node's code focused on its main workflow (`prep`, `exec`, `post`).\n\n## Under the Hood: How the Utilities Work\n\nLet's briefly peek inside the toolbox and see how these tools operate.\n\n**1. Link Checker (`validate_youtube_url`)**\n\nThis utility doesn't actually connect to the internet. It just looks at the structure of the URL string you give it. It uses pattern matching (specifically, \"regular expressions\") to see if the URL *looks like* common YouTube video links (e.g., `youtube.com/watch?v=...` or `youtu.be/...`). If it finds a match, it extracts the 11-character part that is the video ID.\n\n```python\n# Simplified from src/utils/validate_youtube_url.py\nimport re # Python's regular expression module\n\ndef validate_youtube_url(url):\n    if not url: return False, \"\"\n\n    # A pattern to match YouTube URLs and capture the ID\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})' # Capture 11 chars\n    )\n    match = re.match(youtube_regex, url)\n\n    if match:\n        video_id = match.group(6) # Get the captured ID part\n        return True, video_id\n    else:\n        # (Also checks other formats like youtu.be/...)\n        return False, \"\"\n```\n\n**2. Info Fetcher (`extract_youtube_metadata`)**\n\nThis utility *does* connect to the internet. It talks to the official **YouTube Data API**, which is a service provided by Google/YouTube for developers to get video information programmatically.\n\n*   **Needs an API Key:** To use this API, you usually need a special key (like a password) to identify yourself. Our utility looks for this key (you'll need to get one from Google and configure it for the project).\n*   **Makes a Request:** It sends the `video_id` and the API key to a specific Google web address.\n*   **Gets a Response:** Google sends back structured data (in JSON format) containing the video's title, description, channel, duration, view count, etc.\n*   **Parses the Response:** The utility extracts the useful bits from the JSON response and returns them as a Python dictionary.\n\n```mermaid\nsequenceDiagram\n    participant Node as InputProcessingNode\n    participant MetadataUtil as extract_youtube_metadata()\n    participant YT_API as YouTube Data API (Google)\n\n    Node->>MetadataUtil: extract_youtube_metadata(\"some_video_id\")\n    Note over MetadataUtil: Needs API Key!\n    MetadataUtil->>YT_API: GET /youtube/v3/videos?id=some_video_id&key=API_KEY&part=snippet...\n    YT_API-->>MetadataUtil: Return JSON data { \"items\": [ { \"snippet\": { \"title\": \"...\", ... } } ] }\n    MetadataUtil->>MetadataUtil: Parse JSON, extract title, channel, etc.\n    MetadataUtil-->>Node: Return metadata_dictionary\n```\n\n```python\n# Simplified from src/utils/extract_youtube_metadata.py\nimport requests # Library to make web requests\nimport os\n\ndef extract_youtube_metadata(video_id):\n    # Get API key (needs to be set up separately)\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    if api_key == \"YOUR_API_KEY\":\n        return {\"error\": \"YouTube API Key not configured\"}\n\n    api_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\", # Ask for specific info\n        \"id\": video_id,\n        \"key\": api_key\n    }\n\n    try:\n        response = requests.get(api_url, params=params)\n        response.raise_for_status() # Check for web errors (like 404)\n        data = response.json() # Parse the JSON response\n\n        if not data.get(\"items\"):\n             return {\"error\": \"Video not found or API key invalid\"}\n\n        # ... (code to extract title, channel, duration etc. from data) ...\n        metadata = { \"title\": \"...\", \"channel_name\": \"...\", ... }\n        return metadata\n\n    except Exception as e:\n        return {\"error\": f\"API request error: {str(e)}\"}\n```\n\n**3. Script Grabber (`extract_youtube_transcript`)**\n\nThis utility uses a third-party Python library called `youtube_transcript_api`. This library knows how to find and download the captions or transcripts that YouTube provides for many videos.\n\n*   **Uses the Library:** It calls functions from the `youtube_transcript_api` library, passing the `video_id`.\n*   **Library Handles Complexity:** The library deals with finding available languages, downloading the transcript data (which includes text and timing information), and potentially translating it if needed.\n*   **Returns Text:** Our utility simplifies the result, combining all the text pieces into one single string and returning it. It also handles cases where transcripts are disabled or not found.\n\n```python\n# Simplified from src/utils/extract_youtube_transcript.py\n\n# Import the specialized library\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    try:\n        # Ask the library to find and fetch the transcript (prefer English)\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        transcript = transcript_list.find_transcript(['en'])\n        transcript_data = transcript.fetch()\n\n        # Combine the text parts into one string\n        full_transcript = \"\"\n        for segment in transcript_data:\n            full_transcript += segment['text'] + \" \"\n\n        return full_transcript.strip()\n\n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n```\n\n## Conclusion\n\nIn this chapter, we opened up the toolbox and looked at our **YouTube Data Utilities**. These are essential helper functions that allow our project to interact directly with YouTube data.\n\n*   They are **specialized tools** for specific tasks: validating URLs, fetching metadata, and grabbing transcripts.\n*   They are **not Nodes**, but rather **used by Nodes** (like `InputProcessingNode` and `ContentExtractionNode`) within their `exec` methods.\n*   They live in the `src/utils/` directory, keeping our code organized.\n*   They handle the details of pattern matching (`validate_youtube_url`), talking to the YouTube Data API (`extract_youtube_metadata`), and using libraries to get transcripts (`extract_youtube_transcript`).\n\nThese utilities provide the raw materials (metadata, transcript) that the rest of our pipeline ([Pipeline Orchestration](01_pipeline_orchestration.md)) processes to create the final summary.\n\nNow that we have the video's transcript, how do we actually analyze it to find topics, generate Q&A, or create ELI5 summaries? This often involves using powerful Artificial Intelligence models. Let's explore the utility designed to help us interact with these AI models next.\n\nReady to see how we talk to the AI? Let's move on to [Chapter 6: LLM Utility](06_llm_utility.md)!\n\nRelevant Code Snippets:\n--- File: src/nodes/topic_extraction_node.py ---\n\"\"\"\nTopic Extraction Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport concurrent.futures\nfrom typing import List, Dict, Any\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicExtractionNode(BaseNode):\n    \"\"\"\n    Node for identifying main topics from the video transcript.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, chunk_size=0, overlap=0, max_topics=5):\n        \"\"\"\n        Initialize the node with shared memory and processing parameters.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            chunk_size (int): Size of transcript chunks (0 = no chunking)\n            overlap (int): Overlap between chunks (when chunk_size > 0)\n            max_topics (int): Maximum number of topics to extract\n        \"\"\"\n        super().__init__(shared_memory)\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.max_topics = max_topics\n        self.chunks = []\n        self.chunk_topics = []\n        logger.debug(f\"TopicExtractionNode initialized with chunk_size={chunk_size}, overlap={overlap}, max_topics={max_topics}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if transcript exists in shared memory\n        and splitting it into chunks if chunk_size > 0.\n        \"\"\"\n        if \"transcript\" not in self.shared_memory:\n            error_msg = \"Transcript not found in shared memory\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping Topic Extraction due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        transcript = self.shared_memory[\"transcript\"]\n        logger.debug(f\"Preparing to extract topics from transcript ({len(transcript)} characters)\")\n        \n        # Only chunk if chunk_size is specified\n        if self.chunk_size > 0:\n            # Split into overlapping chunks\n            start = 0\n            while start < len(transcript):\n                end = min(start + self.chunk_size, len(transcript))\n                self.chunks.append(transcript[start:end])\n                start = end - self.overlap\n            logger.info(f\"Split transcript into {len(self.chunks)} chunks for processing\")\n            logger.debug(f\"Chunk sizes: {[len(chunk) for chunk in self.chunks]}\")\n        else:\n            # Process as a single chunk\n            self.chunks = [transcript]\n            logger.info(\"Processing transcript as a single chunk\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic extraction for each chunk and combine results.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Use ThreadPoolExecutor for parallel processing of chunks\n        max_workers = min(len(self.chunks), 4)  # Limit concurrent API calls\n        logger.info(f\"Processing {len(self.chunks)} chunks with {max_workers} parallel workers\")\n        \n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all chunk processing tasks\n            future_to_chunk = {\n                executor.submit(self._process_chunk, i, chunk): i \n                for i, chunk in enumerate(self.chunks)\n            }\n            \n            # Collect results as they complete\n            for future in concurrent.futures.as_completed(future_to_chunk):\n                chunk_index = future_to_chunk[future]\n                try:\n                    topics = future.result()\n                    if topics:\n                        self.chunk_topics.append(topics)\n                        logger.info(f\"Completed processing for chunk {chunk_index+1}/{len(self.chunks)}\")\n                except Exception as e:\n                    logger.error(f\"Error processing chunk {chunk_index+1}: {str(e)}\")\n                    \n        # If we couldn't extract any topics, add some default ones\n        if not self.chunk_topics:\n            logger.warning(\"No topics extracted, using default topics\")\n            self.chunk_topics.append([\"Main Content\", \"Key Points\", \"Summary\"])\n    \n    def _process_chunk(self, chunk_index: int, chunk: str) -> List[str]:\n        \"\"\"\n        Process a single chunk to extract topics.\n        \n        Args:\n            chunk_index (int): Index of the chunk\n            chunk (str): Content of the chunk\n            \n        Returns:\n            List[str]: Extracted topics\n        \"\"\"\n        logger.info(f\"Processing chunk {chunk_index+1}/{len(self.chunks)}...\")\n        \n        # Create prompt for topic extraction\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content and identifying main topics.\n        \n        I'll provide you with a transcript from a YouTube video. Your task is to:\n        1. Identify the main topics discussed in this segment\n        2. List each topic as a short, clear phrase (3-7 words)\n        3. Provide at most {self.max_topics} topics\n        4. Focus on substantive content, not introductions or conclusions\n        \n        Here is the transcript segment:\n        \n        {chunk[:2000]}...\n        \n        Respond with ONLY a JSON array of topic strings. For example:\n        [\"Topic One\", \"Topic Two\", \"Topic Three\"]\n        \"\"\")\n        \n        # Call LLM to extract topics\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1} (timeout: 30s)...\")\n            response = call_llm(prompt, temperature=0.3, max_tokens=200, timeout=30)\n            \n            # Check if we got an error response\n            if response.startswith(\"Error:\"):\n                logger.warning(f\"LLM API error for chunk {chunk_index+1}: {response}\")\n                # Add some default topics if we can't get them from the LLM\n                if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                    logger.info(\"Using default topics due to LLM error\")\n                    return [\"Main Content\", \"Key Points\", \"Summary\"]\n                return []\n            \n            logger.info(f\"Received LLM response for chunk {chunk_index+1} ({len(response)} characters)\")\n            logger.debug(f\"LLM response preview: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                import json\n                topics = json.loads(cleaned_response)\n                if isinstance(topics, list):\n                    logger.info(f\"Extracted {len(topics)} topics from chunk {chunk_index+1}\")\n                    logger.debug(f\"Topics from chunk {chunk_index+1}: {topics}\")\n                    return topics\n                else:\n                    logger.warning(f\"Expected list but got {type(topics)} from LLM\")\n                    # Try to extract topics from text response\n                    return [cleaned_response]\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response}\")\n                # Try to extract topics from text response\n                lines = cleaned_response.split(\"\\n\")\n                potential_topics = [line.strip().strip('\",[]') for line in lines if line.strip()]\n                logger.info(f\"Extracted {len(potential_topics)} topics from non-JSON response\")\n                return potential_topics\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # Add some default topics if we can't get them from the LLM\n            if chunk_index == 0:  # Only for the first chunk to avoid duplicates\n                logger.info(\"Using default topics due to exception\")\n                return [\"Main Content\", \"Key Points\", \"Summary\"]\n            return []\n    \n    def post(self):\n        \"\"\"\n        Post-process by combining topics from all chunks and selecting the most relevant ones.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in Topic Extraction Node: {self.shared_memory['error']}\")\n            return\n        \n        if not self.chunk_topics:\n            error_msg = \"Failed to extract topics from transcript\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        # Flatten the list of topics from all chunks\n        all_topics = []\n        for topics in self.chunk_topics:\n            all_topics.extend(topics)\n        \n        logger.debug(f\"All extracted topics (before deduplication): {all_topics}\")\n        \n        # Count topic occurrences to find the most common ones\n        topic_counts = {}\n        for topic in all_topics:\n            topic_lower = topic.lower()\n            topic_counts[topic_lower] = topic_counts.get(topic_lower, 0) + 1\n        \n        logger.debug(f\"Topic frequency counts: {topic_counts}\")\n        \n        # Sort topics by frequency\n        sorted_topics = sorted(\n            [(count, topic) for topic, count in topic_counts.items()], \n            reverse=True\n        )\n        \n        # Select the top topics (using the original casing)\n        top_topics = []\n        seen_topics = set()\n        \n        for _, topic_lower in sorted_topics:\n            # Find the original casing version\n            original_casing = next(\n                (t for t in all_topics if t.lower() == topic_lower), \n                topic_lower.title()\n            )\n            \n            # Skip if we've already added this topic\n            if topic_lower in seen_topics:\n                continue\n                \n            top_topics.append(original_casing)\n            seen_topics.add(topic_lower)\n            \n            # Limit to max_topics\n            if len(top_topics) >= self.max_topics:\n                break\n        \n        # Store the final list of topics\n        self.shared_memory[\"topics\"] = top_topics\n        \n        logger.info(f\"Extracted {len(top_topics)} final topics:\")\n        for i, topic in enumerate(top_topics):\n            logger.info(f\"  {i+1}. {topic}\")\n        \n        logger.info(\"Topic Extraction Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with a sample transcript\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \n    Now let's discuss some applications of machine learning. In healthcare, machine learning \n    is used for disease diagnosis, drug discovery, and personalized medicine. In finance, \n    it's used for fraud detection, algorithmic trading, and credit scoring. In transportation, \n    self-driving cars rely heavily on machine learning algorithms.\n    \n    Ethical considerations in machine learning include bias in training data, model interpretability, \n    and privacy concerns. It's important to develop responsible AI systems that are fair and transparent.\n    \"\"\"\n    \n    # Initialize shared memory\n    shared_memory = {\"transcript\": test_transcript}\n    \n    # Create and run the node\n    node = TopicExtractionNode(shared_memory)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"Topics: {updated_memory.get('topics', [])}\")\n\n\n--- File: src/nodes/topic_processor_node.py ---\n\"\"\"\nTopic Processor Node for YouTube Video Summarizer.\nProcesses a single topic with Q&A generation and ELI5 transformation.\n\"\"\"\nimport sys\nimport os\nimport textwrap\nimport json\n\n# Add the project root to the path so we can import from src.utils\nsys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.call_llm import call_llm\nfrom src.utils.logger import logger\n\nclass TopicProcessorNode(BaseNode):\n    \"\"\"\n    Node for processing a single topic, including Q&A generation and ELI5 transformation.\n    This node is designed to be used as part of a Map-Reduce pattern.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, topic=None, transcript=None, questions_per_topic=3):\n        \"\"\"\n        Initialize the node with topic and transcript.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary (optional)\n            topic (str): The topic to process\n            transcript (str): The video transcript\n            questions_per_topic (int): Number of questions to generate per topic\n        \"\"\"\n        super().__init__(shared_memory or {})\n        self.topic = topic\n        self.transcript = transcript\n        self.questions_per_topic = questions_per_topic\n        self.result = {\n            \"topic\": topic,\n            \"qa_pairs\": [],\n            \"eli5_content\": \"\"\n        }\n        logger.debug(f\"TopicProcessorNode initialized for topic: {topic}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if topic and transcript are available.\n        \"\"\"\n        # If topic and transcript were not provided in constructor, try to get from shared memory\n        if self.topic is None and \"current_topic\" in self.shared_memory:\n            self.topic = self.shared_memory[\"current_topic\"]\n            \n        if self.transcript is None and \"transcript\" in self.shared_memory:\n            self.transcript = self.shared_memory[\"transcript\"]\n            \n        if self.topic is None:\n            error_msg = \"No topic provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        if self.transcript is None:\n            error_msg = \"No transcript provided for processing\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n            \n        logger.info(f\"Processing topic: {self.topic}\")\n    \n    def exec(self):\n        \"\"\"\n        Execute topic processing: generate Q&A pairs and ELI5 explanation.\n        \"\"\"\n        # Step 1: Generate Q&A pairs\n        self._generate_qa_pairs()\n        \n        # Step 2: Create ELI5 explanation\n        self._create_eli5_explanation()\n    \n    def _generate_qa_pairs(self):\n        \"\"\"\n        Generate Q&A pairs for the topic.\n        \"\"\"\n        logger.info(f\"Generating Q&A pairs for topic: {self.topic}\")\n        \n        # Create prompt for Q&A generation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at creating educational content for videos.\n        \n        I'll provide you with a transcript from a YouTube video and a specific topic from that video.\n        Your task is to:\n        1. Generate {self.questions_per_topic} insightful questions about this topic\n        2. Provide clear, accurate answers to each question based on the transcript\n        3. Make sure the questions cover different aspects of the topic\n        4. Ensure answers are based only on information in the transcript\n        \n        Topic: {self.topic}\n        \n        Transcript:\n        {self.transcript[:4000]}...\n        \n        Respond with ONLY a JSON array of question-answer objects. For example:\n        [\n            {{\n                \"question\": \"What is the main purpose of X?\",\n                \"answer\": \"According to the video, the main purpose of X is...\"\n            }},\n            {{\n                \"question\": \"How does Y relate to Z?\",\n                \"answer\": \"The video explains that Y and Z are connected through...\"\n            }}\n        ]\n        \"\"\")\n        \n        # Call LLM to generate Q&A pairs\n        try:\n            logger.debug(f\"Calling LLM for Q&A generation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=1000)\n            logger.debug(f\"LLM response for Q&A generation: {response[:100]}...\")\n            \n            # Clean up the response to extract just the JSON array\n            cleaned_response = response.strip()\n            if cleaned_response.startswith(\"```json\"):\n                cleaned_response = cleaned_response.split(\"```json\")[1]\n            elif cleaned_response.startswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[1]\n            if cleaned_response.endswith(\"```\"):\n                cleaned_response = cleaned_response.split(\"```\")[0]\n            \n            # Try to parse as JSON, but handle errors gracefully\n            try:\n                qa_pairs = json.loads(cleaned_response)\n                if isinstance(qa_pairs, list):\n                    self.result[\"qa_pairs\"] = qa_pairs\n                    logger.info(f\"Generated {len(qa_pairs)} Q&A pairs for topic '{self.topic}'\")\n                    for i, qa in enumerate(qa_pairs):\n                        logger.debug(f\"  Q{i+1}: {qa.get('question', '')}\")\n                        logger.debug(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n                else:\n                    logger.warning(f\"Expected list but got {type(qa_pairs)} from LLM\")\n                    self.result[\"qa_pairs\"] = []\n            except json.JSONDecodeError:\n                logger.warning(f\"Could not parse LLM response as JSON: {cleaned_response[:200]}...\")\n                # Try to extract Q&A pairs from text response\n                extracted_qa = self._extract_qa_from_text(cleaned_response)\n                self.result[\"qa_pairs\"] = extracted_qa\n                logger.info(f\"Extracted {len(extracted_qa)} Q&A pairs from non-JSON response\")\n        except Exception as e:\n            logger.error(f\"Error generating Q&A pairs for topic '{self.topic}': {str(e)}\")\n            self.result[\"qa_pairs\"] = []\n    \n    def _extract_qa_from_text(self, text):\n        \"\"\"\n        Attempt to extract Q&A pairs from non-JSON text.\n        \n        Args:\n            text (str): Text containing Q&A pairs\n            \n        Returns:\n            list: List of Q&A pair dictionaries\n        \"\"\"\n        logger.debug(\"Attempting to extract Q&A pairs from non-JSON text\")\n        lines = text.split(\"\\n\")\n        qa_pairs = []\n        current_question = None\n        current_answer = \"\"\n        \n        for line in lines:\n            line = line.strip()\n            if not line:\n                continue\n                \n            # Check if line starts with Q: or Question:\n            if line.startswith((\"Q:\", \"Question:\")):\n                # If we have a previous question, save it\n                if current_question:\n                    qa_pairs.append({\n                        \"question\": current_question,\n                        \"answer\": current_answer.strip()\n                    })\n                \n                # Extract new question\n                current_question = line.split(\":\", 1)[1].strip()\n                current_answer = \"\"\n            # Check if line starts with A: or Answer:\n            elif line.startswith((\"A:\", \"Answer:\")) and current_question:\n                current_answer += line.split(\":\", 1)[1].strip() + \" \"\n            # Otherwise, add to current answer if we have a question\n            elif current_question:\n                current_answer += line + \" \"\n        \n        # Add the last Q&A pair if exists\n        if current_question:\n            qa_pairs.append({\n                \"question\": current_question,\n                \"answer\": current_answer.strip()\n            })\n            \n        logger.debug(f\"Extracted {len(qa_pairs)} Q&A pairs from text\")\n        return qa_pairs\n    \n    def _create_eli5_explanation(self):\n        \"\"\"\n        Create an ELI5 (Explain Like I'm 5) explanation for the topic.\n        \"\"\"\n        logger.info(f\"Creating ELI5 explanation for topic: {self.topic}\")\n        \n        # Get Q&A pairs for context\n        qa_text = \"\"\n        for qa in self.result[\"qa_pairs\"]:\n            question = qa.get(\"question\", \"\")\n            answer = qa.get(\"answer\", \"\")\n            qa_text += f\"Q: {question}\\nA: {answer}\\n\\n\"\n        \n        # Create prompt for ELI5 transformation\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at explaining complex topics to young children (5-7 years old).\n        \n        I'll provide you with a topic and some Q&A pairs about that topic from a YouTube video.\n        Your task is to:\n        1. Create a simple, friendly explanation of the topic that a 5-year-old would understand\n        2. Use simple words, short sentences, and concrete examples\n        3. Avoid jargon and technical terms\n        4. Use analogies to familiar concepts when possible\n        5. Keep the explanation under 200 words\n        6. Maintain the core information while simplifying the language\n        \n        Topic: {self.topic}\n        \n        Q&A Context:\n        {qa_text}\n        \n        Respond with ONLY the child-friendly explanation, without any introduction or meta-text.\n        \"\"\")\n        \n        # Call LLM to generate ELI5 explanation\n        try:\n            logger.debug(f\"Calling LLM for ELI5 transformation on topic '{self.topic}'\")\n            response = call_llm(prompt, temperature=0.7, max_tokens=500)\n            logger.debug(f\"LLM response for ELI5 transformation: {response[:100]}...\")\n            \n            # Clean up the response\n            explanation = response.strip()\n            \n            # Store the ELI5 explanation\n            self.result[\"eli5_content\"] = explanation\n            logger.info(f\"Generated ELI5 explanation for topic '{self.topic}' ({len(explanation)} characters)\")\n            \n        except Exception as e:\n            error_msg = f\"Error creating ELI5 explanation for topic '{self.topic}': {str(e)}\"\n            logger.error(error_msg)\n            self.result[\"eli5_content\"] = f\"Sorry, I couldn't create a simple explanation for {self.topic}.\"\n    \n    def post(self):\n        \"\"\"\n        Post-process and return the results.\n        \"\"\"\n        # Store the results in shared memory if needed\n        if \"topic_results\" not in self.shared_memory:\n            self.shared_memory[\"topic_results\"] = {}\n            \n        self.shared_memory[\"topic_results\"][self.topic] = self.result\n        \n        logger.info(f\"Topic processing completed for '{self.topic}'\")\n        logger.debug(f\"Generated {len(self.result['qa_pairs'])} Q&A pairs and {len(self.result['eli5_content'])} characters of ELI5 content\")\n        \n        return self.result\n\n\nif __name__ == \"__main__\":\n    # Test with a sample topic and transcript\n    test_topic = \"Machine Learning Basics\"\n    test_transcript = \"\"\"\n    In this video, we're going to talk about machine learning and its applications. \n    Machine learning is a subset of artificial intelligence that focuses on building systems \n    that learn from data. Unlike traditional programming where you explicitly program rules, \n    in machine learning, you train models on data and they learn patterns.\n    \n    There are several types of machine learning. First, supervised learning, where the model \n    is trained on labeled data. For example, you might have images labeled as \"cat\" or \"dog\" \n    and the model learns to distinguish between them. Second, unsupervised learning, where \n    the model finds patterns in unlabeled data. Clustering is a common unsupervised learning task.\n    \n    Deep learning is a subset of machine learning that uses neural networks with many layers. \n    These deep neural networks have revolutionized fields like computer vision and natural \n    language processing. For instance, convolutional neural networks (CNNs) are excellent at \n    image recognition tasks.\n    \"\"\"\n    \n    # Create and run the node\n    node = TopicProcessorNode(topic=test_topic, transcript=test_transcript)\n    result = node.run()\n    \n    # Print the results\n    logger.info(\"\\nTopic Processing Results:\")\n    logger.info(f\"Topic: {result['topic']}\")\n    logger.info(f\"Q&A Pairs: {len(result['qa_pairs'])}\")\n    for i, qa in enumerate(result['qa_pairs']):\n        logger.info(f\"  Q{i+1}: {qa.get('question', '')}\")\n        logger.info(f\"  A{i+1}: {qa.get('answer', '')[:100]}...\")\n    logger.info(f\"ELI5 Explanation: {result['eli5_content']}\")\n\n\n--- File: src/utils/call_llm.py ---\n\"\"\"\nUtility function to call a Large Language Model (LLM) API.\n\"\"\"\nimport os\nimport time\nfrom openai import OpenAI\nfrom openai import OpenAIError\nimport logging\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=1000, timeout=60):\n    \"\"\"\n    Calls an LLM API with the given prompt and returns the response.\n    \n    Args:\n        prompt (str): The prompt to send to the LLM\n        model (str): The model to use (default: gpt-4o)\n        temperature (float): Controls randomness (0.0-1.0)\n        max_tokens (int): Maximum number of tokens to generate\n        timeout (int): Maximum time to wait for a response in seconds\n        \n    Returns:\n        str: The LLM's response\n    \"\"\"\n    # Get API key from environment variable\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n    \n    if not api_key:\n        logging.error(\"OpenAI API key not found in environment variables\")\n        return \"Error: OpenAI API key not found in environment variables.\"\n    \n    # Log API key first few and last few characters for debugging\n    masked_key = f\"{api_key[:5]}...{api_key[-5:]}\"\n    logging.debug(f\"Using API key: {masked_key}\")\n    \n    # Initialize the OpenAI client\n    try:\n        client = OpenAI(\n            api_key=api_key\n        )\n        \n        # Make the API call with proper timeout handling\n        start_time = time.time()\n        logging.debug(f\"Starting OpenAI API call to model {model}\")\n        \n        try:\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                temperature=temperature,\n                max_tokens=max_tokens,\n                timeout=timeout\n            )\n            \n            # Calculate and log response time\n            elapsed = time.time() - start_time\n            logging.debug(f\"OpenAI API call completed in {elapsed:.2f} seconds\")\n            \n            # Extract and return the response content\n            content = response.choices[0].message.content\n            logging.debug(f\"Received response of length {len(content)} characters\")\n            return content\n            \n        except Exception as api_error:\n            elapsed = time.time() - start_time\n            logging.error(f\"OpenAI API error after {elapsed:.2f} seconds: {str(api_error)}\")\n            \n            # Handle specific error types\n            error_message = str(api_error).lower()\n            if \"timeout\" in error_message:\n                return f\"Error: LLM API call timed out after {elapsed:.1f} seconds.\"\n            elif \"rate limit\" in error_message:\n                return \"Error: Rate limit exceeded. Please try again later.\"\n            elif \"invalid auth\" in error_message or \"authentication\" in error_message:\n                return \"Error: Authentication failed. Please check your API key.\"\n            else:\n                return f\"Error calling LLM API: {str(api_error)}\"\n                \n    except Exception as e:\n        logging.exception(\"Unexpected error initializing OpenAI client\")\n        return f\"Error initializing OpenAI client: {str(e)}\"\n\n\nif __name__ == \"__main__\":\n    # Configure logging for testing\n    logging.basicConfig(level=logging.DEBUG)\n    \n    # Test the function with a simple prompt\n    test_prompt = \"Explain what a YouTube video summarizer does in one sentence.\"\n    \n    print(\"Testing call_llm function...\")\n    response = call_llm(test_prompt, max_tokens=100)\n    print(f\"Prompt: {test_prompt}\")\n    print(f\"Response: {response}\")\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 6: LLM Utility`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 6: LLM Utility\n\nWelcome back! In [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md), we saw how our project uses specialized tools to fetch raw data from YouTube, like the video's title and the full transcript (the spoken words). We have the text, but how do we make *sense* of it? How do we automatically figure out the main topics discussed, generate questions and answers about them, or create super-simple explanations?\n\nThat sounds like a job for a super-smart assistant!\n\n## The Problem: Understanding and Generating Content\n\nImagine you have the complete script of a movie. Just reading the script doesn't automatically tell you the main plot points, the underlying themes, or answers to specific questions about the story. You need to *analyze* it.\n\nSimilarly, our Nodes, like the `TopicExtractionNode` or the `TopicProcessorNode`, need to analyze the video transcript. They need to:\n*   Identify the core subjects being discussed.\n*   Maybe create questions a viewer might have about those subjects.\n*   Perhaps summarize a complex idea in simple terms.\n\nDoing this analysis accurately and creatively requires advanced understanding of language and context \u2013 something traditional programming struggles with. We need help from a powerful Artificial Intelligence (AI).\n\n## Our Solution: A Direct Line to an AI Expert (LLM Utility)\n\nModern AI includes something called **Large Language Models (LLMs)**. Think of models like GPT-4 (from OpenAI) as incredibly knowledgeable experts you can chat with. They have read vast amounts of text and can understand language, generate creative text, answer questions, summarize information, and much more.\n\nBut how do our different Nodes talk to this AI expert? We don't want each Node to figure out the complex details of connecting to the AI service, handling authentication, formatting requests, and parsing responses every time. That would be repetitive and complicated!\n\nSo, we created the **LLM Utility**. It's like having a **dedicated, standardized phone line** straight to our AI expert (the LLM).\n\n*   **Standardized:** Every Node uses the *same* simple function (`call_llm`) to talk to the AI.\n*   **Simplified:** The utility hides all the complex connection details. Nodes just need to know *what* question to ask.\n*   **Centralized:** All AI communication goes through this one utility, making it easier to manage and update.\n\nAny Node in our pipeline that needs help from the AI \u2013 whether it's the `TopicExtractionNode` asking \"What are the main topics here?\" or the `TopicProcessorNode` asking \"Explain this topic like I'm 5\" \u2013 uses this same utility function.\n\n## Asking the Right Question: The Prompt\n\nWhen you call an expert, you don't just say \"Hello?\". You need to give them context and clearly state what you need. When we talk to an LLM, this is called a **Prompt**.\n\nA prompt typically includes:\n1.  **Instructions:** What task should the AI perform? (e.g., \"Identify the main topics\", \"Generate questions and answers\", \"Explain this simply\")\n2.  **Context:** What information does the AI need to do the task? (e.g., the video transcript segment, the specific topic name)\n3.  **Format Constraints (Optional):** How should the AI format its answer? (e.g., \"Respond with ONLY a JSON list\", \"Keep it under 100 words\")\n\nHere's a simple example of a prompt the `TopicExtractionNode` might create:\n\n```text\nYou are an expert at analyzing video content.\nI have a segment of a video transcript. Please identify the 3 main topics discussed.\nList them as short phrases.\n\nTranscript Segment:\n[... a chunk of the video transcript text goes here ...]\n\nRespond with ONLY a JSON array of topic strings, like [\"Topic A\", \"Topic B\", \"Topic C\"].\n```\nThis prompt tells the AI its role, the task, provides the necessary text, and specifies the output format.\n\n## How Nodes Use the LLM Utility: `call_llm`\n\nOur Nodes use a single, simple function from our toolbox (`src/utils/`) called `call_llm`. This function takes the carefully crafted prompt and handles sending it to the AI and getting the response back.\n\nLet's look at a simplified example from the `TopicExtractionNode` (which we saw in the context provided earlier). Inside its `_process_chunk` method, it needs to find topics within a piece of the transcript (`chunk`):\n\n```python\n# Simplified from src/nodes/topic_extraction_node.py\n\n# Import the utility function\nfrom src.utils.call_llm import call_llm\n# ... other imports like textwrap ...\n\nclass TopicExtractionNode(BaseNode):\n    # ... other methods like __init__, prep ...\n\n    def _process_chunk(self, chunk_index: int, chunk: str):\n        logger.info(f\"Processing chunk {chunk_index+1}...\")\n\n        # 1. Create the prompt (instructions + context)\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content.\n        Identify the main topics in this transcript segment.\n        Provide at most {self.max_topics} topics as short phrases.\n\n        Transcript segment:\n        {chunk[:2000]}...\n\n        Respond with ONLY a JSON array of topic strings.\n        \"\"\")\n\n        # 2. Call the LLM Utility!\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1}...\")\n            # Just one simple function call to talk to the AI!\n            response_text = call_llm(\n                prompt=prompt,\n                temperature=0.3, # Lower temperature = more focused/deterministic output\n                max_tokens=200,  # Limit the length of the response\n                timeout=30       # Wait max 30 seconds for a response\n            )\n            logger.info(f\"Received LLM response for chunk {chunk_index+1}\")\n\n            # 3. Process the response (e.g., parse the JSON)\n            # ... (code to handle the response_text) ...\n            # extracted_topics = parse_json(response_text)\n            # return extracted_topics\n\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # ... (handle error) ...\n            return []\n\n```\n\nIn this example:\n1.  The Node constructs the `prompt` string with instructions and the relevant `chunk` of the transcript.\n2.  It makes a single call to `call_llm`, passing the `prompt`. It can also pass optional parameters like:\n    *   `temperature`: Controls creativity. Lower values (like 0.3) make the output more predictable and focused; higher values make it more random.\n    *   `max_tokens`: Limits how long the AI's response can be.\n    *   `timeout`: Sets a maximum time to wait for the AI to respond.\n3.  The `call_llm` function handles all the communication complexity. It returns the AI's response as a simple text string (`response_text`).\n4.  The Node then processes this `response_text` (e.g., parsing it if it expects JSON).\n\nThat's it! The Node doesn't need to worry about API keys, network requests, or complex error handling for the AI communication itself \u2013 the `call_llm` utility takes care of it.\n\n## Under the Hood: Dialing the Expert\n\nWhat actually happens when a Node calls `call_llm`? Let's trace the call.\n\n1.  **Get Credentials:** The `call_llm` function first needs the secret \"phone number\" and \"password\" to connect to the AI service (like OpenAI). This is usually an **API Key**, which it securely reads from the system's environment settings (so we don't store secrets directly in the code).\n2.  **Prepare the Call:** It packages the `prompt` and any other parameters (like `temperature`) into the specific format the AI service expects.\n3.  **Make the Connection:** It uses a library (like OpenAI's official Python library) to send the request over the internet to the AI service's servers.\n4.  **Wait for Response:** It waits for the AI to process the prompt and generate a response. This can take a few seconds. It also keeps an eye on the `timeout` limit.\n5.  **Receive Answer:** It gets the response text back from the AI service.\n6.  **Return or Report Error:** If successful, it returns the response text to the Node. If anything went wrong (bad API key, network error, timeout, rate limit), it logs the error and returns a helpful error message string.\n\nHere\u2019s a simplified diagram of this flow:\n\n```mermaid\nsequenceDiagram\n    participant Node as Pipeline Node (e.g., TopicExtraction)\n    participant LLMUtil as call_llm() Utility\n    participant OpenAI_Lib as OpenAI Python Library\n    participant OpenAI_API as OpenAI API Service\n\n    Node->>LLMUtil: call_llm(prompt=\"...\", ...)\n    LLMUtil->>LLMUtil: Get API Key (from environment)\n    Note right of LLMUtil: Prepare request data\n    LLMUtil->>OpenAI_Lib: client.chat.completions.create(...)\n    OpenAI_Lib->>OpenAI_API: Send HTTPS Request (prompt, key, etc.)\n    Note over OpenAI_API: Processes request...\n    OpenAI_API-->>OpenAI_Lib: Return HTTPS Response (text, usage, etc.)\n    OpenAI_Lib-->>LLMUtil: Return response object\n    LLMUtil->>LLMUtil: Extract response text\n    LLMUtil-->>Node: Return response string (or error message)\n```\n\n## Diving into the Code (`src/utils/call_llm.py`)\n\nLet's peek inside the `call_llm` function in `src/utils/call_llm.py`. Remember, we'll keep it simple!\n\n**1. Getting the API Key**\n\nThe function first needs to find the secret API key. It looks for an \"environment variable\" named `OPENAI_API_KEY`. You need to set this variable up in your system separately.\n\n```python\n# Simplified from src/utils/call_llm.py\nimport os\nimport logging\n\ndef call_llm(prompt, ...):\n    # Get API key from environment variable\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n    if not api_key:\n        logging.error(\"OpenAI API key not found!\")\n        return \"Error: OpenAI API key not found.\"\n\n    # ... (rest of the function)\n```\nThis code uses Python's `os` module to read the environment variable. If it's not found, it immediately returns an error message.\n\n**2. Setting up the Connection (Client)**\n\nNext, it uses the official `openai` library to create a \"client\" object, which knows how to talk to the OpenAI API.\n\n```python\n# Simplified from src/utils/call_llm.py\nfrom openai import OpenAI # The official library\n# ... other imports ...\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=1000, timeout=60):\n    # ... (get api_key) ...\n\n    try:\n        # Initialize the client with the API key\n        client = OpenAI(api_key=api_key)\n\n        # ... (make the API call using 'client') ...\n\n    except Exception as e:\n        logging.exception(\"Error initializing OpenAI client\")\n        return f\"Error initializing client: {str(e)}\"\n```\nThis sets up the communication channel using the API key.\n\n**3. Making the Call**\n\nNow, it uses the `client` object to actually send the prompt to the specified AI model.\n\n```python\n# Simplified from src/utils/call_llm.py\n# ... inside the 'try' block ...\n\n        try:\n            logging.debug(f\"Calling OpenAI model {model}...\")\n            # This is the actual call to the AI service\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}], # Pass the prompt\n                temperature=temperature,     # Pass other parameters\n                max_tokens=max_tokens,\n                timeout=timeout\n            )\n            logging.debug(\"OpenAI call successful.\")\n\n            # ... (extract the text response) ...\n\n        except Exception as api_error:\n            logging.error(f\"OpenAI API error: {str(api_error)}\")\n            # Handle specific errors like timeouts or rate limits\n            if \"timeout\" in str(api_error).lower():\n                 return \"Error: LLM API call timed out.\"\n            # ... other error handling ...\n            return f\"Error calling LLM API: {str(api_error)}\"\n\n```\nThe `client.chat.completions.create` method does the heavy lifting of communicating with OpenAI. We wrap this in another `try...except` block to catch specific API errors like timeouts.\n\n**4. Getting the Answer**\n\nIf the call is successful, the `response` object contains the AI's answer. We need to extract the actual text content.\n\n```python\n# Simplified from src/utils/call_llm.py\n# ... inside the successful API call 'try' block ...\n\n            # Extract the text from the response object\n            content = response.choices[0].message.content\n            logging.debug(f\"Received response length: {len(content)}\")\n            return content # Return the text to the Node\n\n```\nThe useful text is nested inside the `response` object. We extract it and return it as a simple string.\n\nThis utility simplifies AI interaction for all Nodes, providing a consistent and robust way to leverage the power of LLMs.\n\n## Conclusion\n\nIn this chapter, we learned about the **LLM Utility**, our project's standardized way of communicating with Large Language Models like GPT-4.\n\n*   **Problem:** Nodes need to perform complex language tasks (topic extraction, Q&A, summarization) that require AI.\n*   **Solution:** A central utility function (`call_llm`) provides a simple, consistent interface to the AI.\n*   **Analogy:** It's like a dedicated phone line to an AI expert.\n*   **Prompts:** Nodes craft instructions and context (`prompts`) to tell the AI what to do.\n*   **Usage:** Nodes call `call_llm(prompt, ...)` and receive the AI's response as text.\n*   **Under the Hood:** The utility handles API keys, network requests, timeouts, and basic error handling using the official OpenAI library.\n\nThis utility is a key component, enabling sophisticated analysis and content generation within our pipeline. Without it, tasks like topic extraction or creating ELI5 summaries would be much harder!\n\nNow that we've gathered video info ([Chapter 5](05_youtube_data_utilities.md)), extracted the transcript, and analyzed it using the LLM ([Chapter 6](06_llm_utility.md)) to get topics, Q&A, and summaries stored in [Shared Memory](03_shared_memory.md), how do we present all this information nicely to the user?\n\nLet's move on to the final step: putting everything together into a user-friendly report with [Chapter 7: HTML Generation](07_html_generation.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"HTML Generation\". This is Chapter 7.\n\nConcept Details:\n- Description:\nThis component is responsible for creating the final output: a user-friendly HTML file. It takes all the processed data (metadata, topics, Q&A, ELI5 explanations) and formats it into a structured web page with styling.\nIt's like a report generator or a desktop publisher that takes raw data and results, and lays them out nicely in a final document (the HTML page) for easy reading.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n---\n# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!\n---\n# Chapter 3: Shared Memory\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we saw our pipeline manager (`run_pipeline`) coordinating the different steps. In [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), we met the specialized workers (Nodes) that perform each task, like getting the video transcript or finding topics.\n\nBut how does the \"transcript extractor\" Node hand over the transcript it found to the \"topic finder\" Node? They need a way to pass information along the assembly line. That's where **Shared Memory** comes in!\n\n## The Problem: Passing Notes Between Workers\n\nImagine our assembly line again.\n1.  Worker A gets the video ID.\n2.  Worker B needs that video ID to fetch the transcript.\n3.  Worker C needs the transcript to find the main topics.\n\nHow does Worker A tell Worker B the video ID? How does Worker B give the transcript to Worker C? They can't just shout across the factory floor! They need a reliable way to share their results.\n\n## Our Solution: The Shared Whiteboard (A Python Dictionary)\n\nIn `pf1`, the **Shared Memory** is our solution. Think of it like a **shared whiteboard** or a **central notebook** that every worker (Node) in the pipeline can access.\n\n*   When a Node finishes its job, it writes its results onto the whiteboard (e.g., \"Video ID: xyz123\", \"Transcript: Hello world...\").\n*   When the next Node starts its job, it looks at the whiteboard to find the information it needs (e.g., it reads the \"Transcript\" section).\n\nTechnically, this \"whiteboard\" is just a standard **Python dictionary**. If you're new to Python dictionaries, they simply store information as **key-value pairs**.\n\n```python\n# A simple Python dictionary example\nmy_info = {\n  \"name\": \"Alex\",\n  \"task\": \"Write chapter\",\n  \"tools\": [\"laptop\", \"keyboard\"]\n}\n\n# Accessing information using the key\nprint(my_info[\"name\"])  # Output: Alex\n\n# Adding new information\nmy_info[\"status\"] = \"In progress\"\nprint(my_info)\n# Output: {'name': 'Alex', 'task': 'Write chapter', 'tools': ['laptop', 'keyboard'], 'status': 'In progress'}\n```\n\nOur `shared_memory` works exactly like this! It's a dictionary where keys are strings describing the data (like `\"video_url\"`, `\"video_id\"`, `\"transcript\"`, `\"topics\"`), and the values are the actual data (the URL string, the ID string, the long transcript text, a list of topic strings, etc.).\n\n## How Shared Memory Flows Through the Pipeline\n\n1.  **Initialization:** The pipeline orchestrator (`run_pipeline` in `src/main.py`) creates the `shared_memory` dictionary when the pipeline starts. Initially, it usually just contains the input needed for the very first step, like the YouTube video URL.\n\n    ```python\n    # From src/main.py (simplified)\n    def run_pipeline(youtube_url, ...):\n        # Create the shared memory dictionary\n        shared_memory = {\"video_url\": youtube_url}\n        logger.info(\"Shared memory initialized.\")\n        # ... rest of the pipeline ...\n    ```\n    At this point, `shared_memory` looks like: `{\"video_url\": \"https://youtube.com/watch?v=some_id\"}`\n\n2.  **Passing to Nodes:** The orchestrator passes this `shared_memory` dictionary to the first Node when it calls the Node's `run()` method.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... inside run_pipeline ...\n    \n    # 1. Input Processing Node\n    logger.info(\"Calling InputProcessingNode...\")\n    input_node = InputProcessingNode(shared_memory) # Give the Node access\n    shared_memory = input_node.run() # Node runs and returns the UPDATED dictionary\n    ```\n\n3.  **Nodes Read and Write:** Inside the Node's `run()` method, the `prep`, `exec`, and `post` methods interact with the dictionary:\n    *   **`prep()` reads:** It looks up the keys it needs (e.g., the `InputProcessingNode` reads `\"video_url\"`).\n    *   **`post()` writes:** After `exec()` does the work, `post()` adds the Node's results back into the *same* dictionary using new keys (e.g., the `InputProcessingNode` adds `\"video_id\"` and `\"metadata\"`).\n\n    ```python\n    # Inside InputProcessingNode (simplified)\n    \n    def prep(self):\n        # Read the URL provided by the orchestrator\n        self.url = self.shared_memory[\"video_url\"]\n        logger.debug(f\"Input node read URL: {self.url}\")\n\n    def exec(self):\n        # Do work: Get video ID and metadata from the URL\n        # (Simplified - actual logic calls YouTube utilities)\n        self.video_id_result = \"xyz123\"\n        self.metadata_result = {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n        logger.debug(\"Input node got video ID and metadata.\")\n\n    def post(self):\n        # Write the results back to the dictionary\n        self.shared_memory[\"video_id\"] = self.video_id_result\n        self.shared_memory[\"metadata\"] = self.metadata_result\n        logger.debug(\"Input node wrote video_id and metadata to shared memory.\")\n    ```\n\n4.  **Passing Updated Memory:** When the Node's `run()` method finishes, it returns the modified `shared_memory` dictionary back to the orchestrator. The orchestrator then passes this *updated* dictionary to the *next* Node in the sequence.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... after InputProcessingNode runs ...\n    # shared_memory now contains:\n    # {\n    #   \"video_url\": \"https://youtube.com/watch?v=some_id\",\n    #   \"video_id\": \"xyz123\",\n    #   \"metadata\": {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n    # }\n    \n    # 2. Content Extraction Node\n    logger.info(\"Calling ContentExtractionNode...\")\n    content_node = ContentExtractionNode(shared_memory) # Pass the UPDATED memory\n    shared_memory = content_node.run() # Get the next update\n    ```\n\n5.  **Chain Reaction:** This process repeats for every Node in the pipeline. Each Node reads the data left by previous Nodes and adds its own results.\n\n    *   `ContentExtractionNode` reads `\"video_id\"`, does its work, and adds `\"transcript\"`.\n    *   `TopicExtractionNode` reads `\"transcript\"`, does its work, and adds `\"topics\"`.\n    *   ...and so on.\n\nThe `shared_memory` dictionary grows as it passes through the pipeline, accumulating all the results.\n\n## Under the Hood: Passing the Dictionary Around\n\nIs this some magic \"shared\" space in the computer's memory? Not really! It's much simpler in our case.\n\nPython dictionaries are \"mutable\" objects. This means when you pass a dictionary to a function (or a Node's method), you're passing a reference to the *exact same* dictionary object in memory. If the function modifies the dictionary (like adding a new key), the original dictionary that was passed in also changes.\n\nSo, the orchestrator creates *one* dictionary object. It passes a reference to this object to Node 1. Node 1 modifies it directly. The orchestrator then passes the reference to the *same, now modified* object to Node 2, and so on.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline (Orchestrator)\n    participant SMDict as Shared Memory (Python Dictionary)\n    participant NodeA as InputProcessingNode\n    participant NodeB as ContentExtractionNode\n\n    Orch->>SMDict: Create { \"video_url\": \"...\" }\n    Orch->>NodeA: run(reference to SMDict)\n    NodeA->>NodeA: prep() [reads \"video_url\" from SMDict]\n    NodeA->>NodeA: exec() [gets video_id, metadata]\n    NodeA->>SMDict: post() [adds \"video_id\", \"metadata\"]\n    NodeA-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now modified\n\n    Orch->>NodeB: run(reference to SMDict)\n    NodeB->>NodeB: prep() [reads \"video_id\" from SMDict]\n    NodeB->>NodeB: exec() [gets transcript]\n    NodeB->>SMDict: post() [adds \"transcript\"]\n    NodeB-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now further modified\n```\n\nIt's like passing the *same physical notebook* from person to person. Each person adds their notes to it before passing it on.\n\n## Example: Reading and Writing\n\nLet's see how the `ContentExtractionNode` might read the `\"video_id\"` written by `InputProcessingNode` and then write the `\"transcript\"`.\n\n```python\n# Inside ContentExtractionNode (simplified)\n\ndef prep(self):\n    # Read the video_id written by the previous node\n    if \"video_id\" in self.shared_memory:\n        self.video_id_to_process = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Content node read video_id: {self.video_id_to_process}\")\n    else:\n        # Handle missing data (important!)\n        logger.error(\"Error: video_id not found in shared memory!\")\n        self.shared_memory[\"error\"] = \"Missing video_id for transcript extraction\"\n\ndef exec(self):\n    # Only run if prep was successful (no error)\n    if \"error\" not in self.shared_memory:\n        # Use self.video_id_to_process to get the transcript\n        # (Simplified - actual logic calls YouTube utilities)\n        self.transcript_result = \"This is the transcript text...\"\n        logger.debug(\"Content node got the transcript.\")\n\ndef post(self):\n    # Only run if prep and exec were successful\n    if \"error\" not in self.shared_memory:\n        # Write the transcript result to shared memory\n        self.shared_memory[\"transcript\"] = self.transcript_result\n        logger.debug(\"Content node wrote transcript to shared memory.\")\n```\nThis Node first checks if the key `\"video_id\"` exists in the `shared_memory` it received (this is crucial error handling!). If it exists, it uses that ID in its `exec` step. Finally, in `post`, it adds its own result under the key `\"transcript\"`. The next Node can now read `\"transcript\"`.\n\n## Why Use a Dictionary?\n\n*   **Simplicity:** It's a fundamental Python data structure, easy to understand and use.\n*   **Flexibility:** Dictionaries can hold various data types (strings, numbers, lists, even other dictionaries), which is perfect for the different kinds of results our Nodes produce.\n*   **Clarity:** Using meaningful keys (like `\"transcript\"`, `\"topics\"`) makes it clear what data is being passed around.\n\n## Conclusion\n\nWe've learned about **Shared Memory**, the crucial mechanism that allows our pipeline Nodes to communicate and share data.\n\n*   It's implemented as a simple **Python dictionary**.\n*   The pipeline orchestrator ([Chapter 1](01_pipeline_orchestration.md)) creates it and passes it sequentially to each Node ([Chapter 2](02_node__pipeline_step_.md)).\n*   Nodes **read** the inputs they need from the dictionary using keys (usually in `prep`).\n*   Nodes **write** their results back into the dictionary using new keys (usually in `post`).\n*   This allows data like the video ID, transcript, and topics to flow smoothly from one step to the next.\n\nUnderstanding Shared Memory is key to seeing how the different parts of our `pf1` project connect and build upon each other's work.\n\nNow that we understand the basic building blocks (Orchestration, Nodes, Shared Memory), we can look at more specialized components. Next, we'll explore a Node that acts as a mini-orchestrator itself, managing the detailed processing of topics.\n\nLet's dive into the [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)!\n---\n# Chapter 4: Topic Processing Orchestrator\n\nWelcome back! In [Chapter 3: Shared Memory](03_shared_memory.md), we learned how different steps (Nodes) in our pipeline share information using a central dictionary called `shared_memory`. We saw how the `TopicExtractionNode` adds a list of topics it found into this shared space.\n\nNow, imagine our video analyzer found 10 different topics in a long video! For each topic, we want to:\n1.  Generate some Questions and Answers (Q&A).\n2.  Create a super simple \"Explain Like I'm 5\" (ELI5) summary.\n\nIf we did this one topic at a time, it could take a while, especially since generating Q&A and ELI5 often involves asking an AI model (like the ones we'll see in [Chapter 6: LLM Utility](06_llm_utility.md)), which can be slow. How can we speed this up?\n\n## The Problem: Analyzing Many Topics Takes Time\n\nLet's say analyzing one topic takes 30 seconds. If we have 10 topics, doing them one after another would take 300 seconds (5 minutes!). That's a long coffee break!\n\n```\nTopic 1 Analysis --> Wait 30s --> Topic 2 Analysis --> Wait 30s --> ... --> Topic 10 Analysis --> Wait 30s\nTotal Time = 10 * 30s = 300s\n```\n\nWe need a smarter way to handle this.\n\n## The Solution: Divide and Conquer with Parallelism!\n\nThink about a big research project given to a team. Does the team lead make everyone wait while one person researches the first part, then the next person researches the second part? No! The lead divides the work: \"Alice, you research Topic A. Bob, you research Topic B. Charlie, you research Topic C. Go!\" Everyone works *at the same time* (in parallel). When they're done, the lead collects all their findings.\n\nThis is exactly what the **Topic Processing Orchestrator** (`TopicOrchestratorNode`) does! It acts like that efficient team lead for analyzing topics.\n\n*   **The Big Task:** Analyze all topics found in the video.\n*   **The Team Lead:** `TopicOrchestratorNode`.\n*   **The Team Members:** Specialized workers called `TopicProcessorNode` (we'll see these workers in action soon).\n*   **The Assignment:** The `TopicOrchestratorNode` gives *each* topic to a *separate* `TopicProcessorNode`.\n*   **Working in Parallel:** It lets all the `TopicProcessorNode`s work simultaneously to generate Q&A and ELI5 for their assigned topic.\n*   **Collecting Results:** Once all workers are finished, the `TopicOrchestratorNode` gathers their results and puts them together.\n\nIf analyzing one topic still takes 30 seconds, but we have, say, 3 workers (or more!) operating in parallel, we can analyze 3 topics in roughly 30 seconds. If we have enough workers for all 10 topics, the total time might be closer to just 30 seconds (plus a little overhead for managing them), instead of 300!\n\n```\n                                Topic 1 Analysis (Worker 1) \\\nSimultaneously Start -->        Topic 2 Analysis (Worker 2)  --> Wait ~30s --> All Done!\n                                Topic 3 Analysis (Worker 3) /\n                                ... and so on ...\n```\n\nThis parallel approach is sometimes called **Map-Reduce**:\n1.  **Map:** Assign (map) each piece of work (topic) to a worker (`TopicProcessorNode`).\n2.  **Reduce:** Collect (reduce) the results from all workers into a final combined output.\n\n## How the Topic Orchestrator Fits In\n\nThe `TopicOrchestratorNode` is just another [Node (Pipeline Step)](02_node__pipeline_step_.md) in our main pipeline, run by the [Pipeline Orchestration](01_pipeline_orchestration.md) (`run_pipeline` function).\n\nHere's what it does in the pipeline flow:\n\n1.  **Input:** It reads the `topics` (a list of strings) and the full `transcript` (a long string) from the [Shared Memory](03_shared_memory.md), put there by previous Nodes.\n2.  **Processing (Parallel):** It performs the \"Map-Reduce\" strategy described above, using multiple `TopicProcessorNode` instances internally.\n3.  **Output:** It writes the collected results back into [Shared Memory](03_shared_memory.md). The results are typically stored in dictionaries keyed by topic, for example:\n    *   `\"qa_pairs\"`: A dictionary where keys are topics and values are lists of Q&A for that topic.\n    *   `\"eli5_content\"`: A dictionary where keys are topics and values are the ELI5 explanations.\n    *   `\"topic_results\"`: A combined dictionary holding both Q&A and ELI5 for each topic.\n\n## Using the Topic Orchestrator Node\n\nWe don't usually call the `TopicOrchestratorNode` directly. The main `run_pipeline` function in `src/main.py` calls it after the topics have been extracted.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory contains 'topics' and 'transcript')\n\n# 4. Topic Orchestration Node\nlogger.info(\"[4/5] Starting Topic Processing Orchestration...\")\n# Create the orchestrator node, giving it shared memory\n# We can configure how many 'workers' (max_workers) it can use\ntopic_orchestrator = TopicOrchestratorNode(shared_memory, max_workers=3)\n\n# Run the node - this triggers the parallel processing\nshared_memory = topic_orchestrator.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"Topic Orchestration failed...\")\n    return shared_memory\n\nlogger.info(\"Successfully processed all topics.\")\n\n# Now, shared_memory contains the new keys like 'qa_pairs' and 'eli5_content'\n# Example: shared_memory['qa_pairs']['Machine Learning Basics'] might be a list of Q&A\n# Example: shared_memory['eli5_content']['Machine Learning Basics'] might be the ELI5 text\n\n# ... (Next node, like HTML Generation, can use these results) ...\n```\n\nIn this example:\n*   We create `TopicOrchestratorNode`, passing the current `shared_memory` (which must contain `\"topics\"` and `\"transcript\"`).\n*   We also tell it it can use up to `max_workers=3` parallel workers.\n*   Calling `topic_orchestrator.run()` kicks off the parallel processing.\n*   After it finishes, the `shared_memory` dictionary is updated with the results (`\"qa_pairs\"`, `\"eli5_content\"`, etc.).\n\n## Under the Hood: How Parallelism Works\n\nLet's visualize how the `TopicOrchestratorNode` (TON) manages the `TopicProcessorNode` (TPN) workers.\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline\n    participant TON as TopicOrchestratorNode\n    participant SM as Shared Memory\n    participant TPN1 as TopicProcessorNode (Worker 1)\n    participant TPN2 as TopicProcessorNode (Worker 2)\n\n    RP->>TON: run(SM) [SM has topics: T1, T2]\n    TON->>SM: prep() [Reads topics=[T1, T2], transcript]\n    Note over TON, TPN2: exec() starts Map Phase (using Thread Pool)\n    TON->>TPN1: Create & submit _process_topic(T1)\n    TON->>TPN2: Create & submit _process_topic(T2)\n    Note over TPN1, TPN2: TPN1 and TPN2 run in parallel\n    TPN1->>TPN1: Runs its own prep/exec/post (gets Q&A/ELI5 for T1)\n    TPN2->>TPN2: Runs its own prep/exec/post (gets Q&A/ELI5 for T2)\n    TPN1-->>TON: Returns result for T1\n    TPN2-->>TON: Returns result for T2\n    Note over TON: Map Phase ends, Reduce Phase starts\n    TON->>TON: _reduce_phase() [Collects results for T1, T2]\n    TON->>SM: post() [Writes combined 'qa_pairs', 'eli5_content' to SM]\n    TON-->>RP: Return updated SM\n```\n\nThis diagram shows:\n1.  `run_pipeline` calls the `TopicOrchestratorNode` (`TON`).\n2.  `TON` reads the list of topics (e.g., T1, T2) from `Shared Memory`.\n3.  `TON` starts its `exec` phase, creating and launching worker `TopicProcessorNode`s (`TPN1`, `TPN2`) in parallel, one for each topic.\n4.  Each `TPN` does its work (generating Q&A/ELI5 for its specific topic).\n5.  `TON` waits for all `TPN` workers to finish and collects their individual results.\n6.  `TON` combines these results and writes them back to `Shared Memory`.\n\n## Diving into the Code (`src/nodes/topic_orchestrator_node.py`)\n\nLet's peek inside the `TopicOrchestratorNode`. Remember, it follows the standard [Node (Pipeline Step)](02_node__pipeline_step_.md) structure: `prep`, `exec`, `post`.\n\n**1. Preparation (`prep`)**\n\nThe `prep` method gets the necessary inputs from `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\nclass TopicOrchestratorNode(BaseNode):\n    # ... (init method stores max_workers) ...\n\n    def prep(self):\n        logger.info(\"TopicOrchestrator: Preparing...\")\n        # Check if needed data exists in shared memory\n        if \"topics\" not in self.shared_memory:\n            raise ValueError(\"Topics not found in shared memory\")\n        if \"transcript\" not in self.shared_memory:\n            raise ValueError(\"Transcript not found in shared memory\")\n\n        # Get the topics and transcript\n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        logger.info(f\"Found {len(self.topics)} topics to process.\")\n```\nThis part just checks if the `\"topics\"` list and `\"transcript\"` string are available in the shared memory and stores them within the node instance for later use.\n\n**2. Execution (`exec`)**\n\nThe `exec` method orchestrates the Map and Reduce phases.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def exec(self):\n        logger.info(\"TopicOrchestrator: Executing...\")\n        # Map phase: Process topics in parallel\n        self._map_phase()\n\n        # Reduce phase: Combine the results\n        self._reduce_phase()\n```\nIt simply calls two helper methods: `_map_phase` to run the workers and `_reduce_phase` to collect the results.\n\n**3. Map Phase (`_map_phase`)**\n\nThis is where the parallelism happens using Python's `concurrent.futures.ThreadPoolExecutor`. Think of the executor as a manager for a pool of worker threads.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nimport concurrent.futures # Tool for running things in parallel\n\n    def _map_phase(self):\n        logger.info(f\"Starting Map phase with {self.max_workers} workers...\")\n        self.topic_results = {} # Dictionary to store results\n\n        # Create a pool of workers\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Give each topic to a worker by submitting the _process_topic task\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic\n                for topic in self.topics\n            }\n\n            # Wait for workers to finish and collect results\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    # Get the result from the finished worker\n                    result = future.result()\n                    self.topic_results[topic] = result # Store result\n                    logger.info(f\"Finished processing topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {e}\")\n                    # Store an error message for this topic\n                    self.topic_results[topic] = {\"error\": str(e)}\n```\n*   It creates a `ThreadPoolExecutor` which manages the worker threads.\n*   `executor.submit(self._process_topic, topic)` tells the executor: \"Hey, run the `_process_topic` function with this `topic` as input, using one of your available workers.\" This happens for *all* topics.\n*   `concurrent.futures.as_completed(...)` waits for any worker to finish, gets its result (`future.result()`), and stores it in `self.topic_results` dictionary, keyed by the topic name.\n\n**4. Processing a Single Topic (`_process_topic`)**\n\nThis helper function is what each worker thread actually runs. It creates and runs the specialized `TopicProcessorNode` for a single topic.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nfrom src.nodes.topic_processor_node import TopicProcessorNode # The worker node\n\n    def _process_topic(self, topic):\n        logger.info(f\"Worker starting on topic: {topic}\")\n        # Create the specialist node for *this specific topic*\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic # Configurable option\n        )\n        # Run the specialist node and get its results\n        # The result is a dictionary with 'qa_pairs' and 'eli5_content' for this topic\n        result_dict = processor.run()\n        # We only need the part specific to this topic from the processor's output\n        return result_dict[\"topic_results\"][topic]\n\n```\nThis function takes one `topic`, creates a `TopicProcessorNode` specifically for it (giving it the topic and the full transcript), runs that node, and returns the Q&A/ELI5 results just for that single topic.\n\n**5. Reduce Phase (`_reduce_phase`)**\n\nAfter the `_map_phase` has collected results from all workers into `self.topic_results`, the `_reduce_phase` combines them into the final structure needed in `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def _reduce_phase(self):\n        logger.info(\"Starting Reduce phase...\")\n        qa_pairs = {}\n        eli5_content = {}\n\n        # Go through results collected from workers\n        for topic, result in self.topic_results.items():\n            if \"error\" not in result: # Skip topics that had errors\n                qa_pairs[topic] = result.get(\"qa_pairs\", [])\n                eli5_content[topic] = result.get(\"eli5_content\", \"\")\n\n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        # Also store the raw results per topic\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        logger.info(\"Reduce phase complete. Results added to shared memory.\")\n```\nThis function iterates through the `self.topic_results` gathered during the map phase. It creates two dictionaries, `qa_pairs` and `eli5_content`, where the keys are the topic strings. It then populates these dictionaries with the results from each worker and finally adds these combined dictionaries to the main `shared_memory`.\n\n**6. Post-Processing (`post`)**\n\nThe `post` method mainly does final checks and logging.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def post(self):\n        # Basic check if any results were generated\n        if not self.topic_results:\n             self.shared_memory[\"error\"] = \"No topic results were generated\"\n             logger.error(self.shared_memory[\"error\"])\n             return # Stop if no results\n\n        logger.info(\"Topic Orchestrator Node completed successfully.\")\n```\n\nAnd that's how the `TopicOrchestratorNode` uses parallelism (Map-Reduce) to speed up the processing of multiple topics by delegating the work to `TopicProcessorNode` instances!\n\n## Conclusion\n\nIn this chapter, we explored the `TopicOrchestratorNode`, a smart manager within our pipeline. We learned:\n\n*   **Problem:** Analyzing many topics one-by-one is slow.\n*   **Solution:** Use parallelism with a **Map-Reduce** approach.\n*   **TopicOrchestratorNode:** Acts as a team lead, assigning each topic to a worker (`TopicProcessorNode`).\n*   **Parallel Execution:** Workers run simultaneously using threads, significantly speeding up the process.\n*   **Map Phase:** Assigns tasks to workers and runs them.\n*   **Reduce Phase:** Collects results from all workers and combines them.\n*   **Integration:** It reads `topics` and `transcript` from [Shared Memory](03_shared_memory.md) and writes back `qa_pairs` and `eli5_content`.\n\nThis node demonstrates a more advanced pattern within our pipeline architecture, showing how we can optimize for performance.\n\nSo far, we've seen how the pipeline flows, how individual nodes work, how they share data, and how we can use parallelism. But where does data like the transcript or video title actually come from? Next, we'll look at the tools used to interact with YouTube itself.\n\nLet's move on to [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)!\n---\n# Chapter 5: YouTube Data Utilities\n\nWelcome back! In the previous chapter, [Topic Processing Orchestrator](04_topic_processing_orchestrator.md), we saw how our pipeline cleverly analyzes multiple topics from a video transcript in parallel. We also saw in earlier chapters how Nodes like `InputProcessingNode` and `ContentExtractionNode` magically seem to get information *about* the video (like its title) and its transcript (the spoken words).\n\nBut where does this information actually *come from*? How does our program talk to YouTube? That's where our **YouTube Data Utilities** come in!\n\n## The Problem: Getting Data *From* YouTube\n\nOur summarizer project starts with just one thing: a YouTube video URL (like `https://www.youtube.com/watch?v=some_video_id`). To do anything useful, we need to extract specific pieces of information directly related to that URL:\n\n1.  **Is this even a real YouTube link?** We need to check if the URL points to a valid YouTube video before we try to process it.\n2.  **What is this video about?** We need basic details like the video's title, the channel that uploaded it, and maybe how long it is (its duration). We call this \"metadata\".\n3.  **What is being said in the video?** For topic analysis and summarization, we crucially need the spoken words, known as the transcript or captions.\n\nSimply having the URL isn't enough. We need tools to interact with YouTube (or YouTube data sources) to fetch this information.\n\n## Our Solution: Specialized YouTube Tools (Utilities)\n\nImagine you have a toolbox. You don't just have one giant \"Do-Everything\" tool. You have specific tools for specific jobs: a screwdriver for screws, a wrench for bolts, a hammer for nails.\n\nOur **YouTube Data Utilities** are like these specialized tools in our project's toolbox. They are *not* full pipeline [Nodes (Pipeline Step)](02_node__pipeline_step_.md) themselves. Instead, they are smaller, focused helper functions that our Nodes *use* to perform specific tasks related to YouTube.\n\nWe have three main utility tools in our `src/utils/` directory:\n\n1.  **The Link Checker (`validate_youtube_url`)**: Checks if a given text string looks like a valid YouTube URL and extracts the unique 11-character video ID.\n2.  **The Info Fetcher (`extract_youtube_metadata`)**: Takes a video ID and fetches details like the title, channel name, duration, etc., by talking to YouTube's official Data API.\n3.  **The Script Grabber (`extract_youtube_transcript`)**: Takes a video ID and retrieves the transcript (the spoken words) for that video using a specialized library.\n\nThese utilities live in their own files (like `validate_youtube_url.py`, `extract_youtube_metadata.py`, `extract_youtube_transcript.py`) inside the `src/utils/` folder, keeping our toolbox organized.\n\n## How Nodes Use These Utilities\n\nOur pipeline Nodes, which we learned about in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), act like the workers on the assembly line. When a worker needs to perform a YouTube-specific action, they reach into the toolbox and use the appropriate utility. This usually happens inside the Node's `exec()` method.\n\n**Example 1: Input Processing Node using the Link Checker and Info Fetcher**\n\nThe `InputProcessingNode`'s job is to take the initial URL, validate it, and get basic video info. Here's a simplified look at how its `exec` method might use our utilities:\n\n```python\n# Simplified from src/nodes/input_processing_node.py\n\n# Import the utility functions (tools)\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\n# ... other imports ...\n\nclass InputProcessingNode(BaseNode):\n    # ... (prep method gets the URL from shared_memory) ...\n\n    def exec(self):\n        # Get the URL we stored during prep\n        youtube_url = self.shared_memory[\"video_url\"]\n\n        # Use the 'Link Checker' utility\n        is_valid, video_id = validate_youtube_url(youtube_url)\n\n        if not is_valid:\n            # If the link is bad, report an error and stop\n            self.shared_memory[\"error\"] = \"Invalid YouTube URL\"\n            return\n\n        # Store the extracted video ID in shared memory\n        self.shared_memory[\"video_id\"] = video_id\n\n        # Use the 'Info Fetcher' utility with the video ID\n        metadata = extract_youtube_metadata(video_id)\n\n        # Check if fetching metadata failed\n        if \"error\" in metadata:\n            self.shared_memory[\"error\"] = metadata[\"error\"]\n            return\n\n        # Store the fetched metadata in shared memory\n        self.shared_memory[\"metadata\"] = metadata\n\n    # ... (post method logs success or failure) ...\n\n```\n\nIn this snippet:\n1.  The Node calls `validate_youtube_url` with the input URL.\n2.  If valid, it gets back `True` and the `video_id`. It stores the `video_id` in [Shared Memory](03_shared_memory.md).\n3.  It then calls `extract_youtube_metadata` with the `video_id`.\n4.  If successful, it gets back a dictionary of metadata (title, channel, etc.) and stores it in [Shared Memory](03_shared_memory.md).\n\n**Example 2: Content Extraction Node using the Script Grabber**\n\nThe `ContentExtractionNode`'s job is to get the transcript. It expects the `video_id` to already be in [Shared Memory](03_shared_memory.md) (put there by the `InputProcessingNode`).\n\n```python\n# Simplified from src/nodes/content_extraction_node.py\n\n# Import the utility function (tool)\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\n# ... other imports ...\n\nclass ContentExtractionNode(BaseNode):\n    # ... (prep method gets video_id from shared_memory) ...\n\n    def exec(self):\n        # Get the video_id we stored during prep\n        video_id = self.shared_memory[\"video_id\"]\n\n        # Use the 'Script Grabber' utility\n        transcript_text = extract_youtube_transcript(video_id)\n\n        # Check if fetching the transcript failed\n        if transcript_text.startswith(\"Error:\"):\n            self.shared_memory[\"error\"] = transcript_text\n            return\n\n        # Store the fetched transcript in shared memory\n        self.shared_memory[\"transcript\"] = transcript_text\n\n    # ... (post method logs success or failure) ...\n```\n\nHere:\n1.  The Node gets the `video_id` from [Shared Memory](03_shared_memory.md).\n2.  It calls `extract_youtube_transcript` with the `video_id`.\n3.  If successful, it gets back the transcript text as a string and stores it in [Shared Memory](03_shared_memory.md) under the key `\"transcript\"`.\n\nThese examples show how Nodes delegate specific YouTube interactions to our utility functions, keeping the Node's code focused on its main workflow (`prep`, `exec`, `post`).\n\n## Under the Hood: How the Utilities Work\n\nLet's briefly peek inside the toolbox and see how these tools operate.\n\n**1. Link Checker (`validate_youtube_url`)**\n\nThis utility doesn't actually connect to the internet. It just looks at the structure of the URL string you give it. It uses pattern matching (specifically, \"regular expressions\") to see if the URL *looks like* common YouTube video links (e.g., `youtube.com/watch?v=...` or `youtu.be/...`). If it finds a match, it extracts the 11-character part that is the video ID.\n\n```python\n# Simplified from src/utils/validate_youtube_url.py\nimport re # Python's regular expression module\n\ndef validate_youtube_url(url):\n    if not url: return False, \"\"\n\n    # A pattern to match YouTube URLs and capture the ID\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})' # Capture 11 chars\n    )\n    match = re.match(youtube_regex, url)\n\n    if match:\n        video_id = match.group(6) # Get the captured ID part\n        return True, video_id\n    else:\n        # (Also checks other formats like youtu.be/...)\n        return False, \"\"\n```\n\n**2. Info Fetcher (`extract_youtube_metadata`)**\n\nThis utility *does* connect to the internet. It talks to the official **YouTube Data API**, which is a service provided by Google/YouTube for developers to get video information programmatically.\n\n*   **Needs an API Key:** To use this API, you usually need a special key (like a password) to identify yourself. Our utility looks for this key (you'll need to get one from Google and configure it for the project).\n*   **Makes a Request:** It sends the `video_id` and the API key to a specific Google web address.\n*   **Gets a Response:** Google sends back structured data (in JSON format) containing the video's title, description, channel, duration, view count, etc.\n*   **Parses the Response:** The utility extracts the useful bits from the JSON response and returns them as a Python dictionary.\n\n```mermaid\nsequenceDiagram\n    participant Node as InputProcessingNode\n    participant MetadataUtil as extract_youtube_metadata()\n    participant YT_API as YouTube Data API (Google)\n\n    Node->>MetadataUtil: extract_youtube_metadata(\"some_video_id\")\n    Note over MetadataUtil: Needs API Key!\n    MetadataUtil->>YT_API: GET /youtube/v3/videos?id=some_video_id&key=API_KEY&part=snippet...\n    YT_API-->>MetadataUtil: Return JSON data { \"items\": [ { \"snippet\": { \"title\": \"...\", ... } } ] }\n    MetadataUtil->>MetadataUtil: Parse JSON, extract title, channel, etc.\n    MetadataUtil-->>Node: Return metadata_dictionary\n```\n\n```python\n# Simplified from src/utils/extract_youtube_metadata.py\nimport requests # Library to make web requests\nimport os\n\ndef extract_youtube_metadata(video_id):\n    # Get API key (needs to be set up separately)\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    if api_key == \"YOUR_API_KEY\":\n        return {\"error\": \"YouTube API Key not configured\"}\n\n    api_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\", # Ask for specific info\n        \"id\": video_id,\n        \"key\": api_key\n    }\n\n    try:\n        response = requests.get(api_url, params=params)\n        response.raise_for_status() # Check for web errors (like 404)\n        data = response.json() # Parse the JSON response\n\n        if not data.get(\"items\"):\n             return {\"error\": \"Video not found or API key invalid\"}\n\n        # ... (code to extract title, channel, duration etc. from data) ...\n        metadata = { \"title\": \"...\", \"channel_name\": \"...\", ... }\n        return metadata\n\n    except Exception as e:\n        return {\"error\": f\"API request error: {str(e)}\"}\n```\n\n**3. Script Grabber (`extract_youtube_transcript`)**\n\nThis utility uses a third-party Python library called `youtube_transcript_api`. This library knows how to find and download the captions or transcripts that YouTube provides for many videos.\n\n*   **Uses the Library:** It calls functions from the `youtube_transcript_api` library, passing the `video_id`.\n*   **Library Handles Complexity:** The library deals with finding available languages, downloading the transcript data (which includes text and timing information), and potentially translating it if needed.\n*   **Returns Text:** Our utility simplifies the result, combining all the text pieces into one single string and returning it. It also handles cases where transcripts are disabled or not found.\n\n```python\n# Simplified from src/utils/extract_youtube_transcript.py\n\n# Import the specialized library\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    try:\n        # Ask the library to find and fetch the transcript (prefer English)\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        transcript = transcript_list.find_transcript(['en'])\n        transcript_data = transcript.fetch()\n\n        # Combine the text parts into one string\n        full_transcript = \"\"\n        for segment in transcript_data:\n            full_transcript += segment['text'] + \" \"\n\n        return full_transcript.strip()\n\n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n```\n\n## Conclusion\n\nIn this chapter, we opened up the toolbox and looked at our **YouTube Data Utilities**. These are essential helper functions that allow our project to interact directly with YouTube data.\n\n*   They are **specialized tools** for specific tasks: validating URLs, fetching metadata, and grabbing transcripts.\n*   They are **not Nodes**, but rather **used by Nodes** (like `InputProcessingNode` and `ContentExtractionNode`) within their `exec` methods.\n*   They live in the `src/utils/` directory, keeping our code organized.\n*   They handle the details of pattern matching (`validate_youtube_url`), talking to the YouTube Data API (`extract_youtube_metadata`), and using libraries to get transcripts (`extract_youtube_transcript`).\n\nThese utilities provide the raw materials (metadata, transcript) that the rest of our pipeline ([Pipeline Orchestration](01_pipeline_orchestration.md)) processes to create the final summary.\n\nNow that we have the video's transcript, how do we actually analyze it to find topics, generate Q&A, or create ELI5 summaries? This often involves using powerful Artificial Intelligence models. Let's explore the utility designed to help us interact with these AI models next.\n\nReady to see how we talk to the AI? Let's move on to [Chapter 6: LLM Utility](06_llm_utility.md)!\n---\n# Chapter 6: LLM Utility\n\nWelcome back! In [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md), we saw how our project uses specialized tools to fetch raw data from YouTube, like the video's title and the full transcript (the spoken words). We have the text, but how do we make *sense* of it? How do we automatically figure out the main topics discussed, generate questions and answers about them, or create super-simple explanations?\n\nThat sounds like a job for a super-smart assistant!\n\n## The Problem: Understanding and Generating Content\n\nImagine you have the complete script of a movie. Just reading the script doesn't automatically tell you the main plot points, the underlying themes, or answers to specific questions about the story. You need to *analyze* it.\n\nSimilarly, our Nodes, like the `TopicExtractionNode` or the `TopicProcessorNode`, need to analyze the video transcript. They need to:\n*   Identify the core subjects being discussed.\n*   Maybe create questions a viewer might have about those subjects.\n*   Perhaps summarize a complex idea in simple terms.\n\nDoing this analysis accurately and creatively requires advanced understanding of language and context \u2013 something traditional programming struggles with. We need help from a powerful Artificial Intelligence (AI).\n\n## Our Solution: A Direct Line to an AI Expert (LLM Utility)\n\nModern AI includes something called **Large Language Models (LLMs)**. Think of models like GPT-4 (from OpenAI) as incredibly knowledgeable experts you can chat with. They have read vast amounts of text and can understand language, generate creative text, answer questions, summarize information, and much more.\n\nBut how do our different Nodes talk to this AI expert? We don't want each Node to figure out the complex details of connecting to the AI service, handling authentication, formatting requests, and parsing responses every time. That would be repetitive and complicated!\n\nSo, we created the **LLM Utility**. It's like having a **dedicated, standardized phone line** straight to our AI expert (the LLM).\n\n*   **Standardized:** Every Node uses the *same* simple function (`call_llm`) to talk to the AI.\n*   **Simplified:** The utility hides all the complex connection details. Nodes just need to know *what* question to ask.\n*   **Centralized:** All AI communication goes through this one utility, making it easier to manage and update.\n\nAny Node in our pipeline that needs help from the AI \u2013 whether it's the `TopicExtractionNode` asking \"What are the main topics here?\" or the `TopicProcessorNode` asking \"Explain this topic like I'm 5\" \u2013 uses this same utility function.\n\n## Asking the Right Question: The Prompt\n\nWhen you call an expert, you don't just say \"Hello?\". You need to give them context and clearly state what you need. When we talk to an LLM, this is called a **Prompt**.\n\nA prompt typically includes:\n1.  **Instructions:** What task should the AI perform? (e.g., \"Identify the main topics\", \"Generate questions and answers\", \"Explain this simply\")\n2.  **Context:** What information does the AI need to do the task? (e.g., the video transcript segment, the specific topic name)\n3.  **Format Constraints (Optional):** How should the AI format its answer? (e.g., \"Respond with ONLY a JSON list\", \"Keep it under 100 words\")\n\nHere's a simple example of a prompt the `TopicExtractionNode` might create:\n\n```text\nYou are an expert at analyzing video content.\nI have a segment of a video transcript. Please identify the 3 main topics discussed.\nList them as short phrases.\n\nTranscript Segment:\n[... a chunk of the video transcript text goes here ...]\n\nRespond with ONLY a JSON array of topic strings, like [\"Topic A\", \"Topic B\", \"Topic C\"].\n```\nThis prompt tells the AI its role, the task, provides the necessary text, and specifies the output format.\n\n## How Nodes Use the LLM Utility: `call_llm`\n\nOur Nodes use a single, simple function from our toolbox (`src/utils/`) called `call_llm`. This function takes the carefully crafted prompt and handles sending it to the AI and getting the response back.\n\nLet's look at a simplified example from the `TopicExtractionNode` (which we saw in the context provided earlier). Inside its `_process_chunk` method, it needs to find topics within a piece of the transcript (`chunk`):\n\n```python\n# Simplified from src/nodes/topic_extraction_node.py\n\n# Import the utility function\nfrom src.utils.call_llm import call_llm\n# ... other imports like textwrap ...\n\nclass TopicExtractionNode(BaseNode):\n    # ... other methods like __init__, prep ...\n\n    def _process_chunk(self, chunk_index: int, chunk: str):\n        logger.info(f\"Processing chunk {chunk_index+1}...\")\n\n        # 1. Create the prompt (instructions + context)\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content.\n        Identify the main topics in this transcript segment.\n        Provide at most {self.max_topics} topics as short phrases.\n\n        Transcript segment:\n        {chunk[:2000]}...\n\n        Respond with ONLY a JSON array of topic strings.\n        \"\"\")\n\n        # 2. Call the LLM Utility!\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1}...\")\n            # Just one simple function call to talk to the AI!\n            response_text = call_llm(\n                prompt=prompt,\n                temperature=0.3, # Lower temperature = more focused/deterministic output\n                max_tokens=200,  # Limit the length of the response\n                timeout=30       # Wait max 30 seconds for a response\n            )\n            logger.info(f\"Received LLM response for chunk {chunk_index+1}\")\n\n            # 3. Process the response (e.g., parse the JSON)\n            # ... (code to handle the response_text) ...\n            # extracted_topics = parse_json(response_text)\n            # return extracted_topics\n\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # ... (handle error) ...\n            return []\n\n```\n\nIn this example:\n1.  The Node constructs the `prompt` string with instructions and the relevant `chunk` of the transcript.\n2.  It makes a single call to `call_llm`, passing the `prompt`. It can also pass optional parameters like:\n    *   `temperature`: Controls creativity. Lower values (like 0.3) make the output more predictable and focused; higher values make it more random.\n    *   `max_tokens`: Limits how long the AI's response can be.\n    *   `timeout`: Sets a maximum time to wait for the AI to respond.\n3.  The `call_llm` function handles all the communication complexity. It returns the AI's response as a simple text string (`response_text`).\n4.  The Node then processes this `response_text` (e.g., parsing it if it expects JSON).\n\nThat's it! The Node doesn't need to worry about API keys, network requests, or complex error handling for the AI communication itself \u2013 the `call_llm` utility takes care of it.\n\n## Under the Hood: Dialing the Expert\n\nWhat actually happens when a Node calls `call_llm`? Let's trace the call.\n\n1.  **Get Credentials:** The `call_llm` function first needs the secret \"phone number\" and \"password\" to connect to the AI service (like OpenAI). This is usually an **API Key**, which it securely reads from the system's environment settings (so we don't store secrets directly in the code).\n2.  **Prepare the Call:** It packages the `prompt` and any other parameters (like `temperature`) into the specific format the AI service expects.\n3.  **Make the Connection:** It uses a library (like OpenAI's official Python library) to send the request over the internet to the AI service's servers.\n4.  **Wait for Response:** It waits for the AI to process the prompt and generate a response. This can take a few seconds. It also keeps an eye on the `timeout` limit.\n5.  **Receive Answer:** It gets the response text back from the AI service.\n6.  **Return or Report Error:** If successful, it returns the response text to the Node. If anything went wrong (bad API key, network error, timeout, rate limit), it logs the error and returns a helpful error message string.\n\nHere\u2019s a simplified diagram of this flow:\n\n```mermaid\nsequenceDiagram\n    participant Node as Pipeline Node (e.g., TopicExtraction)\n    participant LLMUtil as call_llm() Utility\n    participant OpenAI_Lib as OpenAI Python Library\n    participant OpenAI_API as OpenAI API Service\n\n    Node->>LLMUtil: call_llm(prompt=\"...\", ...)\n    LLMUtil->>LLMUtil: Get API Key (from environment)\n    Note right of LLMUtil: Prepare request data\n    LLMUtil->>OpenAI_Lib: client.chat.completions.create(...)\n    OpenAI_Lib->>OpenAI_API: Send HTTPS Request (prompt, key, etc.)\n    Note over OpenAI_API: Processes request...\n    OpenAI_API-->>OpenAI_Lib: Return HTTPS Response (text, usage, etc.)\n    OpenAI_Lib-->>LLMUtil: Return response object\n    LLMUtil->>LLMUtil: Extract response text\n    LLMUtil-->>Node: Return response string (or error message)\n```\n\n## Diving into the Code (`src/utils/call_llm.py`)\n\nLet's peek inside the `call_llm` function in `src/utils/call_llm.py`. Remember, we'll keep it simple!\n\n**1. Getting the API Key**\n\nThe function first needs to find the secret API key. It looks for an \"environment variable\" named `OPENAI_API_KEY`. You need to set this variable up in your system separately.\n\n```python\n# Simplified from src/utils/call_llm.py\nimport os\nimport logging\n\ndef call_llm(prompt, ...):\n    # Get API key from environment variable\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n    if not api_key:\n        logging.error(\"OpenAI API key not found!\")\n        return \"Error: OpenAI API key not found.\"\n\n    # ... (rest of the function)\n```\nThis code uses Python's `os` module to read the environment variable. If it's not found, it immediately returns an error message.\n\n**2. Setting up the Connection (Client)**\n\nNext, it uses the official `openai` library to create a \"client\" object, which knows how to talk to the OpenAI API.\n\n```python\n# Simplified from src/utils/call_llm.py\nfrom openai import OpenAI # The official library\n# ... other imports ...\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=1000, timeout=60):\n    # ... (get api_key) ...\n\n    try:\n        # Initialize the client with the API key\n        client = OpenAI(api_key=api_key)\n\n        # ... (make the API call using 'client') ...\n\n    except Exception as e:\n        logging.exception(\"Error initializing OpenAI client\")\n        return f\"Error initializing client: {str(e)}\"\n```\nThis sets up the communication channel using the API key.\n\n**3. Making the Call**\n\nNow, it uses the `client` object to actually send the prompt to the specified AI model.\n\n```python\n# Simplified from src/utils/call_llm.py\n# ... inside the 'try' block ...\n\n        try:\n            logging.debug(f\"Calling OpenAI model {model}...\")\n            # This is the actual call to the AI service\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}], # Pass the prompt\n                temperature=temperature,     # Pass other parameters\n                max_tokens=max_tokens,\n                timeout=timeout\n            )\n            logging.debug(\"OpenAI call successful.\")\n\n            # ... (extract the text response) ...\n\n        except Exception as api_error:\n            logging.error(f\"OpenAI API error: {str(api_error)}\")\n            # Handle specific errors like timeouts or rate limits\n            if \"timeout\" in str(api_error).lower():\n                 return \"Error: LLM API call timed out.\"\n            # ... other error handling ...\n            return f\"Error calling LLM API: {str(api_error)}\"\n\n```\nThe `client.chat.completions.create` method does the heavy lifting of communicating with OpenAI. We wrap this in another `try...except` block to catch specific API errors like timeouts.\n\n**4. Getting the Answer**\n\nIf the call is successful, the `response` object contains the AI's answer. We need to extract the actual text content.\n\n```python\n# Simplified from src/utils/call_llm.py\n# ... inside the successful API call 'try' block ...\n\n            # Extract the text from the response object\n            content = response.choices[0].message.content\n            logging.debug(f\"Received response length: {len(content)}\")\n            return content # Return the text to the Node\n\n```\nThe useful text is nested inside the `response` object. We extract it and return it as a simple string.\n\nThis utility simplifies AI interaction for all Nodes, providing a consistent and robust way to leverage the power of LLMs.\n\n## Conclusion\n\nIn this chapter, we learned about the **LLM Utility**, our project's standardized way of communicating with Large Language Models like GPT-4.\n\n*   **Problem:** Nodes need to perform complex language tasks (topic extraction, Q&A, summarization) that require AI.\n*   **Solution:** A central utility function (`call_llm`) provides a simple, consistent interface to the AI.\n*   **Analogy:** It's like a dedicated phone line to an AI expert.\n*   **Prompts:** Nodes craft instructions and context (`prompts`) to tell the AI what to do.\n*   **Usage:** Nodes call `call_llm(prompt, ...)` and receive the AI's response as text.\n*   **Under the Hood:** The utility handles API keys, network requests, timeouts, and basic error handling using the official OpenAI library.\n\nThis utility is a key component, enabling sophisticated analysis and content generation within our pipeline. Without it, tasks like topic extraction or creating ELI5 summaries would be much harder!\n\nNow that we've gathered video info ([Chapter 5](05_youtube_data_utilities.md)), extracted the transcript, and analyzed it using the LLM ([Chapter 6](06_llm_utility.md)) to get topics, Q&A, and summaries stored in [Shared Memory](03_shared_memory.md), how do we present all this information nicely to the user?\n\nLet's move on to the final step: putting everything together into a user-friendly report with [Chapter 7: HTML Generation](07_html_generation.md)!\n\nRelevant Code Snippets:\n--- File: src/nodes/html_generation_node.py ---\n\"\"\"\nHTML Generation Node for YouTube Video Summarizer.\n\"\"\"\nimport sys\nimport os\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.generate_html import generate_html\nfrom src.utils.logger import logger\n\nclass HTMLGenerationNode(BaseNode):\n    \"\"\"\n    Node for generating HTML visualization of the video summary.\n    \"\"\"\n    \n    def __init__(self, shared_memory=None, output_path=None):\n        \"\"\"\n        Initialize the node with shared memory and output path.\n        \n        Args:\n            shared_memory (dict): Shared memory dictionary\n            output_path (str): Path to save the HTML output file\n        \"\"\"\n        super().__init__(shared_memory)\n        self.output_path = output_path\n        logger.debug(f\"HTMLGenerationNode initialized with output_path={output_path}\")\n    \n    def prep(self):\n        \"\"\"\n        Prepare for execution by checking if all required data exists in shared memory.\n        \"\"\"\n        required_keys = [\"video_id\", \"metadata\", \"topics\", \"qa_pairs\", \"eli5_content\"]\n        missing_keys = [key for key in required_keys if key not in self.shared_memory]\n        \n        if missing_keys:\n            error_msg = f\"Missing required data in shared memory: {', '.join(missing_keys)}\"\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n        \n        if \"error\" in self.shared_memory:\n            logger.warning(f\"Skipping HTML Generation due to previous error: {self.shared_memory['error']}\")\n            return\n        \n        logger.info(\"Preparing to generate HTML summary\")\n        logger.debug(f\"Found {len(self.shared_memory['topics'])} topics and {len(self.shared_memory['eli5_content'])} ELI5 explanations\")\n    \n    def exec(self):\n        \"\"\"\n        Execute HTML generation using the data in shared memory.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            return\n            \n        # Prepare the summary data for HTML generation\n        summary_data = {\n            \"video_id\": self.shared_memory[\"video_id\"],\n            \"metadata\": self.shared_memory[\"metadata\"],\n            \"topics\": self.shared_memory[\"topics\"],\n            \"qa_pairs\": self.shared_memory[\"qa_pairs\"],\n            \"eli5_content\": self.shared_memory[\"eli5_content\"]\n        }\n        \n        # Generate the HTML content\n        logger.debug(\"Calling generate_html function\")\n        html_content = generate_html(summary_data)\n        \n        # Store the HTML content in shared memory\n        self.shared_memory[\"html_output\"] = html_content\n        logger.info(f\"Generated HTML content ({len(html_content)} characters)\")\n        \n        # Save the HTML to a file if output_path is provided\n        if self.output_path:\n            try:\n                # Create directory if it doesn't exist\n                os.makedirs(os.path.dirname(os.path.abspath(self.output_path)), exist_ok=True)\n                logger.debug(f\"Ensuring output directory exists: {os.path.dirname(os.path.abspath(self.output_path))}\")\n                \n                # Write the HTML content to the file\n                with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html_content)\n                \n                logger.info(f\"HTML summary saved to: {self.output_path}\")\n                \n            except Exception as e:\n                error_msg = f\"Error saving HTML file: {str(e)}\"\n                logger.error(error_msg)\n                self.shared_memory[\"error\"] = error_msg\n    \n    def post(self):\n        \"\"\"\n        Post-process and check for errors.\n        \"\"\"\n        if \"error\" in self.shared_memory:\n            logger.error(f\"Error in HTML Generation Node: {self.shared_memory['error']}\")\n            return\n        \n        if \"html_output\" not in self.shared_memory:\n            error_msg = \"Failed to generate HTML output\"\n            logger.error(error_msg)\n            self.shared_memory[\"error\"] = error_msg\n            return\n        \n        html_length = len(self.shared_memory[\"html_output\"])\n        logger.info(f\"Generated HTML summary ({html_length} characters)\")\n        logger.info(\"HTML Generation Node completed successfully\")\n\n\nif __name__ == \"__main__\":\n    # Test with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    # Initialize shared memory\n    shared_memory = test_data\n    \n    # Create and run the node with a test output path\n    test_output_path = \"test_summary.html\"\n    node = HTMLGenerationNode(shared_memory, test_output_path)\n    updated_memory = node.run()\n    \n    # Print the results\n    logger.info(\"\\nShared Memory after processing:\")\n    logger.info(f\"HTML output length: {len(updated_memory.get('html_output', ''))}\")\n    logger.info(f\"HTML file saved to: {test_output_path}\")\n\n\n--- File: src/utils/generate_html.py ---\n\"\"\"\nUtility function to generate HTML output for the YouTube video summary.\n\"\"\"\nimport html\n\ndef generate_html(summary_data):\n    \"\"\"\n    Generates an HTML page to visualize the YouTube video summary.\n    \n    Args:\n        summary_data (dict): A dictionary containing all the processed information\n            - video_id (str): YouTube video ID\n            - metadata (dict): Video metadata\n            - topics (list): List of extracted topics\n            - qa_pairs (dict): Q&A pairs organized by topic\n            - eli5_content (dict): Child-friendly explanations\n            \n    Returns:\n        str: HTML content as a string\n    \"\"\"\n    if not summary_data:\n        return \"<html><body><h1>Error: No summary data provided</h1></body></html>\"\n    \n    video_id = summary_data.get(\"video_id\", \"\")\n    metadata = summary_data.get(\"metadata\", {})\n    topics = summary_data.get(\"topics\", [])\n    qa_pairs = summary_data.get(\"qa_pairs\", {})\n    eli5_content = summary_data.get(\"eli5_content\", {})\n    \n    # Escape HTML special characters to prevent XSS\n    title = html.escape(metadata.get(\"title\", \"YouTube Video Summary\"))\n    channel = html.escape(metadata.get(\"channel_name\", \"Unknown Channel\"))\n    duration = html.escape(metadata.get(\"duration\", \"\"))\n    published_at = html.escape(metadata.get(\"published_at\", \"\"))\n    thumbnail_url = metadata.get(\"thumbnail_url\", \"\")\n    \n    # Generate the HTML content\n    html_content = f\"\"\"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{title} - Summary</title>\n    <style>\n        :root {{\n            --primary-color: #ff5252;\n            --secondary-color: #3f51b5;\n            --background-color: #f9f9f9;\n            --card-color: #ffffff;\n            --text-color: #333333;\n            --border-radius: 10px;\n            --box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n        }}\n        \n        body {{\n            font-family: 'Comic Sans MS', 'Chalkboard SE', 'Arial', sans-serif;\n            background-color: var(--background-color);\n            color: var(--text-color);\n            line-height: 1.6;\n            margin: 0;\n            padding: 0;\n        }}\n        \n        .container {{\n            max-width: 1000px;\n            margin: 0 auto;\n            padding: 20px;\n        }}\n        \n        header {{\n            text-align: center;\n            margin-bottom: 30px;\n        }}\n        \n        h1 {{\n            color: var(--primary-color);\n            font-size: 2.5rem;\n            margin-bottom: 10px;\n        }}\n        \n        h2 {{\n            color: var(--secondary-color);\n            font-size: 1.8rem;\n            border-bottom: 2px solid var(--secondary-color);\n            padding-bottom: 10px;\n            margin-top: 30px;\n        }}\n        \n        h3 {{\n            color: var(--primary-color);\n            font-size: 1.5rem;\n        }}\n        \n        .video-info {{\n            display: flex;\n            align-items: center;\n            background-color: var(--card-color);\n            border-radius: var(--border-radius);\n            box-shadow: var(--box-shadow);\n            padding: 20px;\n            margin-bottom: 30px;\n        }}\n        \n        .video-thumbnail {{\n            flex: 0 0 280px;\n            margin-right: 20px;\n        }}\n        \n        .video-thumbnail img {{\n            width: 100%;\n            border-radius: var(--border-radius);\n        }}\n        \n        .video-details {{\n            flex: 1;\n        }}\n        \n        .topic-card {{\n            background-color: var(--card-color);\n            border-radius: var(--border-radius);\n            box-shadow: var(--box-shadow);\n            padding: 20px;\n            margin-bottom: 20px;\n        }}\n        \n        .qa-section {{\n            background-color: rgba(63, 81, 181, 0.1);\n            border-radius: var(--border-radius);\n            padding: 15px;\n            margin-top: 15px;\n        }}\n        \n        .question {{\n            font-weight: bold;\n            color: var(--secondary-color);\n            margin-bottom: 5px;\n        }}\n        \n        .answer {{\n            margin-bottom: 15px;\n            padding-left: 15px;\n            border-left: 3px solid var(--primary-color);\n        }}\n        \n        footer {{\n            text-align: center;\n            margin-top: 50px;\n            padding: 20px;\n            font-size: 0.9rem;\n            color: #666;\n        }}\n        \n        @media (max-width: 768px) {{\n            .video-info {{\n                flex-direction: column;\n            }}\n            \n            .video-thumbnail {{\n                margin-right: 0;\n                margin-bottom: 20px;\n                width: 100%;\n            }}\n        }}\n    </style>\n</head>\n<body>\n    <div class=\"container\">\n        <header>\n            <h1>\ud83c\udfac Kid-Friendly Video Summary \ud83c\udfac</h1>\n            <p>Here's a simple explanation of what this video is all about!</p>\n        </header>\n        \n        <div class=\"video-info\">\n            <div class=\"video-thumbnail\">\n                <img src=\"{thumbnail_url}\" alt=\"{title}\" onerror=\"this.src='https://via.placeholder.com/280x158?text=No+Thumbnail'\">\n                <p style=\"text-align: center; margin-top: 10px;\">\n                    <a href=\"https://www.youtube.com/watch?v={video_id}\" target=\"_blank\" style=\"text-decoration: none; color: var(--primary-color);\">\n                        Watch on YouTube\n                    </a>\n                </p>\n            </div>\n            <div class=\"video-details\">\n                <h2>{title}</h2>\n                <p><strong>Channel:</strong> {channel}</p>\n                <p><strong>Duration:</strong> {duration}</p>\n                <p><strong>Published:</strong> {published_at}</p>\n            </div>\n        </div>\n\"\"\"\n    \n    # Add topics and their content\n    for i, topic in enumerate(topics):\n        topic_escaped = html.escape(topic)\n        topic_id = f\"topic-{i+1}\"\n        \n        # Get ELI5 explanation for this topic\n        eli5_explanation = html.escape(eli5_content.get(topic, \"\"))\n        \n        # Get Q&A pairs for this topic\n        topic_qa_pairs = qa_pairs.get(topic, [])\n        \n        html_content += f\"\"\"\n        <h2 id=\"{topic_id}\">Topic {i+1}: {topic_escaped}</h2>\n        <div class=\"topic-card\">\n            <p>{eli5_explanation}</p>\n            \n            <div class=\"qa-section\">\n                <h3>Questions & Answers</h3>\n\"\"\"\n        \n        # Add Q&A pairs\n        if topic_qa_pairs:\n            for qa in topic_qa_pairs:\n                question = html.escape(qa.get(\"question\", \"\"))\n                answer = html.escape(qa.get(\"answer\", \"\"))\n                \n                html_content += f\"\"\"\n                <div class=\"question\">Q: {question}</div>\n                <div class=\"answer\">A: {answer}</div>\n\"\"\"\n        else:\n            html_content += \"\"\"\n                <p>No questions available for this topic.</p>\n\"\"\"\n        \n        html_content += \"\"\"\n            </div>\n        </div>\n\"\"\"\n    \n    # Close the HTML document\n    html_content += \"\"\"\n        <footer>\n            <p>This summary was created by YouTube Video Summarizer</p>\n        </footer>\n    </div>\n</body>\n</html>\n\"\"\"\n    \n    return html_content\n\n\nif __name__ == \"__main__\":\n    # Test the function with sample data\n    test_data = {\n        \"video_id\": \"dQw4w9WgXcQ\",\n        \"metadata\": {\n            \"title\": \"Rick Astley - Never Gonna Give You Up\",\n            \"channel_name\": \"Rick Astley\",\n            \"duration\": \"3 minutes 33 seconds\",\n            \"published_at\": \"October 25, 2009\",\n            \"thumbnail_url\": \"https://i.ytimg.com/vi/dQw4w9WgXcQ/hqdefault.jpg\"\n        },\n        \"topics\": [\n            \"Music Video Plot\",\n            \"Song Lyrics\",\n            \"Cultural Impact\"\n        ],\n        \"qa_pairs\": {\n            \"Music Video Plot\": [\n                {\n                    \"question\": \"What happens in the music video?\",\n                    \"answer\": \"Rick dances and sings in different locations with backup dancers.\"\n                },\n                {\n                    \"question\": \"What is Rick wearing?\",\n                    \"answer\": \"Rick is wearing a long coat and has styled hair.\"\n                }\n            ],\n            \"Song Lyrics\": [\n                {\n                    \"question\": \"What is the main message of the song?\",\n                    \"answer\": \"The song is about commitment and never letting someone down.\"\n                }\n            ],\n            \"Cultural Impact\": [\n                {\n                    \"question\": \"Why is this song famous on the internet?\",\n                    \"answer\": \"It became an internet prank called 'Rickrolling' where people are tricked into clicking links to this video.\"\n                }\n            ]\n        },\n        \"eli5_content\": {\n            \"Music Video Plot\": \"In this video, a man named Rick is dancing and singing. He moves his arms and legs in a funny way that people like to copy. He sings in different places like a stage and outside.\",\n            \"Song Lyrics\": \"Rick is singing about being a good friend. He promises to always be there for someone special and never make them sad or tell lies. It's like when you promise to always be nice to your best friend.\",\n            \"Cultural Impact\": \"This song became super famous because people on the internet started using it as a funny joke. They would trick their friends by saying 'click here for something cool' but the link would take them to this song instead. This joke is called 'Rickrolling'.\"\n        }\n    }\n    \n    html_output = generate_html(test_data)\n    \n    # Save the HTML to a file for testing\n    with open(\"test_summary.html\", \"w\", encoding=\"utf-8\") as f:\n        f.write(html_output)\n    \n    print(\"HTML generated and saved to test_summary.html\")\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 7: HTML Generation`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 7: HTML Generation\n\nWelcome back! In [Chapter 6: LLM Utility](06_llm_utility.md), we saw how our project uses a powerful AI assistant (a Large Language Model) to analyze the video transcript and generate insightful content like topics, Q&A pairs, and simple ELI5 explanations. All these results are carefully stored in our central [Shared Memory](03_shared_memory.md) dictionary.\n\nBut right now, all that great information is just sitting inside a Python dictionary. That's not very easy for a person to read or share! Imagine getting a big pile of notes and data scribbled on scraps of paper. You'd want someone to organize it into a polished, easy-to-read final report.\n\nThat's exactly what **HTML Generation** does in our `pf1` project!\n\n## The Problem: Presenting Results Nicely\n\nOur pipeline has worked hard:\n1.  It fetched video info ([Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)).\n2.  It extracted the transcript.\n3.  It identified topics ([Chapter 4: Topic Processing Orchestrator](04_topic_processing_orchestrator.md)).\n4.  It generated Q&A and ELI5 summaries using AI ([Chapter 6: LLM Utility](06_llm_utility.md)).\n\nAll these pieces \u2013 the video title, the list of topics, the questions and answers, the simple explanations \u2013 are collected in the [Shared Memory](03_shared_memory.md) dictionary. But how do we show this to the user in a way that looks good and is easy to understand?\n\n## Our Solution: The Report Creator (HTML Generation Node)\n\nWe need a final step in our pipeline that acts like a **report generator** or a **desktop publisher**. Its job is to take all the processed data and arrange it neatly into a final document.\n\nIn our project, this final document is an **HTML file**. HTML (HyperText Markup Language) is the standard language used to create web pages. By generating an HTML file, we create a report that can be easily opened and viewed in any web browser (like Chrome, Firefox, or Safari), complete with nice formatting, images, and links.\n\nThe component responsible for this is the **`HTMLGenerationNode`**. It's typically the *last* [Node (Pipeline Step)](02_node__pipeline_step_.md) run by our [Pipeline Orchestration](01_pipeline_orchestration.md).\n\n**What it does:**\n1.  **Gathers Ingredients:** Reads all the necessary results (metadata, topics, Q&A, ELI5) from the [Shared Memory](03_shared_memory.md).\n2.  **Follows a Recipe:** Uses a template (like a recipe) to structure this information into an HTML format.\n3.  **Creates the Dish:** Generates the final HTML text.\n4.  **Serves It:** Saves this HTML text into a file (e.g., `summary.html`) that the user can open.\n\n## How the HTML Generation Node Fits In\n\nThe `run_pipeline` function ([Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md)) calls the `HTMLGenerationNode` after all the analysis is done.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory has metadata, topics, qa_pairs, eli5_content)\n\n# 5. HTML Generation Node\nlogger.info(\"[5/5] Starting HTML Generation...\")\n# Create the node, telling it where to save the file\nhtml_node = HTMLGenerationNode(\n    shared_memory,\n    output_path=f\"{output_dir}/summary_{shared_memory['video_id']}.html\"\n)\n\n# Run the node - this creates the HTML content and file\nshared_memory = html_node.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"HTML Generation failed...\")\n    # Handle error\nelse:\n    logger.info(f\"HTML report saved successfully!\")\n    # The path to the HTML file is usually stored in shared_memory now\n    # (or determined by the output_path we gave it)\n\nlogger.info(\"Pipeline finished!\")\nreturn shared_memory\n```\nHere, the orchestrator creates the `HTMLGenerationNode`, passes it the final `shared_memory` containing all results, and tells it where to save the output file (e.g., `output/summary_videoID.html`). Calling `html_node.run()` triggers the creation of the HTML report.\n\n## What Happens Inside the Node: Prep, Exec, Post\n\nLike all our Nodes ([Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)), the `HTMLGenerationNode` follows the standard `prep -> exec -> post` workflow.\n\n1.  **`prep()` (Get Ready):**\n    *   Checks the [Shared Memory](03_shared_memory.md) to make sure all the required data pieces are present (e.g., `\"metadata\"`, `\"topics\"`, `\"qa_pairs\"`, `\"eli5_content\"`).\n    *   If anything crucial is missing, it raises an error.\n    *   If a previous step already reported an error, it might decide to skip HTML generation.\n\n2.  **`exec()` (Do the Work):**\n    *   Bundles the required data (metadata, topics, etc.) into a structure that's easy to work with.\n    *   Calls a special helper function (a utility like the ones in [Chapter 5](05_youtube_data_utilities.md)) called `generate_html`. This utility takes the data and does the actual work of creating the HTML string.\n    *   Stores the generated HTML content back into [Shared Memory](03_shared_memory.md) (e.g., under the key `\"html_output\"`).\n    *   If an `output_path` was provided (like in the `main.py` example), it writes the generated HTML content to that file on the disk.\n\n3.  **`post()` (Clean Up):**\n    *   Logs whether the HTML was generated successfully.\n    *   Checks if the HTML content was actually created and added to shared memory. Reports an error if not.\n\n## The Magic Tool: `generate_html` Utility\n\nThe `HTMLGenerationNode` doesn't contain messy HTML code directly. Instead, it delegates the task of creating the HTML structure to a helper function: `generate_html` (found in `src/utils/generate_html.py`).\n\nThink of `generate_html` as a **template filler**. It has a predefined structure for the HTML page (like a fill-in-the-blanks form letter) and knows where to plug in the specific data (video title, topics, Q&A, etc.) that it receives from the `HTMLGenerationNode`.\n\n**How it works (simplified):**\n1.  **Gets Data:** Receives the `summary_data` dictionary from the Node.\n2.  **Starts HTML:** Begins creating the HTML string with standard tags (`<html>`, `<head>`, `<body>`).\n3.  **Adds Styling:** Includes CSS rules inside `<style>` tags to make the page look nice (colors, fonts, layout).\n4.  **Inserts Header:** Puts the video title, channel name, thumbnail, and YouTube link near the top.\n5.  **Loops Through Topics:** Goes through each topic in the list. For each topic:\n    *   Creates a section header (e.g., \"Topic 1: Music Video Plot\").\n    *   Adds the ELI5 explanation for that topic.\n    *   Adds the Q&A pairs associated with that topic, formatting the questions and answers clearly.\n6.  **Adds Footer:** Puts some concluding text at the bottom.\n7.  **Returns HTML:** Returns the complete HTML content as one long string.\n\n**Important Note on Security:** When putting data from potentially external sources (like video titles or Q&A generated by AI) into HTML, it's crucial to \"escape\" special characters (like `<`, `>`, `&`). The `generate_html` utility uses Python's `html.escape()` function to do this, preventing potential security issues like Cross-Site Scripting (XSS).\n\n## Visualizing the Flow\n\nHere's how the `HTMLGenerationNode` works with the `generate_html` utility:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline\n    participant HTMLNode as HTMLGenerationNode\n    participant SM as Shared Memory\n    participant HTMLUtil as generate_html()\n    participant FileSystem as Output File\n\n    Orch->>HTMLNode: run(SM, output_path)\n    HTMLNode->>SM: prep() [Reads metadata, topics, qa_pairs, eli5_content]\n    Note right of HTMLNode: Checks if all data is present\n    HTMLNode->>HTMLNode: exec() [Bundles data]\n    HTMLNode->>HTMLUtil: generate_html(summary_data)\n    HTMLUtil->>HTMLUtil: Build HTML string using data & template\n    HTMLUtil-->>HTMLNode: Return html_content string\n    HTMLNode->>SM: Store html_content in Shared Memory\n    HTMLNode->>FileSystem: Write html_content to output_path\n    HTMLNode->>HTMLNode: post() [Log success]\n    HTMLNode-->>Orch: Return updated SM\n```\nThis shows the Node getting data from Shared Memory, using the utility to create the HTML, and then saving it both back to Shared Memory and to the file system.\n\n## Diving into the Code\n\nLet's look at simplified snippets.\n\n**`HTMLGenerationNode` (`src/nodes/html_generation_node.py`)**\n\n```python\n# Simplified from src/nodes/html_generation_node.py\nimport os\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.generate_html import generate_html # Import the helper\nfrom src.utils.logger import logger\n\nclass HTMLGenerationNode(BaseNode):\n    def __init__(self, shared_memory=None, output_path=None):\n        super().__init__(shared_memory)\n        self.output_path = output_path # Store the save location\n        logger.debug(\"HTML Node ready.\")\n\n    def prep(self):\n        # Check if needed data exists in shared_memory\n        required = [\"video_id\", \"metadata\", \"topics\", \"qa_pairs\", \"eli5_content\"]\n        if not all(key in self.shared_memory for key in required):\n            raise ValueError(\"Missing data for HTML generation!\")\n        logger.info(\"Preparing to generate HTML.\")\n\n    def exec(self):\n        # Bundle the data\n        summary_data = {key: self.shared_memory[key] for key in required}\n\n        # Call the utility to create the HTML string\n        logger.debug(\"Calling generate_html function\")\n        html_content = generate_html(summary_data)\n        self.shared_memory[\"html_output\"] = html_content\n\n        # Save to file if path is given\n        if self.output_path:\n            try:\n                # Ensure directory exists\n                os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n                # Write file (UTF-8 ensures special characters work)\n                with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html_content)\n                logger.info(f\"HTML saved to: {self.output_path}\")\n            except Exception as e:\n                logger.error(f\"Error saving HTML file: {e}\")\n                self.shared_memory[\"error\"] = f\"Failed to save HTML: {e}\"\n\n    def post(self):\n        if \"error\" not in self.shared_memory:\n            logger.info(\"HTML Generation completed successfully.\")\n        # (Error logging happens in prep/exec)\n```\nThis Node clearly shows the `prep` check, calling `generate_html` in `exec`, storing the result, and saving the file.\n\n**`generate_html` Utility (`src/utils/generate_html.py`)**\n\n```python\n# Simplified from src/utils/generate_html.py\nimport html # For escaping special characters\n\ndef generate_html(summary_data):\n    # Get data, use defaults if missing\n    title = html.escape(summary_data.get(\"metadata\", {}).get(\"title\", \"Summary\"))\n    topics = summary_data.get(\"topics\", [])\n    qa_pairs = summary_data.get(\"qa_pairs\", {})\n    eli5 = summary_data.get(\"eli5_content\", {})\n\n    # Start building the HTML string\n    html_str = f\"\"\"<!DOCTYPE html>\n<html><head><title>{title}</title><style>/* CSS styles */</style></head><body>\n<h1>Summary for: {title}</h1>\"\"\"\n\n    # Loop through topics\n    for i, topic in enumerate(topics):\n        topic_esc = html.escape(topic)\n        eli5_esc = html.escape(eli5.get(topic, \"N/A\"))\n        html_str += f\"<h2>Topic {i+1}: {topic_esc}</h2><p>{eli5_esc}</p>\"\n\n        # Add Q&A for this topic\n        html_str += \"<h3>Questions & Answers</h3>\"\n        topic_qas = qa_pairs.get(topic, [])\n        if topic_qas:\n            for qa in topic_qas:\n                q = html.escape(qa.get(\"question\", \"\"))\n                a = html.escape(qa.get(\"answer\", \"\"))\n                html_str += f\"<div class='question'>Q: {q}</div><div class='answer'>A: {a}</div>\"\n        else:\n            html_str += \"<p>No Q&A available.</p>\"\n\n    # Finish the HTML\n    html_str += \"<footer>Generated by pf1.</footer></body></html>\"\n    return html_str\n```\nThis utility function takes the data and constructs the HTML string, making sure to use `html.escape()` on dynamic content like titles, topics, questions, and answers.\n\n## Conclusion\n\nIn this chapter, we explored the final step in our pipeline: **HTML Generation**.\n\n*   **Purpose:** To present all the analyzed video information (metadata, topics, Q&A, ELI5) in a user-friendly, visually appealing format.\n*   **Mechanism:** The `HTMLGenerationNode` reads the results from [Shared Memory](03_shared_memory.md).\n*   **Helper Utility:** It uses the `generate_html` function, which acts like a template filler, to create the actual HTML content.\n*   **Output:** Produces an HTML string (stored in Shared Memory) and saves it as an `.html` file that can be opened in a web browser.\n\nThis node turns the complex data collected throughout the pipeline into a tangible, understandable report for the end-user.\n\nThroughout these chapters, we've seen mentions of `logger.info`, `logger.error`, etc. How does our program keep track of what it's doing, report progress, and help us find problems? That's the job of our logging system.\n\nLet's dive into how logging works in our project in the next chapter: [Chapter 8: Logging](08_logging.md)!", "\nWrite a very beginner-friendly tutorial chapter (in Markdown format) for the project `pf1` about the concept: \"Logging\". This is Chapter 8.\n\nConcept Details:\n- Description:\nProvides a consistent way to record messages about the application's execution, including informational messages, warnings, and errors. This helps in monitoring the pipeline's progress and debugging issues.\nIt's like a detailed ship's log, recording every significant event and observation as the application sails through its tasks, making it easier to understand what happened later.\n\n\nComplete Tutorial Structure:\n1. [Pipeline Orchestration](01_pipeline_orchestration.md)\n2. [Node (Pipeline Step)](02_node__pipeline_step_.md)\n3. [Shared Memory](03_shared_memory.md)\n4. [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)\n5. [YouTube Data Utilities](05_youtube_data_utilities.md)\n6. [LLM Utility](06_llm_utility.md)\n7. [HTML Generation](07_html_generation.md)\n8. [Logging](08_logging.md)\n\nContext from previous chapters (summary):\n# Chapter 1: Pipeline Orchestration\n\nWelcome to the `pf1` YouTube Summarizer project! We're thrilled to have you here. Our goal is to build a tool that takes a YouTube video URL and automatically creates a nice summary report.\n\nImagine you want to build a car. You don't just magically get a car! You need an assembly line: a frame is built, then the engine is added, then the wheels, the doors, the paint, and so on. Each step builds upon the previous one, and they have to happen in the right order.\n\nOur YouTube summarizer works similarly. We can't just get a summary instantly from a URL. We need a series of steps:\n1.  Get information *about* the video (like its title).\n2.  Extract the spoken words (the transcript).\n3.  Figure out the main topics discussed.\n4.  Analyze each topic (maybe generate questions and answers).\n5.  Put everything together into a readable report (like an HTML page).\n\nThis sequence of steps is what we call a **Pipeline**.\n\nBut who manages this assembly line? Who makes sure step 1 finishes before step 2 starts, and that the output from step 1 is correctly passed to step 2? That's where **Pipeline Orchestration** comes in.\n\nThink of Pipeline Orchestration like the **Project Manager** or the **Factory Foreman** for our summarization task. It doesn't do the specific jobs itself (like extracting the transcript or generating HTML), but it manages the overall *flow*, ensuring each specialist task runs at the right time and in the right order.\n\n## Why Do We Need Orchestration?\n\nYou might wonder, why not just write one giant piece of code that does everything?\n\n1.  **Order Matters:** You can't analyze topics before you have the transcript, and you can't create the final report before you've analyzed the topics. Orchestration enforces this correct sequence.\n2.  **Managing Complexity:** Breaking the big problem (summarizing a video) into smaller, manageable steps makes the code easier to understand, develop, and fix. Orchestration connects these smaller pieces.\n3.  **Passing Information:** The output of one step is often the input for the next. For example, the transcript extracted in one step is needed to find topics in the next step. The orchestrator helps manage how this information (or \"data\") is passed along.\n\n## Our Project Manager: The `run_pipeline` Function\n\nIn our project (`pf1`), the main orchestrator is a function called `run_pipeline` located in the `src/main.py` file. This function defines the sequence of steps needed to go from a YouTube URL to a final summary report.\n\nIt calls upon different specialists, which we call **Nodes**. Each Node is responsible for one specific task in the pipeline. We'll learn all about Nodes in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md).\n\nFor now, just know that `run_pipeline` calls these Nodes one after another.\n\nHere's a simplified view of what `run_pipeline` does:\n\n```python\n# This is a simplified example!\n# Find the real code in src/main.py\n\ndef run_pipeline(youtube_url):\n    # 1. Get basic video info (like title)\n    step1_output = run_input_processing_node(youtube_url)\n\n    # 2. Get the video's transcript\n    step2_output = run_content_extraction_node(step1_output)\n\n    # 3. Find the main topics in the transcript\n    step3_output = run_topic_extraction_node(step2_output)\n\n    # 4. Analyze each topic (e.g., generate Q&A)\n    step4_output = run_topic_orchestration_node(step3_output)\n\n    # 5. Create the final HTML report\n    final_report = run_html_generation_node(step4_output)\n\n    print(\"Pipeline complete! Report created.\")\n    return final_report\n\n# Imagine calling it like this:\n# run_pipeline(\"https://www.youtube.com/watch?v=some_video_id\")\n```\n\nIn this simplified example, you can clearly see the sequence. The `run_pipeline` function calls each step (`run_..._node`) in order, passing the result from one step to the next.\n\n## Passing Notes: How Data Flows\n\nHow does the output from `run_input_processing_node` get to `run_content_extraction_node`? The orchestrator uses a central place to store the results from each step. Think of it like a shared notebook or clipboard that each specialist Node can read from and write to.\n\nIn our project, we call this the [Shared Memory](03_shared_memory.md). The `run_pipeline` function initializes this shared memory (initially just containing the input URL) and passes it to the first Node. That Node does its work, adds its results to the shared memory, and returns it. The orchestrator then passes the updated shared memory to the next Node, and so on. We'll dive deeper into this in [Chapter 3](03_shared_memory.md).\n\n## Visualizing the Flow\n\nLet's visualize this flow using a simple diagram. `run_pipeline` is the manager, directing the Nodes and passing the `shared_memory` (our shared notebook) between them.\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as InputProcessingNode\n    participant CNode as ContentExtractionNode\n    participant TNode as TopicExtractionNode\n    participant HNode as HTMLGenerationNode\n    participant SM as Shared Memory\n\n    User->>RP: Start with YouTube URL\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>SM: Add video metadata\n    INode-->>RP: Return updated SM\n    RP->>CNode: run(SM)\n    CNode->>SM: Add transcript\n    CNode-->>RP: Return updated SM\n    RP->>TNode: run(SM)\n    TNode->>SM: Add topics\n    TNode-->>RP: Return updated SM\n    Note right of RP: ... other nodes run ...\n    RP->>HNode: run(SM)\n    HNode->>SM: Generate HTML path\n    HNode-->>RP: Return final SM\n    RP-->>User: Report results (including HTML path)\n```\n\nThis diagram shows how `run_pipeline` calls each Node in sequence (`INode`, `CNode`, `TNode`, `HNode`), passing the `Shared Memory` (`SM`) along. Each Node reads from `SM` and adds its own results back into it.\n\n## A Look at the Real Code (`src/main.py`)\n\nLet's look at the actual `run_pipeline` function in `src/main.py`. Don't worry if not everything makes sense yet; we'll cover the details in later chapters. Focus on seeing the sequence of Nodes being called.\n\nFirst, the function starts by setting things up, including the `shared_memory`:\n\n```python\n# From src/main.py\n\nimport os\n# ... other imports ...\nfrom src.nodes.input_processing_node import InputProcessingNode\n# ... imports for other nodes ...\nfrom src.utils.logger import logger\n\ndef run_pipeline(youtube_url, output_dir=\"output\", ...):\n    logger.info(\"Starting YouTube Video Summarizer...\")\n    logger.info(f\"Processing URL: {youtube_url}\")\n\n    # Initialize shared memory - our 'shared notebook'\n    shared_memory = {\"video_url\": youtube_url}\n\n    # Create output directory if it doesn't exist\n    os.makedirs(output_dir, exist_ok=True)\n\n    try:\n        # ... (rest of the pipeline steps below) ...\n```\n\nThis part just logs some information and creates the `shared_memory` dictionary, initially holding only the `video_url`. It also ensures the directory for saving the final report exists.\n\nNow, the orchestrator starts calling the Nodes one by one:\n\n```python\n        # 1. Input Processing Node\n        logger.info(\"[1/5] Starting Input Processing...\")\n        # Create the specialist Node\n        input_node = InputProcessingNode(shared_memory)\n        # Ask the Node to run and update shared_memory\n        shared_memory = input_node.run()\n        # Check if the Node reported an error\n        if \"error\" in shared_memory:\n            # Handle error and stop\n            logger.error(f\"Input Processing failed...\")\n            return shared_memory\n        logger.info(\"Successfully processed video info.\")\n```\n\nThis block shows the pattern:\n1.  Log that the step is starting.\n2.  Create an instance of the specific Node needed (e.g., `InputProcessingNode`), passing it the current `shared_memory`.\n3.  Call the Node's `run()` method. The Node performs its task and returns the *updated* `shared_memory`.\n4.  Update the `shared_memory` variable in `run_pipeline` with the returned value.\n5.  Check for errors. If an error occurred, the pipeline stops.\n\nThis pattern repeats for all the steps:\n\n```python\n        # 2. Content Extraction Node\n        logger.info(\"[2/5] Starting Content Extraction...\")\n        content_node = ContentExtractionNode(shared_memory)\n        shared_memory = content_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Content Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted transcript.\")\n\n        # 3. Topic Extraction Node\n        logger.info(\"[3/5] Starting Topic Extraction...\")\n        topic_node = TopicExtractionNode(shared_memory, ...) # Some nodes need extra config\n        shared_memory = topic_node.run()\n        if \"error\" in shared_memory: # Error check\n            logger.error(f\"Topic Extraction failed...\")\n            return shared_memory\n        logger.info(\"Successfully extracted topics.\")\n\n        # ... (Steps 4 and 5 follow the same pattern) ...\n```\n\nYou can see how `run_pipeline` acts as the orchestrator, calling each Node (`InputProcessingNode`, `ContentExtractionNode`, `TopicExtractionNode`, etc.) in the correct order and passing the `shared_memory` between them.\n\nFinally, after all steps are complete (if no errors occurred), it logs a success message:\n\n```python\n        # ... After step 5 (HTML Generation) ...\n\n        logger.info(\"Summary Generation Complete!\")\n        # ... Print final summary info ...\n        return shared_memory # Return the final state\n\n    except Exception as e:\n        # Catch any unexpected problems\n        logger.exception(f\"Unexpected error in pipeline: {str(e)}\")\n        shared_memory[\"error\"] = f\"Pipeline error: {str(e)}\"\n        return shared_memory\n```\n\nThe function returns the final `shared_memory`, which now contains all the results from the pipeline, including the path to the generated HTML report.\n\n## Conclusion\n\nIn this chapter, we learned about **Pipeline Orchestration**. It's the process of managing a sequence of steps (a pipeline) to achieve a complex goal, like summarizing a YouTube video.\n\nWe saw that orchestration is crucial for:\n*   Ensuring steps run in the correct order.\n*   Breaking down a large task into smaller, manageable parts (Nodes).\n*   Passing information between steps using a shared space (Shared Memory).\n\nIn our `pf1` project, the `run_pipeline` function in `src/main.py` acts as our orchestrator, calling different **Nodes** in sequence and managing the data flow via **Shared Memory**.\n\nNow that we understand the overall flow managed by the orchestrator, let's zoom in on the individual specialists that perform the actual work: the Nodes.\n\nReady to meet the specialists? Let's move on to [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)!\n---\n# Chapter 2: Node (Pipeline Step)\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we learned how our YouTube summarizer uses a pipeline, like an assembly line, to perform a series of steps. We saw that the `run_pipeline` function acts as the project manager, ensuring each step happens in the right order.\n\nBut who actually does the work at each step? If the orchestrator is the manager, who are the specialized workers on the assembly line? That's where **Nodes** come in!\n\n## What is a Node? The Specialist Worker\n\nImagine our car assembly line again. One worker installs the engine, another fits the wheels, and another paints the body. Each worker has a very specific job they are trained to do.\n\nIn our `pf1` project, a **Node** is like one of these specialized workers. It's a self-contained piece of code responsible for **one specific task** within the overall summarization pipeline.\n\nFor example, we have Nodes dedicated to tasks like:\n*   Getting the video's title and ID (`InputProcessingNode`)\n*   Extracting the spoken words (transcript) (`ContentExtractionNode`)\n*   Figuring out the main topics discussed (`TopicExtractionNode`)\n*   Generating Questions & Answers for a topic (`TopicProcessorNode`)\n*   Creating the final HTML report (`HTMLGenerationNode`)\n\nEach Node focuses *only* on its job. The `ContentExtractionNode` doesn't worry about how to generate HTML; it just focuses on getting the transcript accurately.\n\n## Why Break Tasks into Nodes?\n\nWhy not just have one giant function do everything? Breaking the work into Nodes helps us in several ways:\n\n1.  **Clarity:** It's much easier to understand a small piece of code that does one thing well (like getting a transcript) than a huge chunk of code that does everything.\n2.  **Organization:** It keeps our project tidy. All the code related to transcript extraction lives within the `ContentExtractionNode`'s file.\n3.  **Reusability:** If we needed to extract transcripts for another purpose, we could potentially reuse the `ContentExtractionNode`.\n4.  **Testing:** We can test each Node individually to make sure it works correctly before plugging it into the main pipeline.\n5.  **Teamwork:** Different people could work on different Nodes simultaneously.\n\n## The Node's Standard Workflow: Prep, Exec, Post\n\nThink about our assembly line worker again. Before starting their main task, they need to get ready. Maybe they need to grab the right tools or pick up the specific part they need to work on (like the engine block). After doing their job, they need to put the finished piece back onto the assembly line correctly so the next worker can use it.\n\nOur Nodes follow a similar standard process, defined by a template (called a \"base class\" in programming, specifically `BaseNode` in `src/nodes/base_node.py`). Every Node has three main phases, implemented as methods (functions belonging to the Node):\n\n1.  **`prep()` (Prepare):**\n    *   **What it does:** Gets the Node ready to work.\n    *   **Analogy:** The worker checks the instructions, grabs the necessary tools, and picks up the required input part (e.g., the video ID) from the shared workspace ([Shared Memory](03_shared_memory.md)).\n    *   **In code:** Reads necessary information from the `shared_memory` dictionary. Checks if the required inputs are actually there.\n\n2.  **`exec()` (Execute):**\n    *   **What it does:** Performs the Node's main task. This is where the real work happens.\n    *   **Analogy:** The worker installs the engine, extracts the transcript, or identifies the topics.\n    *   **In code:** Calls other utility functions or uses libraries to perform its specific job (like calling the YouTube transcript API or using an AI model).\n\n3.  **`post()` (Post-Process):**\n    *   **What it does:** Cleans up and stores the results.\n    *   **Analogy:** The worker puts their tools away and places the finished part (e.g., the extracted transcript) back onto the shared workspace ([Shared Memory](03_shared_memory.md)) for the next Node.\n    *   **In code:** Takes the results from the `exec` phase and adds them to the `shared_memory` dictionary using specific keys (like `\"transcript\"`). It might also do some final checks.\n\nThese three methods are always called in the order: `prep`, then `exec`, then `post`.\n\n## How Does a Node Run? The `run()` Method\n\nSo, how do these `prep`, `exec`, and `post` methods get called? The [Pipeline Orchestration](01_pipeline_orchestration.md) (our `run_pipeline` function) doesn't call them individually. Instead, each Node has a `run()` method (provided by the `BaseNode` template).\n\nWhen the orchestrator wants a Node to do its job, it simply calls that Node's `run()` method. The `run()` method then takes care of calling `prep()`, `exec()`, and `post()` in the correct sequence.\n\n```python\n# Simplified concept of the BaseNode's run method\n# (See src/nodes/base_node.py for the real code)\n\nclass BaseNode:\n    # ... (other parts like __init__) ...\n\n    def prep(self):\n        # To be implemented by specific nodes\n        pass\n\n    def exec(self):\n        # To be implemented by specific nodes\n        pass\n\n    def post(self):\n        # To be implemented by specific nodes\n        pass\n\n    def run(self):\n        # 1. Prepare\n        self.prep()\n\n        # 2. Execute (only if prep was okay)\n        # (Simplified: Real code checks for errors)\n        self.exec()\n\n        # 3. Post-process (only if exec was okay)\n        # (Simplified: Real code checks for errors)\n        self.post()\n\n        # Return the updated shared memory\n        return self.shared_memory\n```\nThis `run()` method acts like the Node's internal manager, ensuring the standard workflow is followed. The pipeline orchestrator just needs to call `run()`, making the main pipeline code cleaner.\n\n## Example: The Content Extraction Node\n\nLet's look at a concrete example: the `ContentExtractionNode` (found in `src/nodes/content_extraction_node.py`). Its job is to get the transcript (spoken words) for the YouTube video.\n\nHere's a simplified view of its `prep`, `exec`, and `post` methods:\n\n```python\n# Simplified version of ContentExtractionNode\n# (See src/nodes/content_extraction_node.py for full code)\n\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript # Utility function\n\nclass ContentExtractionNode(BaseNode):\n\n    def prep(self):\n        # Get ready: Need the video_id from shared memory\n        print(\"ContentExtractionNode: Preparing...\")\n        if \"video_id\" not in self.shared_memory:\n            print(\"Error: video_id is missing!\")\n            # (Real node would set an error flag)\n            return\n        self.video_id = self.shared_memory[\"video_id\"]\n        print(f\"Found video_id: {self.video_id}\")\n\n    def exec(self):\n        # Do the work: Call the transcript extraction utility\n        print(f\"ContentExtractionNode: Executing - Getting transcript for {self.video_id}...\")\n        # This function actually calls the YouTube API\n        transcript_text = extract_youtube_transcript(self.video_id)\n        # Store the result temporarily within the node\n        self.extracted_transcript = transcript_text\n        print(\"Got the transcript!\")\n\n    def post(self):\n        # Store result: Put the transcript into shared memory\n        print(\"ContentExtractionNode: Posting results...\")\n        self.shared_memory[\"transcript\"] = self.extracted_transcript\n        print(f\"Added 'transcript' (length: {len(self.extracted_transcript)}) to shared memory.\")\n\n```\n\nLet's break down what happens when the orchestrator calls `run()` on this Node:\n\n1.  **`run()` starts.**\n2.  **`prep()` runs:** It looks into the `shared_memory` dictionary given to it and expects to find a key called `\"video_id\"`. If it's missing, it signals an error (in the real code). If found, it stores the ID for later use.\n3.  **`exec()` runs:** It calls the specialized utility function `extract_youtube_transcript`, passing the `video_id` it got during `prep`. This utility function does the heavy lifting of contacting YouTube and getting the transcript text. The result is stored temporarily inside the Node.\n4.  **`post()` runs:** It takes the `extracted_transcript` from the `exec` phase and adds it to the `shared_memory` dictionary under the key `\"transcript\"`.\n5.  **`run()` finishes:** It returns the *updated* `shared_memory` (which now includes the transcript) back to the orchestrator.\n\nThe orchestrator then takes this updated `shared_memory` and passes it to the *next* Node in the pipeline (e.g., the `TopicExtractionNode`), which can now read the `\"transcript\"` key.\n\n## How Nodes Fit Together (Recap)\n\nAs we saw in [Chapter 1](01_pipeline_orchestration.md), the `run_pipeline` function manages this flow:\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline (Orchestrator)\n    participant INode as Input Node\n    participant CNode as Content Extraction Node\n    participant TNode as Topic Extraction Node\n    participant SM as Shared Memory\n\n    RP->>SM: Initialize with URL\n    RP->>INode: run(SM)\n    INode->>INode: prep() [reads URL]\n    INode->>INode: exec() [gets video_id, metadata]\n    INode->>INode: post() [writes video_id, metadata to SM]\n    INode-->>RP: Return updated SM\n    Note right of RP: SM now contains video_id, metadata\n\n    RP->>CNode: run(SM)\n    CNode->>CNode: prep() [reads video_id from SM]\n    CNode->>CNode: exec() [calls YT API, gets transcript]\n    CNode->>CNode: post() [writes transcript to SM]\n    CNode-->>RP: Return updated SM\n    Note right of RP: SM now contains transcript\n\n    RP->>TNode: run(SM)\n    TNode->>TNode: prep() [reads transcript from SM]\n    TNode->>TNode: exec() [analyzes transcript, finds topics]\n    TNode->>TNode: post() [writes topics to SM]\n    TNode-->>RP: Return updated SM\n    Note right of RP: SM now contains topics\n    RP->>...: Continues with next Node...\n```\n\nThis diagram shows how the orchestrator (`run_pipeline`) calls the `run()` method of each Node in turn. Inside `run()`, the `prep`, `exec`, and `post` methods handle reading from and writing to the Shared Memory (`SM`), effectively passing information down the assembly line.\n\n## Conclusion\n\nIn this chapter, we zoomed in on the individual workers of our pipeline: the **Nodes**.\n\n*   A Node is a **specialized component** responsible for **one specific task** (like extracting transcripts or topics).\n*   Using Nodes makes our code **organized, modular, and easier to manage**.\n*   All Nodes follow a standard **`prep` -> `exec` -> `post` workflow**, managed by their `run()` method.\n    *   `prep`: Get ready, read inputs from Shared Memory.\n    *   `exec`: Do the main work.\n    *   `post`: Store results back into Shared Memory.\n*   The pipeline orchestrator calls each Node's `run()` method in sequence, passing the updated Shared Memory along.\n\nNow we understand the orchestrator (the manager) and the Nodes (the workers). But how exactly does this \"shared workspace\" or \"shared notebook\" work where Nodes read inputs and write outputs?\n\nLet's dive into the details of that central data hub in [Chapter 3: Shared Memory](03_shared_memory.md)!\n---\n# Chapter 3: Shared Memory\n\nIn [Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md), we saw our pipeline manager (`run_pipeline`) coordinating the different steps. In [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), we met the specialized workers (Nodes) that perform each task, like getting the video transcript or finding topics.\n\nBut how does the \"transcript extractor\" Node hand over the transcript it found to the \"topic finder\" Node? They need a way to pass information along the assembly line. That's where **Shared Memory** comes in!\n\n## The Problem: Passing Notes Between Workers\n\nImagine our assembly line again.\n1.  Worker A gets the video ID.\n2.  Worker B needs that video ID to fetch the transcript.\n3.  Worker C needs the transcript to find the main topics.\n\nHow does Worker A tell Worker B the video ID? How does Worker B give the transcript to Worker C? They can't just shout across the factory floor! They need a reliable way to share their results.\n\n## Our Solution: The Shared Whiteboard (A Python Dictionary)\n\nIn `pf1`, the **Shared Memory** is our solution. Think of it like a **shared whiteboard** or a **central notebook** that every worker (Node) in the pipeline can access.\n\n*   When a Node finishes its job, it writes its results onto the whiteboard (e.g., \"Video ID: xyz123\", \"Transcript: Hello world...\").\n*   When the next Node starts its job, it looks at the whiteboard to find the information it needs (e.g., it reads the \"Transcript\" section).\n\nTechnically, this \"whiteboard\" is just a standard **Python dictionary**. If you're new to Python dictionaries, they simply store information as **key-value pairs**.\n\n```python\n# A simple Python dictionary example\nmy_info = {\n  \"name\": \"Alex\",\n  \"task\": \"Write chapter\",\n  \"tools\": [\"laptop\", \"keyboard\"]\n}\n\n# Accessing information using the key\nprint(my_info[\"name\"])  # Output: Alex\n\n# Adding new information\nmy_info[\"status\"] = \"In progress\"\nprint(my_info)\n# Output: {'name': 'Alex', 'task': 'Write chapter', 'tools': ['laptop', 'keyboard'], 'status': 'In progress'}\n```\n\nOur `shared_memory` works exactly like this! It's a dictionary where keys are strings describing the data (like `\"video_url\"`, `\"video_id\"`, `\"transcript\"`, `\"topics\"`), and the values are the actual data (the URL string, the ID string, the long transcript text, a list of topic strings, etc.).\n\n## How Shared Memory Flows Through the Pipeline\n\n1.  **Initialization:** The pipeline orchestrator (`run_pipeline` in `src/main.py`) creates the `shared_memory` dictionary when the pipeline starts. Initially, it usually just contains the input needed for the very first step, like the YouTube video URL.\n\n    ```python\n    # From src/main.py (simplified)\n    def run_pipeline(youtube_url, ...):\n        # Create the shared memory dictionary\n        shared_memory = {\"video_url\": youtube_url}\n        logger.info(\"Shared memory initialized.\")\n        # ... rest of the pipeline ...\n    ```\n    At this point, `shared_memory` looks like: `{\"video_url\": \"https://youtube.com/watch?v=some_id\"}`\n\n2.  **Passing to Nodes:** The orchestrator passes this `shared_memory` dictionary to the first Node when it calls the Node's `run()` method.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... inside run_pipeline ...\n    \n    # 1. Input Processing Node\n    logger.info(\"Calling InputProcessingNode...\")\n    input_node = InputProcessingNode(shared_memory) # Give the Node access\n    shared_memory = input_node.run() # Node runs and returns the UPDATED dictionary\n    ```\n\n3.  **Nodes Read and Write:** Inside the Node's `run()` method, the `prep`, `exec`, and `post` methods interact with the dictionary:\n    *   **`prep()` reads:** It looks up the keys it needs (e.g., the `InputProcessingNode` reads `\"video_url\"`).\n    *   **`post()` writes:** After `exec()` does the work, `post()` adds the Node's results back into the *same* dictionary using new keys (e.g., the `InputProcessingNode` adds `\"video_id\"` and `\"metadata\"`).\n\n    ```python\n    # Inside InputProcessingNode (simplified)\n    \n    def prep(self):\n        # Read the URL provided by the orchestrator\n        self.url = self.shared_memory[\"video_url\"]\n        logger.debug(f\"Input node read URL: {self.url}\")\n\n    def exec(self):\n        # Do work: Get video ID and metadata from the URL\n        # (Simplified - actual logic calls YouTube utilities)\n        self.video_id_result = \"xyz123\"\n        self.metadata_result = {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n        logger.debug(\"Input node got video ID and metadata.\")\n\n    def post(self):\n        # Write the results back to the dictionary\n        self.shared_memory[\"video_id\"] = self.video_id_result\n        self.shared_memory[\"metadata\"] = self.metadata_result\n        logger.debug(\"Input node wrote video_id and metadata to shared memory.\")\n    ```\n\n4.  **Passing Updated Memory:** When the Node's `run()` method finishes, it returns the modified `shared_memory` dictionary back to the orchestrator. The orchestrator then passes this *updated* dictionary to the *next* Node in the sequence.\n\n    ```python\n    # From src/main.py (simplified)\n    # ... after InputProcessingNode runs ...\n    # shared_memory now contains:\n    # {\n    #   \"video_url\": \"https://youtube.com/watch?v=some_id\",\n    #   \"video_id\": \"xyz123\",\n    #   \"metadata\": {\"title\": \"Cool Video\", \"author\": \"Creator\"}\n    # }\n    \n    # 2. Content Extraction Node\n    logger.info(\"Calling ContentExtractionNode...\")\n    content_node = ContentExtractionNode(shared_memory) # Pass the UPDATED memory\n    shared_memory = content_node.run() # Get the next update\n    ```\n\n5.  **Chain Reaction:** This process repeats for every Node in the pipeline. Each Node reads the data left by previous Nodes and adds its own results.\n\n    *   `ContentExtractionNode` reads `\"video_id\"`, does its work, and adds `\"transcript\"`.\n    *   `TopicExtractionNode` reads `\"transcript\"`, does its work, and adds `\"topics\"`.\n    *   ...and so on.\n\nThe `shared_memory` dictionary grows as it passes through the pipeline, accumulating all the results.\n\n## Under the Hood: Passing the Dictionary Around\n\nIs this some magic \"shared\" space in the computer's memory? Not really! It's much simpler in our case.\n\nPython dictionaries are \"mutable\" objects. This means when you pass a dictionary to a function (or a Node's method), you're passing a reference to the *exact same* dictionary object in memory. If the function modifies the dictionary (like adding a new key), the original dictionary that was passed in also changes.\n\nSo, the orchestrator creates *one* dictionary object. It passes a reference to this object to Node 1. Node 1 modifies it directly. The orchestrator then passes the reference to the *same, now modified* object to Node 2, and so on.\n\nHere's a diagram showing this flow:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline (Orchestrator)\n    participant SMDict as Shared Memory (Python Dictionary)\n    participant NodeA as InputProcessingNode\n    participant NodeB as ContentExtractionNode\n\n    Orch->>SMDict: Create { \"video_url\": \"...\" }\n    Orch->>NodeA: run(reference to SMDict)\n    NodeA->>NodeA: prep() [reads \"video_url\" from SMDict]\n    NodeA->>NodeA: exec() [gets video_id, metadata]\n    NodeA->>SMDict: post() [adds \"video_id\", \"metadata\"]\n    NodeA-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now modified\n\n    Orch->>NodeB: run(reference to SMDict)\n    NodeB->>NodeB: prep() [reads \"video_id\" from SMDict]\n    NodeB->>NodeB: exec() [gets transcript]\n    NodeB->>SMDict: post() [adds \"transcript\"]\n    NodeB-->>Orch: Return reference to SMDict\n    Note right of Orch: SMDict is now further modified\n```\n\nIt's like passing the *same physical notebook* from person to person. Each person adds their notes to it before passing it on.\n\n## Example: Reading and Writing\n\nLet's see how the `ContentExtractionNode` might read the `\"video_id\"` written by `InputProcessingNode` and then write the `\"transcript\"`.\n\n```python\n# Inside ContentExtractionNode (simplified)\n\ndef prep(self):\n    # Read the video_id written by the previous node\n    if \"video_id\" in self.shared_memory:\n        self.video_id_to_process = self.shared_memory[\"video_id\"]\n        logger.debug(f\"Content node read video_id: {self.video_id_to_process}\")\n    else:\n        # Handle missing data (important!)\n        logger.error(\"Error: video_id not found in shared memory!\")\n        self.shared_memory[\"error\"] = \"Missing video_id for transcript extraction\"\n\ndef exec(self):\n    # Only run if prep was successful (no error)\n    if \"error\" not in self.shared_memory:\n        # Use self.video_id_to_process to get the transcript\n        # (Simplified - actual logic calls YouTube utilities)\n        self.transcript_result = \"This is the transcript text...\"\n        logger.debug(\"Content node got the transcript.\")\n\ndef post(self):\n    # Only run if prep and exec were successful\n    if \"error\" not in self.shared_memory:\n        # Write the transcript result to shared memory\n        self.shared_memory[\"transcript\"] = self.transcript_result\n        logger.debug(\"Content node wrote transcript to shared memory.\")\n```\nThis Node first checks if the key `\"video_id\"` exists in the `shared_memory` it received (this is crucial error handling!). If it exists, it uses that ID in its `exec` step. Finally, in `post`, it adds its own result under the key `\"transcript\"`. The next Node can now read `\"transcript\"`.\n\n## Why Use a Dictionary?\n\n*   **Simplicity:** It's a fundamental Python data structure, easy to understand and use.\n*   **Flexibility:** Dictionaries can hold various data types (strings, numbers, lists, even other dictionaries), which is perfect for the different kinds of results our Nodes produce.\n*   **Clarity:** Using meaningful keys (like `\"transcript\"`, `\"topics\"`) makes it clear what data is being passed around.\n\n## Conclusion\n\nWe've learned about **Shared Memory**, the crucial mechanism that allows our pipeline Nodes to communicate and share data.\n\n*   It's implemented as a simple **Python dictionary**.\n*   The pipeline orchestrator ([Chapter 1](01_pipeline_orchestration.md)) creates it and passes it sequentially to each Node ([Chapter 2](02_node__pipeline_step_.md)).\n*   Nodes **read** the inputs they need from the dictionary using keys (usually in `prep`).\n*   Nodes **write** their results back into the dictionary using new keys (usually in `post`).\n*   This allows data like the video ID, transcript, and topics to flow smoothly from one step to the next.\n\nUnderstanding Shared Memory is key to seeing how the different parts of our `pf1` project connect and build upon each other's work.\n\nNow that we understand the basic building blocks (Orchestration, Nodes, Shared Memory), we can look at more specialized components. Next, we'll explore a Node that acts as a mini-orchestrator itself, managing the detailed processing of topics.\n\nLet's dive into the [Topic Processing Orchestrator](04_topic_processing_orchestrator.md)!\n---\n# Chapter 4: Topic Processing Orchestrator\n\nWelcome back! In [Chapter 3: Shared Memory](03_shared_memory.md), we learned how different steps (Nodes) in our pipeline share information using a central dictionary called `shared_memory`. We saw how the `TopicExtractionNode` adds a list of topics it found into this shared space.\n\nNow, imagine our video analyzer found 10 different topics in a long video! For each topic, we want to:\n1.  Generate some Questions and Answers (Q&A).\n2.  Create a super simple \"Explain Like I'm 5\" (ELI5) summary.\n\nIf we did this one topic at a time, it could take a while, especially since generating Q&A and ELI5 often involves asking an AI model (like the ones we'll see in [Chapter 6: LLM Utility](06_llm_utility.md)), which can be slow. How can we speed this up?\n\n## The Problem: Analyzing Many Topics Takes Time\n\nLet's say analyzing one topic takes 30 seconds. If we have 10 topics, doing them one after another would take 300 seconds (5 minutes!). That's a long coffee break!\n\n```\nTopic 1 Analysis --> Wait 30s --> Topic 2 Analysis --> Wait 30s --> ... --> Topic 10 Analysis --> Wait 30s\nTotal Time = 10 * 30s = 300s\n```\n\nWe need a smarter way to handle this.\n\n## The Solution: Divide and Conquer with Parallelism!\n\nThink about a big research project given to a team. Does the team lead make everyone wait while one person researches the first part, then the next person researches the second part? No! The lead divides the work: \"Alice, you research Topic A. Bob, you research Topic B. Charlie, you research Topic C. Go!\" Everyone works *at the same time* (in parallel). When they're done, the lead collects all their findings.\n\nThis is exactly what the **Topic Processing Orchestrator** (`TopicOrchestratorNode`) does! It acts like that efficient team lead for analyzing topics.\n\n*   **The Big Task:** Analyze all topics found in the video.\n*   **The Team Lead:** `TopicOrchestratorNode`.\n*   **The Team Members:** Specialized workers called `TopicProcessorNode` (we'll see these workers in action soon).\n*   **The Assignment:** The `TopicOrchestratorNode` gives *each* topic to a *separate* `TopicProcessorNode`.\n*   **Working in Parallel:** It lets all the `TopicProcessorNode`s work simultaneously to generate Q&A and ELI5 for their assigned topic.\n*   **Collecting Results:** Once all workers are finished, the `TopicOrchestratorNode` gathers their results and puts them together.\n\nIf analyzing one topic still takes 30 seconds, but we have, say, 3 workers (or more!) operating in parallel, we can analyze 3 topics in roughly 30 seconds. If we have enough workers for all 10 topics, the total time might be closer to just 30 seconds (plus a little overhead for managing them), instead of 300!\n\n```\n                                Topic 1 Analysis (Worker 1) \\\nSimultaneously Start -->        Topic 2 Analysis (Worker 2)  --> Wait ~30s --> All Done!\n                                Topic 3 Analysis (Worker 3) /\n                                ... and so on ...\n```\n\nThis parallel approach is sometimes called **Map-Reduce**:\n1.  **Map:** Assign (map) each piece of work (topic) to a worker (`TopicProcessorNode`).\n2.  **Reduce:** Collect (reduce) the results from all workers into a final combined output.\n\n## How the Topic Orchestrator Fits In\n\nThe `TopicOrchestratorNode` is just another [Node (Pipeline Step)](02_node__pipeline_step_.md) in our main pipeline, run by the [Pipeline Orchestration](01_pipeline_orchestration.md) (`run_pipeline` function).\n\nHere's what it does in the pipeline flow:\n\n1.  **Input:** It reads the `topics` (a list of strings) and the full `transcript` (a long string) from the [Shared Memory](03_shared_memory.md), put there by previous Nodes.\n2.  **Processing (Parallel):** It performs the \"Map-Reduce\" strategy described above, using multiple `TopicProcessorNode` instances internally.\n3.  **Output:** It writes the collected results back into [Shared Memory](03_shared_memory.md). The results are typically stored in dictionaries keyed by topic, for example:\n    *   `\"qa_pairs\"`: A dictionary where keys are topics and values are lists of Q&A for that topic.\n    *   `\"eli5_content\"`: A dictionary where keys are topics and values are the ELI5 explanations.\n    *   `\"topic_results\"`: A combined dictionary holding both Q&A and ELI5 for each topic.\n\n## Using the Topic Orchestrator Node\n\nWe don't usually call the `TopicOrchestratorNode` directly. The main `run_pipeline` function in `src/main.py` calls it after the topics have been extracted.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory contains 'topics' and 'transcript')\n\n# 4. Topic Orchestration Node\nlogger.info(\"[4/5] Starting Topic Processing Orchestration...\")\n# Create the orchestrator node, giving it shared memory\n# We can configure how many 'workers' (max_workers) it can use\ntopic_orchestrator = TopicOrchestratorNode(shared_memory, max_workers=3)\n\n# Run the node - this triggers the parallel processing\nshared_memory = topic_orchestrator.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"Topic Orchestration failed...\")\n    return shared_memory\n\nlogger.info(\"Successfully processed all topics.\")\n\n# Now, shared_memory contains the new keys like 'qa_pairs' and 'eli5_content'\n# Example: shared_memory['qa_pairs']['Machine Learning Basics'] might be a list of Q&A\n# Example: shared_memory['eli5_content']['Machine Learning Basics'] might be the ELI5 text\n\n# ... (Next node, like HTML Generation, can use these results) ...\n```\n\nIn this example:\n*   We create `TopicOrchestratorNode`, passing the current `shared_memory` (which must contain `\"topics\"` and `\"transcript\"`).\n*   We also tell it it can use up to `max_workers=3` parallel workers.\n*   Calling `topic_orchestrator.run()` kicks off the parallel processing.\n*   After it finishes, the `shared_memory` dictionary is updated with the results (`\"qa_pairs\"`, `\"eli5_content\"`, etc.).\n\n## Under the Hood: How Parallelism Works\n\nLet's visualize how the `TopicOrchestratorNode` (TON) manages the `TopicProcessorNode` (TPN) workers.\n\n```mermaid\nsequenceDiagram\n    participant RP as run_pipeline\n    participant TON as TopicOrchestratorNode\n    participant SM as Shared Memory\n    participant TPN1 as TopicProcessorNode (Worker 1)\n    participant TPN2 as TopicProcessorNode (Worker 2)\n\n    RP->>TON: run(SM) [SM has topics: T1, T2]\n    TON->>SM: prep() [Reads topics=[T1, T2], transcript]\n    Note over TON, TPN2: exec() starts Map Phase (using Thread Pool)\n    TON->>TPN1: Create & submit _process_topic(T1)\n    TON->>TPN2: Create & submit _process_topic(T2)\n    Note over TPN1, TPN2: TPN1 and TPN2 run in parallel\n    TPN1->>TPN1: Runs its own prep/exec/post (gets Q&A/ELI5 for T1)\n    TPN2->>TPN2: Runs its own prep/exec/post (gets Q&A/ELI5 for T2)\n    TPN1-->>TON: Returns result for T1\n    TPN2-->>TON: Returns result for T2\n    Note over TON: Map Phase ends, Reduce Phase starts\n    TON->>TON: _reduce_phase() [Collects results for T1, T2]\n    TON->>SM: post() [Writes combined 'qa_pairs', 'eli5_content' to SM]\n    TON-->>RP: Return updated SM\n```\n\nThis diagram shows:\n1.  `run_pipeline` calls the `TopicOrchestratorNode` (`TON`).\n2.  `TON` reads the list of topics (e.g., T1, T2) from `Shared Memory`.\n3.  `TON` starts its `exec` phase, creating and launching worker `TopicProcessorNode`s (`TPN1`, `TPN2`) in parallel, one for each topic.\n4.  Each `TPN` does its work (generating Q&A/ELI5 for its specific topic).\n5.  `TON` waits for all `TPN` workers to finish and collects their individual results.\n6.  `TON` combines these results and writes them back to `Shared Memory`.\n\n## Diving into the Code (`src/nodes/topic_orchestrator_node.py`)\n\nLet's peek inside the `TopicOrchestratorNode`. Remember, it follows the standard [Node (Pipeline Step)](02_node__pipeline_step_.md) structure: `prep`, `exec`, `post`.\n\n**1. Preparation (`prep`)**\n\nThe `prep` method gets the necessary inputs from `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\nclass TopicOrchestratorNode(BaseNode):\n    # ... (init method stores max_workers) ...\n\n    def prep(self):\n        logger.info(\"TopicOrchestrator: Preparing...\")\n        # Check if needed data exists in shared memory\n        if \"topics\" not in self.shared_memory:\n            raise ValueError(\"Topics not found in shared memory\")\n        if \"transcript\" not in self.shared_memory:\n            raise ValueError(\"Transcript not found in shared memory\")\n\n        # Get the topics and transcript\n        self.topics = self.shared_memory[\"topics\"]\n        self.transcript = self.shared_memory[\"transcript\"]\n        logger.info(f\"Found {len(self.topics)} topics to process.\")\n```\nThis part just checks if the `\"topics\"` list and `\"transcript\"` string are available in the shared memory and stores them within the node instance for later use.\n\n**2. Execution (`exec`)**\n\nThe `exec` method orchestrates the Map and Reduce phases.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def exec(self):\n        logger.info(\"TopicOrchestrator: Executing...\")\n        # Map phase: Process topics in parallel\n        self._map_phase()\n\n        # Reduce phase: Combine the results\n        self._reduce_phase()\n```\nIt simply calls two helper methods: `_map_phase` to run the workers and `_reduce_phase` to collect the results.\n\n**3. Map Phase (`_map_phase`)**\n\nThis is where the parallelism happens using Python's `concurrent.futures.ThreadPoolExecutor`. Think of the executor as a manager for a pool of worker threads.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nimport concurrent.futures # Tool for running things in parallel\n\n    def _map_phase(self):\n        logger.info(f\"Starting Map phase with {self.max_workers} workers...\")\n        self.topic_results = {} # Dictionary to store results\n\n        # Create a pool of workers\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Give each topic to a worker by submitting the _process_topic task\n            future_to_topic = {\n                executor.submit(self._process_topic, topic): topic\n                for topic in self.topics\n            }\n\n            # Wait for workers to finish and collect results\n            for future in concurrent.futures.as_completed(future_to_topic):\n                topic = future_to_topic[future]\n                try:\n                    # Get the result from the finished worker\n                    result = future.result()\n                    self.topic_results[topic] = result # Store result\n                    logger.info(f\"Finished processing topic: {topic}\")\n                except Exception as e:\n                    logger.error(f\"Error processing topic '{topic}': {e}\")\n                    # Store an error message for this topic\n                    self.topic_results[topic] = {\"error\": str(e)}\n```\n*   It creates a `ThreadPoolExecutor` which manages the worker threads.\n*   `executor.submit(self._process_topic, topic)` tells the executor: \"Hey, run the `_process_topic` function with this `topic` as input, using one of your available workers.\" This happens for *all* topics.\n*   `concurrent.futures.as_completed(...)` waits for any worker to finish, gets its result (`future.result()`), and stores it in `self.topic_results` dictionary, keyed by the topic name.\n\n**4. Processing a Single Topic (`_process_topic`)**\n\nThis helper function is what each worker thread actually runs. It creates and runs the specialized `TopicProcessorNode` for a single topic.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\nfrom src.nodes.topic_processor_node import TopicProcessorNode # The worker node\n\n    def _process_topic(self, topic):\n        logger.info(f\"Worker starting on topic: {topic}\")\n        # Create the specialist node for *this specific topic*\n        processor = TopicProcessorNode(\n            topic=topic,\n            transcript=self.transcript,\n            questions_per_topic=self.questions_per_topic # Configurable option\n        )\n        # Run the specialist node and get its results\n        # The result is a dictionary with 'qa_pairs' and 'eli5_content' for this topic\n        result_dict = processor.run()\n        # We only need the part specific to this topic from the processor's output\n        return result_dict[\"topic_results\"][topic]\n\n```\nThis function takes one `topic`, creates a `TopicProcessorNode` specifically for it (giving it the topic and the full transcript), runs that node, and returns the Q&A/ELI5 results just for that single topic.\n\n**5. Reduce Phase (`_reduce_phase`)**\n\nAfter the `_map_phase` has collected results from all workers into `self.topic_results`, the `_reduce_phase` combines them into the final structure needed in `shared_memory`.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def _reduce_phase(self):\n        logger.info(\"Starting Reduce phase...\")\n        qa_pairs = {}\n        eli5_content = {}\n\n        # Go through results collected from workers\n        for topic, result in self.topic_results.items():\n            if \"error\" not in result: # Skip topics that had errors\n                qa_pairs[topic] = result.get(\"qa_pairs\", [])\n                eli5_content[topic] = result.get(\"eli5_content\", \"\")\n\n        # Store the combined results in shared memory\n        self.shared_memory[\"qa_pairs\"] = qa_pairs\n        self.shared_memory[\"eli5_content\"] = eli5_content\n        # Also store the raw results per topic\n        self.shared_memory[\"topic_results\"] = self.topic_results\n        logger.info(\"Reduce phase complete. Results added to shared memory.\")\n```\nThis function iterates through the `self.topic_results` gathered during the map phase. It creates two dictionaries, `qa_pairs` and `eli5_content`, where the keys are the topic strings. It then populates these dictionaries with the results from each worker and finally adds these combined dictionaries to the main `shared_memory`.\n\n**6. Post-Processing (`post`)**\n\nThe `post` method mainly does final checks and logging.\n\n```python\n# Simplified from src/nodes/topic_orchestrator_node.py\n\n    def post(self):\n        # Basic check if any results were generated\n        if not self.topic_results:\n             self.shared_memory[\"error\"] = \"No topic results were generated\"\n             logger.error(self.shared_memory[\"error\"])\n             return # Stop if no results\n\n        logger.info(\"Topic Orchestrator Node completed successfully.\")\n```\n\nAnd that's how the `TopicOrchestratorNode` uses parallelism (Map-Reduce) to speed up the processing of multiple topics by delegating the work to `TopicProcessorNode` instances!\n\n## Conclusion\n\nIn this chapter, we explored the `TopicOrchestratorNode`, a smart manager within our pipeline. We learned:\n\n*   **Problem:** Analyzing many topics one-by-one is slow.\n*   **Solution:** Use parallelism with a **Map-Reduce** approach.\n*   **TopicOrchestratorNode:** Acts as a team lead, assigning each topic to a worker (`TopicProcessorNode`).\n*   **Parallel Execution:** Workers run simultaneously using threads, significantly speeding up the process.\n*   **Map Phase:** Assigns tasks to workers and runs them.\n*   **Reduce Phase:** Collects results from all workers and combines them.\n*   **Integration:** It reads `topics` and `transcript` from [Shared Memory](03_shared_memory.md) and writes back `qa_pairs` and `eli5_content`.\n\nThis node demonstrates a more advanced pattern within our pipeline architecture, showing how we can optimize for performance.\n\nSo far, we've seen how the pipeline flows, how individual nodes work, how they share data, and how we can use parallelism. But where does data like the transcript or video title actually come from? Next, we'll look at the tools used to interact with YouTube itself.\n\nLet's move on to [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)!\n---\n# Chapter 5: YouTube Data Utilities\n\nWelcome back! In the previous chapter, [Topic Processing Orchestrator](04_topic_processing_orchestrator.md), we saw how our pipeline cleverly analyzes multiple topics from a video transcript in parallel. We also saw in earlier chapters how Nodes like `InputProcessingNode` and `ContentExtractionNode` magically seem to get information *about* the video (like its title) and its transcript (the spoken words).\n\nBut where does this information actually *come from*? How does our program talk to YouTube? That's where our **YouTube Data Utilities** come in!\n\n## The Problem: Getting Data *From* YouTube\n\nOur summarizer project starts with just one thing: a YouTube video URL (like `https://www.youtube.com/watch?v=some_video_id`). To do anything useful, we need to extract specific pieces of information directly related to that URL:\n\n1.  **Is this even a real YouTube link?** We need to check if the URL points to a valid YouTube video before we try to process it.\n2.  **What is this video about?** We need basic details like the video's title, the channel that uploaded it, and maybe how long it is (its duration). We call this \"metadata\".\n3.  **What is being said in the video?** For topic analysis and summarization, we crucially need the spoken words, known as the transcript or captions.\n\nSimply having the URL isn't enough. We need tools to interact with YouTube (or YouTube data sources) to fetch this information.\n\n## Our Solution: Specialized YouTube Tools (Utilities)\n\nImagine you have a toolbox. You don't just have one giant \"Do-Everything\" tool. You have specific tools for specific jobs: a screwdriver for screws, a wrench for bolts, a hammer for nails.\n\nOur **YouTube Data Utilities** are like these specialized tools in our project's toolbox. They are *not* full pipeline [Nodes (Pipeline Step)](02_node__pipeline_step_.md) themselves. Instead, they are smaller, focused helper functions that our Nodes *use* to perform specific tasks related to YouTube.\n\nWe have three main utility tools in our `src/utils/` directory:\n\n1.  **The Link Checker (`validate_youtube_url`)**: Checks if a given text string looks like a valid YouTube URL and extracts the unique 11-character video ID.\n2.  **The Info Fetcher (`extract_youtube_metadata`)**: Takes a video ID and fetches details like the title, channel name, duration, etc., by talking to YouTube's official Data API.\n3.  **The Script Grabber (`extract_youtube_transcript`)**: Takes a video ID and retrieves the transcript (the spoken words) for that video using a specialized library.\n\nThese utilities live in their own files (like `validate_youtube_url.py`, `extract_youtube_metadata.py`, `extract_youtube_transcript.py`) inside the `src/utils/` folder, keeping our toolbox organized.\n\n## How Nodes Use These Utilities\n\nOur pipeline Nodes, which we learned about in [Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md), act like the workers on the assembly line. When a worker needs to perform a YouTube-specific action, they reach into the toolbox and use the appropriate utility. This usually happens inside the Node's `exec()` method.\n\n**Example 1: Input Processing Node using the Link Checker and Info Fetcher**\n\nThe `InputProcessingNode`'s job is to take the initial URL, validate it, and get basic video info. Here's a simplified look at how its `exec` method might use our utilities:\n\n```python\n# Simplified from src/nodes/input_processing_node.py\n\n# Import the utility functions (tools)\nfrom src.utils.validate_youtube_url import validate_youtube_url\nfrom src.utils.extract_youtube_metadata import extract_youtube_metadata\n# ... other imports ...\n\nclass InputProcessingNode(BaseNode):\n    # ... (prep method gets the URL from shared_memory) ...\n\n    def exec(self):\n        # Get the URL we stored during prep\n        youtube_url = self.shared_memory[\"video_url\"]\n\n        # Use the 'Link Checker' utility\n        is_valid, video_id = validate_youtube_url(youtube_url)\n\n        if not is_valid:\n            # If the link is bad, report an error and stop\n            self.shared_memory[\"error\"] = \"Invalid YouTube URL\"\n            return\n\n        # Store the extracted video ID in shared memory\n        self.shared_memory[\"video_id\"] = video_id\n\n        # Use the 'Info Fetcher' utility with the video ID\n        metadata = extract_youtube_metadata(video_id)\n\n        # Check if fetching metadata failed\n        if \"error\" in metadata:\n            self.shared_memory[\"error\"] = metadata[\"error\"]\n            return\n\n        # Store the fetched metadata in shared memory\n        self.shared_memory[\"metadata\"] = metadata\n\n    # ... (post method logs success or failure) ...\n\n```\n\nIn this snippet:\n1.  The Node calls `validate_youtube_url` with the input URL.\n2.  If valid, it gets back `True` and the `video_id`. It stores the `video_id` in [Shared Memory](03_shared_memory.md).\n3.  It then calls `extract_youtube_metadata` with the `video_id`.\n4.  If successful, it gets back a dictionary of metadata (title, channel, etc.) and stores it in [Shared Memory](03_shared_memory.md).\n\n**Example 2: Content Extraction Node using the Script Grabber**\n\nThe `ContentExtractionNode`'s job is to get the transcript. It expects the `video_id` to already be in [Shared Memory](03_shared_memory.md) (put there by the `InputProcessingNode`).\n\n```python\n# Simplified from src/nodes/content_extraction_node.py\n\n# Import the utility function (tool)\nfrom src.utils.extract_youtube_transcript import extract_youtube_transcript\n# ... other imports ...\n\nclass ContentExtractionNode(BaseNode):\n    # ... (prep method gets video_id from shared_memory) ...\n\n    def exec(self):\n        # Get the video_id we stored during prep\n        video_id = self.shared_memory[\"video_id\"]\n\n        # Use the 'Script Grabber' utility\n        transcript_text = extract_youtube_transcript(video_id)\n\n        # Check if fetching the transcript failed\n        if transcript_text.startswith(\"Error:\"):\n            self.shared_memory[\"error\"] = transcript_text\n            return\n\n        # Store the fetched transcript in shared memory\n        self.shared_memory[\"transcript\"] = transcript_text\n\n    # ... (post method logs success or failure) ...\n```\n\nHere:\n1.  The Node gets the `video_id` from [Shared Memory](03_shared_memory.md).\n2.  It calls `extract_youtube_transcript` with the `video_id`.\n3.  If successful, it gets back the transcript text as a string and stores it in [Shared Memory](03_shared_memory.md) under the key `\"transcript\"`.\n\nThese examples show how Nodes delegate specific YouTube interactions to our utility functions, keeping the Node's code focused on its main workflow (`prep`, `exec`, `post`).\n\n## Under the Hood: How the Utilities Work\n\nLet's briefly peek inside the toolbox and see how these tools operate.\n\n**1. Link Checker (`validate_youtube_url`)**\n\nThis utility doesn't actually connect to the internet. It just looks at the structure of the URL string you give it. It uses pattern matching (specifically, \"regular expressions\") to see if the URL *looks like* common YouTube video links (e.g., `youtube.com/watch?v=...` or `youtu.be/...`). If it finds a match, it extracts the 11-character part that is the video ID.\n\n```python\n# Simplified from src/utils/validate_youtube_url.py\nimport re # Python's regular expression module\n\ndef validate_youtube_url(url):\n    if not url: return False, \"\"\n\n    # A pattern to match YouTube URLs and capture the ID\n    youtube_regex = (\n        r'(https?://)?(www\\.)?'\n        r'(youtube|youtu|youtube-nocookie)\\.(com|be)/'\n        r'(watch\\?v=|embed/|v/|.+\\?v=)?([^&=%\\?]{11})' # Capture 11 chars\n    )\n    match = re.match(youtube_regex, url)\n\n    if match:\n        video_id = match.group(6) # Get the captured ID part\n        return True, video_id\n    else:\n        # (Also checks other formats like youtu.be/...)\n        return False, \"\"\n```\n\n**2. Info Fetcher (`extract_youtube_metadata`)**\n\nThis utility *does* connect to the internet. It talks to the official **YouTube Data API**, which is a service provided by Google/YouTube for developers to get video information programmatically.\n\n*   **Needs an API Key:** To use this API, you usually need a special key (like a password) to identify yourself. Our utility looks for this key (you'll need to get one from Google and configure it for the project).\n*   **Makes a Request:** It sends the `video_id` and the API key to a specific Google web address.\n*   **Gets a Response:** Google sends back structured data (in JSON format) containing the video's title, description, channel, duration, view count, etc.\n*   **Parses the Response:** The utility extracts the useful bits from the JSON response and returns them as a Python dictionary.\n\n```mermaid\nsequenceDiagram\n    participant Node as InputProcessingNode\n    participant MetadataUtil as extract_youtube_metadata()\n    participant YT_API as YouTube Data API (Google)\n\n    Node->>MetadataUtil: extract_youtube_metadata(\"some_video_id\")\n    Note over MetadataUtil: Needs API Key!\n    MetadataUtil->>YT_API: GET /youtube/v3/videos?id=some_video_id&key=API_KEY&part=snippet...\n    YT_API-->>MetadataUtil: Return JSON data { \"items\": [ { \"snippet\": { \"title\": \"...\", ... } } ] }\n    MetadataUtil->>MetadataUtil: Parse JSON, extract title, channel, etc.\n    MetadataUtil-->>Node: Return metadata_dictionary\n```\n\n```python\n# Simplified from src/utils/extract_youtube_metadata.py\nimport requests # Library to make web requests\nimport os\n\ndef extract_youtube_metadata(video_id):\n    # Get API key (needs to be set up separately)\n    api_key = os.environ.get(\"YOUTUBE_API_KEY\", \"YOUR_API_KEY\")\n    if api_key == \"YOUR_API_KEY\":\n        return {\"error\": \"YouTube API Key not configured\"}\n\n    api_url = \"https://www.googleapis.com/youtube/v3/videos\"\n    params = {\n        \"part\": \"snippet,contentDetails,statistics\", # Ask for specific info\n        \"id\": video_id,\n        \"key\": api_key\n    }\n\n    try:\n        response = requests.get(api_url, params=params)\n        response.raise_for_status() # Check for web errors (like 404)\n        data = response.json() # Parse the JSON response\n\n        if not data.get(\"items\"):\n             return {\"error\": \"Video not found or API key invalid\"}\n\n        # ... (code to extract title, channel, duration etc. from data) ...\n        metadata = { \"title\": \"...\", \"channel_name\": \"...\", ... }\n        return metadata\n\n    except Exception as e:\n        return {\"error\": f\"API request error: {str(e)}\"}\n```\n\n**3. Script Grabber (`extract_youtube_transcript`)**\n\nThis utility uses a third-party Python library called `youtube_transcript_api`. This library knows how to find and download the captions or transcripts that YouTube provides for many videos.\n\n*   **Uses the Library:** It calls functions from the `youtube_transcript_api` library, passing the `video_id`.\n*   **Library Handles Complexity:** The library deals with finding available languages, downloading the transcript data (which includes text and timing information), and potentially translating it if needed.\n*   **Returns Text:** Our utility simplifies the result, combining all the text pieces into one single string and returning it. It also handles cases where transcripts are disabled or not found.\n\n```python\n# Simplified from src/utils/extract_youtube_transcript.py\n\n# Import the specialized library\nfrom youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n\ndef extract_youtube_transcript(video_id):\n    try:\n        # Ask the library to find and fetch the transcript (prefer English)\n        transcript_list = YouTubeTranscriptApi.list_transcripts(video_id)\n        transcript = transcript_list.find_transcript(['en'])\n        transcript_data = transcript.fetch()\n\n        # Combine the text parts into one string\n        full_transcript = \"\"\n        for segment in transcript_data:\n            full_transcript += segment['text'] + \" \"\n\n        return full_transcript.strip()\n\n    except TranscriptsDisabled:\n        return \"Error: Transcripts are disabled for this video.\"\n    except NoTranscriptFound:\n        return \"Error: No transcript found for this video.\"\n    except Exception as e:\n        return f\"Error extracting transcript: {str(e)}\"\n```\n\n## Conclusion\n\nIn this chapter, we opened up the toolbox and looked at our **YouTube Data Utilities**. These are essential helper functions that allow our project to interact directly with YouTube data.\n\n*   They are **specialized tools** for specific tasks: validating URLs, fetching metadata, and grabbing transcripts.\n*   They are **not Nodes**, but rather **used by Nodes** (like `InputProcessingNode` and `ContentExtractionNode`) within their `exec` methods.\n*   They live in the `src/utils/` directory, keeping our code organized.\n*   They handle the details of pattern matching (`validate_youtube_url`), talking to the YouTube Data API (`extract_youtube_metadata`), and using libraries to get transcripts (`extract_youtube_transcript`).\n\nThese utilities provide the raw materials (metadata, transcript) that the rest of our pipeline ([Pipeline Orchestration](01_pipeline_orchestration.md)) processes to create the final summary.\n\nNow that we have the video's transcript, how do we actually analyze it to find topics, generate Q&A, or create ELI5 summaries? This often involves using powerful Artificial Intelligence models. Let's explore the utility designed to help us interact with these AI models next.\n\nReady to see how we talk to the AI? Let's move on to [Chapter 6: LLM Utility](06_llm_utility.md)!\n---\n# Chapter 6: LLM Utility\n\nWelcome back! In [Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md), we saw how our project uses specialized tools to fetch raw data from YouTube, like the video's title and the full transcript (the spoken words). We have the text, but how do we make *sense* of it? How do we automatically figure out the main topics discussed, generate questions and answers about them, or create super-simple explanations?\n\nThat sounds like a job for a super-smart assistant!\n\n## The Problem: Understanding and Generating Content\n\nImagine you have the complete script of a movie. Just reading the script doesn't automatically tell you the main plot points, the underlying themes, or answers to specific questions about the story. You need to *analyze* it.\n\nSimilarly, our Nodes, like the `TopicExtractionNode` or the `TopicProcessorNode`, need to analyze the video transcript. They need to:\n*   Identify the core subjects being discussed.\n*   Maybe create questions a viewer might have about those subjects.\n*   Perhaps summarize a complex idea in simple terms.\n\nDoing this analysis accurately and creatively requires advanced understanding of language and context \u2013 something traditional programming struggles with. We need help from a powerful Artificial Intelligence (AI).\n\n## Our Solution: A Direct Line to an AI Expert (LLM Utility)\n\nModern AI includes something called **Large Language Models (LLMs)**. Think of models like GPT-4 (from OpenAI) as incredibly knowledgeable experts you can chat with. They have read vast amounts of text and can understand language, generate creative text, answer questions, summarize information, and much more.\n\nBut how do our different Nodes talk to this AI expert? We don't want each Node to figure out the complex details of connecting to the AI service, handling authentication, formatting requests, and parsing responses every time. That would be repetitive and complicated!\n\nSo, we created the **LLM Utility**. It's like having a **dedicated, standardized phone line** straight to our AI expert (the LLM).\n\n*   **Standardized:** Every Node uses the *same* simple function (`call_llm`) to talk to the AI.\n*   **Simplified:** The utility hides all the complex connection details. Nodes just need to know *what* question to ask.\n*   **Centralized:** All AI communication goes through this one utility, making it easier to manage and update.\n\nAny Node in our pipeline that needs help from the AI \u2013 whether it's the `TopicExtractionNode` asking \"What are the main topics here?\" or the `TopicProcessorNode` asking \"Explain this topic like I'm 5\" \u2013 uses this same utility function.\n\n## Asking the Right Question: The Prompt\n\nWhen you call an expert, you don't just say \"Hello?\". You need to give them context and clearly state what you need. When we talk to an LLM, this is called a **Prompt**.\n\nA prompt typically includes:\n1.  **Instructions:** What task should the AI perform? (e.g., \"Identify the main topics\", \"Generate questions and answers\", \"Explain this simply\")\n2.  **Context:** What information does the AI need to do the task? (e.g., the video transcript segment, the specific topic name)\n3.  **Format Constraints (Optional):** How should the AI format its answer? (e.g., \"Respond with ONLY a JSON list\", \"Keep it under 100 words\")\n\nHere's a simple example of a prompt the `TopicExtractionNode` might create:\n\n```text\nYou are an expert at analyzing video content.\nI have a segment of a video transcript. Please identify the 3 main topics discussed.\nList them as short phrases.\n\nTranscript Segment:\n[... a chunk of the video transcript text goes here ...]\n\nRespond with ONLY a JSON array of topic strings, like [\"Topic A\", \"Topic B\", \"Topic C\"].\n```\nThis prompt tells the AI its role, the task, provides the necessary text, and specifies the output format.\n\n## How Nodes Use the LLM Utility: `call_llm`\n\nOur Nodes use a single, simple function from our toolbox (`src/utils/`) called `call_llm`. This function takes the carefully crafted prompt and handles sending it to the AI and getting the response back.\n\nLet's look at a simplified example from the `TopicExtractionNode` (which we saw in the context provided earlier). Inside its `_process_chunk` method, it needs to find topics within a piece of the transcript (`chunk`):\n\n```python\n# Simplified from src/nodes/topic_extraction_node.py\n\n# Import the utility function\nfrom src.utils.call_llm import call_llm\n# ... other imports like textwrap ...\n\nclass TopicExtractionNode(BaseNode):\n    # ... other methods like __init__, prep ...\n\n    def _process_chunk(self, chunk_index: int, chunk: str):\n        logger.info(f\"Processing chunk {chunk_index+1}...\")\n\n        # 1. Create the prompt (instructions + context)\n        prompt = textwrap.dedent(f\"\"\"\n        You are an expert at analyzing video content.\n        Identify the main topics in this transcript segment.\n        Provide at most {self.max_topics} topics as short phrases.\n\n        Transcript segment:\n        {chunk[:2000]}...\n\n        Respond with ONLY a JSON array of topic strings.\n        \"\"\")\n\n        # 2. Call the LLM Utility!\n        try:\n            logger.info(f\"Calling LLM for chunk {chunk_index+1}...\")\n            # Just one simple function call to talk to the AI!\n            response_text = call_llm(\n                prompt=prompt,\n                temperature=0.3, # Lower temperature = more focused/deterministic output\n                max_tokens=200,  # Limit the length of the response\n                timeout=30       # Wait max 30 seconds for a response\n            )\n            logger.info(f\"Received LLM response for chunk {chunk_index+1}\")\n\n            # 3. Process the response (e.g., parse the JSON)\n            # ... (code to handle the response_text) ...\n            # extracted_topics = parse_json(response_text)\n            # return extracted_topics\n\n        except Exception as e:\n            logger.error(f\"Error calling LLM for chunk {chunk_index+1}: {str(e)}\")\n            # ... (handle error) ...\n            return []\n\n```\n\nIn this example:\n1.  The Node constructs the `prompt` string with instructions and the relevant `chunk` of the transcript.\n2.  It makes a single call to `call_llm`, passing the `prompt`. It can also pass optional parameters like:\n    *   `temperature`: Controls creativity. Lower values (like 0.3) make the output more predictable and focused; higher values make it more random.\n    *   `max_tokens`: Limits how long the AI's response can be.\n    *   `timeout`: Sets a maximum time to wait for the AI to respond.\n3.  The `call_llm` function handles all the communication complexity. It returns the AI's response as a simple text string (`response_text`).\n4.  The Node then processes this `response_text` (e.g., parsing it if it expects JSON).\n\nThat's it! The Node doesn't need to worry about API keys, network requests, or complex error handling for the AI communication itself \u2013 the `call_llm` utility takes care of it.\n\n## Under the Hood: Dialing the Expert\n\nWhat actually happens when a Node calls `call_llm`? Let's trace the call.\n\n1.  **Get Credentials:** The `call_llm` function first needs the secret \"phone number\" and \"password\" to connect to the AI service (like OpenAI). This is usually an **API Key**, which it securely reads from the system's environment settings (so we don't store secrets directly in the code).\n2.  **Prepare the Call:** It packages the `prompt` and any other parameters (like `temperature`) into the specific format the AI service expects.\n3.  **Make the Connection:** It uses a library (like OpenAI's official Python library) to send the request over the internet to the AI service's servers.\n4.  **Wait for Response:** It waits for the AI to process the prompt and generate a response. This can take a few seconds. It also keeps an eye on the `timeout` limit.\n5.  **Receive Answer:** It gets the response text back from the AI service.\n6.  **Return or Report Error:** If successful, it returns the response text to the Node. If anything went wrong (bad API key, network error, timeout, rate limit), it logs the error and returns a helpful error message string.\n\nHere\u2019s a simplified diagram of this flow:\n\n```mermaid\nsequenceDiagram\n    participant Node as Pipeline Node (e.g., TopicExtraction)\n    participant LLMUtil as call_llm() Utility\n    participant OpenAI_Lib as OpenAI Python Library\n    participant OpenAI_API as OpenAI API Service\n\n    Node->>LLMUtil: call_llm(prompt=\"...\", ...)\n    LLMUtil->>LLMUtil: Get API Key (from environment)\n    Note right of LLMUtil: Prepare request data\n    LLMUtil->>OpenAI_Lib: client.chat.completions.create(...)\n    OpenAI_Lib->>OpenAI_API: Send HTTPS Request (prompt, key, etc.)\n    Note over OpenAI_API: Processes request...\n    OpenAI_API-->>OpenAI_Lib: Return HTTPS Response (text, usage, etc.)\n    OpenAI_Lib-->>LLMUtil: Return response object\n    LLMUtil->>LLMUtil: Extract response text\n    LLMUtil-->>Node: Return response string (or error message)\n```\n\n## Diving into the Code (`src/utils/call_llm.py`)\n\nLet's peek inside the `call_llm` function in `src/utils/call_llm.py`. Remember, we'll keep it simple!\n\n**1. Getting the API Key**\n\nThe function first needs to find the secret API key. It looks for an \"environment variable\" named `OPENAI_API_KEY`. You need to set this variable up in your system separately.\n\n```python\n# Simplified from src/utils/call_llm.py\nimport os\nimport logging\n\ndef call_llm(prompt, ...):\n    # Get API key from environment variable\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\n\n    if not api_key:\n        logging.error(\"OpenAI API key not found!\")\n        return \"Error: OpenAI API key not found.\"\n\n    # ... (rest of the function)\n```\nThis code uses Python's `os` module to read the environment variable. If it's not found, it immediately returns an error message.\n\n**2. Setting up the Connection (Client)**\n\nNext, it uses the official `openai` library to create a \"client\" object, which knows how to talk to the OpenAI API.\n\n```python\n# Simplified from src/utils/call_llm.py\nfrom openai import OpenAI # The official library\n# ... other imports ...\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7, max_tokens=1000, timeout=60):\n    # ... (get api_key) ...\n\n    try:\n        # Initialize the client with the API key\n        client = OpenAI(api_key=api_key)\n\n        # ... (make the API call using 'client') ...\n\n    except Exception as e:\n        logging.exception(\"Error initializing OpenAI client\")\n        return f\"Error initializing client: {str(e)}\"\n```\nThis sets up the communication channel using the API key.\n\n**3. Making the Call**\n\nNow, it uses the `client` object to actually send the prompt to the specified AI model.\n\n```python\n# Simplified from src/utils/call_llm.py\n# ... inside the 'try' block ...\n\n        try:\n            logging.debug(f\"Calling OpenAI model {model}...\")\n            # This is the actual call to the AI service\n            response = client.chat.completions.create(\n                model=model,\n                messages=[{\"role\": \"user\", \"content\": prompt}], # Pass the prompt\n                temperature=temperature,     # Pass other parameters\n                max_tokens=max_tokens,\n                timeout=timeout\n            )\n            logging.debug(\"OpenAI call successful.\")\n\n            # ... (extract the text response) ...\n\n        except Exception as api_error:\n            logging.error(f\"OpenAI API error: {str(api_error)}\")\n            # Handle specific errors like timeouts or rate limits\n            if \"timeout\" in str(api_error).lower():\n                 return \"Error: LLM API call timed out.\"\n            # ... other error handling ...\n            return f\"Error calling LLM API: {str(api_error)}\"\n\n```\nThe `client.chat.completions.create` method does the heavy lifting of communicating with OpenAI. We wrap this in another `try...except` block to catch specific API errors like timeouts.\n\n**4. Getting the Answer**\n\nIf the call is successful, the `response` object contains the AI's answer. We need to extract the actual text content.\n\n```python\n# Simplified from src/utils/call_llm.py\n# ... inside the successful API call 'try' block ...\n\n            # Extract the text from the response object\n            content = response.choices[0].message.content\n            logging.debug(f\"Received response length: {len(content)}\")\n            return content # Return the text to the Node\n\n```\nThe useful text is nested inside the `response` object. We extract it and return it as a simple string.\n\nThis utility simplifies AI interaction for all Nodes, providing a consistent and robust way to leverage the power of LLMs.\n\n## Conclusion\n\nIn this chapter, we learned about the **LLM Utility**, our project's standardized way of communicating with Large Language Models like GPT-4.\n\n*   **Problem:** Nodes need to perform complex language tasks (topic extraction, Q&A, summarization) that require AI.\n*   **Solution:** A central utility function (`call_llm`) provides a simple, consistent interface to the AI.\n*   **Analogy:** It's like a dedicated phone line to an AI expert.\n*   **Prompts:** Nodes craft instructions and context (`prompts`) to tell the AI what to do.\n*   **Usage:** Nodes call `call_llm(prompt, ...)` and receive the AI's response as text.\n*   **Under the Hood:** The utility handles API keys, network requests, timeouts, and basic error handling using the official OpenAI library.\n\nThis utility is a key component, enabling sophisticated analysis and content generation within our pipeline. Without it, tasks like topic extraction or creating ELI5 summaries would be much harder!\n\nNow that we've gathered video info ([Chapter 5](05_youtube_data_utilities.md)), extracted the transcript, and analyzed it using the LLM ([Chapter 6](06_llm_utility.md)) to get topics, Q&A, and summaries stored in [Shared Memory](03_shared_memory.md), how do we present all this information nicely to the user?\n\nLet's move on to the final step: putting everything together into a user-friendly report with [Chapter 7: HTML Generation](07_html_generation.md)!\n---\n# Chapter 7: HTML Generation\n\nWelcome back! In [Chapter 6: LLM Utility](06_llm_utility.md), we saw how our project uses a powerful AI assistant (a Large Language Model) to analyze the video transcript and generate insightful content like topics, Q&A pairs, and simple ELI5 explanations. All these results are carefully stored in our central [Shared Memory](03_shared_memory.md) dictionary.\n\nBut right now, all that great information is just sitting inside a Python dictionary. That's not very easy for a person to read or share! Imagine getting a big pile of notes and data scribbled on scraps of paper. You'd want someone to organize it into a polished, easy-to-read final report.\n\nThat's exactly what **HTML Generation** does in our `pf1` project!\n\n## The Problem: Presenting Results Nicely\n\nOur pipeline has worked hard:\n1.  It fetched video info ([Chapter 5: YouTube Data Utilities](05_youtube_data_utilities.md)).\n2.  It extracted the transcript.\n3.  It identified topics ([Chapter 4: Topic Processing Orchestrator](04_topic_processing_orchestrator.md)).\n4.  It generated Q&A and ELI5 summaries using AI ([Chapter 6: LLM Utility](06_llm_utility.md)).\n\nAll these pieces \u2013 the video title, the list of topics, the questions and answers, the simple explanations \u2013 are collected in the [Shared Memory](03_shared_memory.md) dictionary. But how do we show this to the user in a way that looks good and is easy to understand?\n\n## Our Solution: The Report Creator (HTML Generation Node)\n\nWe need a final step in our pipeline that acts like a **report generator** or a **desktop publisher**. Its job is to take all the processed data and arrange it neatly into a final document.\n\nIn our project, this final document is an **HTML file**. HTML (HyperText Markup Language) is the standard language used to create web pages. By generating an HTML file, we create a report that can be easily opened and viewed in any web browser (like Chrome, Firefox, or Safari), complete with nice formatting, images, and links.\n\nThe component responsible for this is the **`HTMLGenerationNode`**. It's typically the *last* [Node (Pipeline Step)](02_node__pipeline_step_.md) run by our [Pipeline Orchestration](01_pipeline_orchestration.md).\n\n**What it does:**\n1.  **Gathers Ingredients:** Reads all the necessary results (metadata, topics, Q&A, ELI5) from the [Shared Memory](03_shared_memory.md).\n2.  **Follows a Recipe:** Uses a template (like a recipe) to structure this information into an HTML format.\n3.  **Creates the Dish:** Generates the final HTML text.\n4.  **Serves It:** Saves this HTML text into a file (e.g., `summary.html`) that the user can open.\n\n## How the HTML Generation Node Fits In\n\nThe `run_pipeline` function ([Chapter 1: Pipeline Orchestration](01_pipeline_orchestration.md)) calls the `HTMLGenerationNode` after all the analysis is done.\n\n```python\n# Simplified snippet from src/main.py\n\n# ... (Previous nodes ran, shared_memory has metadata, topics, qa_pairs, eli5_content)\n\n# 5. HTML Generation Node\nlogger.info(\"[5/5] Starting HTML Generation...\")\n# Create the node, telling it where to save the file\nhtml_node = HTMLGenerationNode(\n    shared_memory,\n    output_path=f\"{output_dir}/summary_{shared_memory['video_id']}.html\"\n)\n\n# Run the node - this creates the HTML content and file\nshared_memory = html_node.run()\n\n# Check for errors\nif \"error\" in shared_memory:\n    logger.error(f\"HTML Generation failed...\")\n    # Handle error\nelse:\n    logger.info(f\"HTML report saved successfully!\")\n    # The path to the HTML file is usually stored in shared_memory now\n    # (or determined by the output_path we gave it)\n\nlogger.info(\"Pipeline finished!\")\nreturn shared_memory\n```\nHere, the orchestrator creates the `HTMLGenerationNode`, passes it the final `shared_memory` containing all results, and tells it where to save the output file (e.g., `output/summary_videoID.html`). Calling `html_node.run()` triggers the creation of the HTML report.\n\n## What Happens Inside the Node: Prep, Exec, Post\n\nLike all our Nodes ([Chapter 2: Node (Pipeline Step)](02_node__pipeline_step_.md)), the `HTMLGenerationNode` follows the standard `prep -> exec -> post` workflow.\n\n1.  **`prep()` (Get Ready):**\n    *   Checks the [Shared Memory](03_shared_memory.md) to make sure all the required data pieces are present (e.g., `\"metadata\"`, `\"topics\"`, `\"qa_pairs\"`, `\"eli5_content\"`).\n    *   If anything crucial is missing, it raises an error.\n    *   If a previous step already reported an error, it might decide to skip HTML generation.\n\n2.  **`exec()` (Do the Work):**\n    *   Bundles the required data (metadata, topics, etc.) into a structure that's easy to work with.\n    *   Calls a special helper function (a utility like the ones in [Chapter 5](05_youtube_data_utilities.md)) called `generate_html`. This utility takes the data and does the actual work of creating the HTML string.\n    *   Stores the generated HTML content back into [Shared Memory](03_shared_memory.md) (e.g., under the key `\"html_output\"`).\n    *   If an `output_path` was provided (like in the `main.py` example), it writes the generated HTML content to that file on the disk.\n\n3.  **`post()` (Clean Up):**\n    *   Logs whether the HTML was generated successfully.\n    *   Checks if the HTML content was actually created and added to shared memory. Reports an error if not.\n\n## The Magic Tool: `generate_html` Utility\n\nThe `HTMLGenerationNode` doesn't contain messy HTML code directly. Instead, it delegates the task of creating the HTML structure to a helper function: `generate_html` (found in `src/utils/generate_html.py`).\n\nThink of `generate_html` as a **template filler**. It has a predefined structure for the HTML page (like a fill-in-the-blanks form letter) and knows where to plug in the specific data (video title, topics, Q&A, etc.) that it receives from the `HTMLGenerationNode`.\n\n**How it works (simplified):**\n1.  **Gets Data:** Receives the `summary_data` dictionary from the Node.\n2.  **Starts HTML:** Begins creating the HTML string with standard tags (`<html>`, `<head>`, `<body>`).\n3.  **Adds Styling:** Includes CSS rules inside `<style>` tags to make the page look nice (colors, fonts, layout).\n4.  **Inserts Header:** Puts the video title, channel name, thumbnail, and YouTube link near the top.\n5.  **Loops Through Topics:** Goes through each topic in the list. For each topic:\n    *   Creates a section header (e.g., \"Topic 1: Music Video Plot\").\n    *   Adds the ELI5 explanation for that topic.\n    *   Adds the Q&A pairs associated with that topic, formatting the questions and answers clearly.\n6.  **Adds Footer:** Puts some concluding text at the bottom.\n7.  **Returns HTML:** Returns the complete HTML content as one long string.\n\n**Important Note on Security:** When putting data from potentially external sources (like video titles or Q&A generated by AI) into HTML, it's crucial to \"escape\" special characters (like `<`, `>`, `&`). The `generate_html` utility uses Python's `html.escape()` function to do this, preventing potential security issues like Cross-Site Scripting (XSS).\n\n## Visualizing the Flow\n\nHere's how the `HTMLGenerationNode` works with the `generate_html` utility:\n\n```mermaid\nsequenceDiagram\n    participant Orch as run_pipeline\n    participant HTMLNode as HTMLGenerationNode\n    participant SM as Shared Memory\n    participant HTMLUtil as generate_html()\n    participant FileSystem as Output File\n\n    Orch->>HTMLNode: run(SM, output_path)\n    HTMLNode->>SM: prep() [Reads metadata, topics, qa_pairs, eli5_content]\n    Note right of HTMLNode: Checks if all data is present\n    HTMLNode->>HTMLNode: exec() [Bundles data]\n    HTMLNode->>HTMLUtil: generate_html(summary_data)\n    HTMLUtil->>HTMLUtil: Build HTML string using data & template\n    HTMLUtil-->>HTMLNode: Return html_content string\n    HTMLNode->>SM: Store html_content in Shared Memory\n    HTMLNode->>FileSystem: Write html_content to output_path\n    HTMLNode->>HTMLNode: post() [Log success]\n    HTMLNode-->>Orch: Return updated SM\n```\nThis shows the Node getting data from Shared Memory, using the utility to create the HTML, and then saving it both back to Shared Memory and to the file system.\n\n## Diving into the Code\n\nLet's look at simplified snippets.\n\n**`HTMLGenerationNode` (`src/nodes/html_generation_node.py`)**\n\n```python\n# Simplified from src/nodes/html_generation_node.py\nimport os\nfrom src.nodes.base_node import BaseNode\nfrom src.utils.generate_html import generate_html # Import the helper\nfrom src.utils.logger import logger\n\nclass HTMLGenerationNode(BaseNode):\n    def __init__(self, shared_memory=None, output_path=None):\n        super().__init__(shared_memory)\n        self.output_path = output_path # Store the save location\n        logger.debug(\"HTML Node ready.\")\n\n    def prep(self):\n        # Check if needed data exists in shared_memory\n        required = [\"video_id\", \"metadata\", \"topics\", \"qa_pairs\", \"eli5_content\"]\n        if not all(key in self.shared_memory for key in required):\n            raise ValueError(\"Missing data for HTML generation!\")\n        logger.info(\"Preparing to generate HTML.\")\n\n    def exec(self):\n        # Bundle the data\n        summary_data = {key: self.shared_memory[key] for key in required}\n\n        # Call the utility to create the HTML string\n        logger.debug(\"Calling generate_html function\")\n        html_content = generate_html(summary_data)\n        self.shared_memory[\"html_output\"] = html_content\n\n        # Save to file if path is given\n        if self.output_path:\n            try:\n                # Ensure directory exists\n                os.makedirs(os.path.dirname(self.output_path), exist_ok=True)\n                # Write file (UTF-8 ensures special characters work)\n                with open(self.output_path, \"w\", encoding=\"utf-8\") as f:\n                    f.write(html_content)\n                logger.info(f\"HTML saved to: {self.output_path}\")\n            except Exception as e:\n                logger.error(f\"Error saving HTML file: {e}\")\n                self.shared_memory[\"error\"] = f\"Failed to save HTML: {e}\"\n\n    def post(self):\n        if \"error\" not in self.shared_memory:\n            logger.info(\"HTML Generation completed successfully.\")\n        # (Error logging happens in prep/exec)\n```\nThis Node clearly shows the `prep` check, calling `generate_html` in `exec`, storing the result, and saving the file.\n\n**`generate_html` Utility (`src/utils/generate_html.py`)**\n\n```python\n# Simplified from src/utils/generate_html.py\nimport html # For escaping special characters\n\ndef generate_html(summary_data):\n    # Get data, use defaults if missing\n    title = html.escape(summary_data.get(\"metadata\", {}).get(\"title\", \"Summary\"))\n    topics = summary_data.get(\"topics\", [])\n    qa_pairs = summary_data.get(\"qa_pairs\", {})\n    eli5 = summary_data.get(\"eli5_content\", {})\n\n    # Start building the HTML string\n    html_str = f\"\"\"<!DOCTYPE html>\n<html><head><title>{title}</title><style>/* CSS styles */</style></head><body>\n<h1>Summary for: {title}</h1>\"\"\"\n\n    # Loop through topics\n    for i, topic in enumerate(topics):\n        topic_esc = html.escape(topic)\n        eli5_esc = html.escape(eli5.get(topic, \"N/A\"))\n        html_str += f\"<h2>Topic {i+1}: {topic_esc}</h2><p>{eli5_esc}</p>\"\n\n        # Add Q&A for this topic\n        html_str += \"<h3>Questions & Answers</h3>\"\n        topic_qas = qa_pairs.get(topic, [])\n        if topic_qas:\n            for qa in topic_qas:\n                q = html.escape(qa.get(\"question\", \"\"))\n                a = html.escape(qa.get(\"answer\", \"\"))\n                html_str += f\"<div class='question'>Q: {q}</div><div class='answer'>A: {a}</div>\"\n        else:\n            html_str += \"<p>No Q&A available.</p>\"\n\n    # Finish the HTML\n    html_str += \"<footer>Generated by pf1.</footer></body></html>\"\n    return html_str\n```\nThis utility function takes the data and constructs the HTML string, making sure to use `html.escape()` on dynamic content like titles, topics, questions, and answers.\n\n## Conclusion\n\nIn this chapter, we explored the final step in our pipeline: **HTML Generation**.\n\n*   **Purpose:** To present all the analyzed video information (metadata, topics, Q&A, ELI5) in a user-friendly, visually appealing format.\n*   **Mechanism:** The `HTMLGenerationNode` reads the results from [Shared Memory](03_shared_memory.md).\n*   **Helper Utility:** It uses the `generate_html` function, which acts like a template filler, to create the actual HTML content.\n*   **Output:** Produces an HTML string (stored in Shared Memory) and saves it as an `.html` file that can be opened in a web browser.\n\nThis node turns the complex data collected throughout the pipeline into a tangible, understandable report for the end-user.\n\nThroughout these chapters, we've seen mentions of `logger.info`, `logger.error`, etc. How does our program keep track of what it's doing, report progress, and help us find problems? That's the job of our logging system.\n\nLet's dive into how logging works in our project in the next chapter: [Chapter 8: Logging](08_logging.md)!\n\nRelevant Code Snippets:\n--- File: src/utils/logger.py ---\n\"\"\"\nLogging module for YouTube Video Summarizer.\n\"\"\"\nimport os\nimport sys\nfrom datetime import datetime\nfrom loguru import logger\n\ndef setup_logger(log_dir=\"logs\"):\n    \"\"\"\n    Set up the logger with appropriate configuration.\n    \n    Args:\n        log_dir (str): Directory to store log files\n        \n    Returns:\n        logger: Configured logger instance\n    \"\"\"\n    # Create logs directory if it doesn't exist\n    os.makedirs(log_dir, exist_ok=True)\n    \n    # Generate timestamp for the log file\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(log_dir, f\"youtube_summarizer_{timestamp}.log\")\n    \n    # Remove any existing handlers\n    logger.remove()\n    \n    # Add console handler with color\n    logger.add(\n        sys.stdout,\n        format=\"<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>\",\n        level=\"INFO\",\n        colorize=True\n    )\n    \n    # Add file handler\n    logger.add(\n        log_file,\n        format=\"{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}\",\n        level=\"DEBUG\",\n        rotation=\"20 MB\",\n        retention=\"1 week\"\n    )\n    \n    logger.info(f\"Logging initialized. Log file: {log_file}\")\n    return logger\n\n# Initialize logger\nlogger = setup_logger()\n\nif __name__ == \"__main__\":\n    # Test the logger\n    logger.debug(\"This is a debug message\")\n    logger.info(\"This is an info message\")\n    logger.warning(\"This is a warning message\")\n    logger.error(\"This is an error message\")\n    logger.critical(\"This is a critical message\")\n    \n    print(f\"Log file created in the logs directory\")\n\n\nInstructions for the chapter:\n- Start with a clear heading (e.g., `# Chapter 8: Logging`).\n\n- If this is not the first chapter, begin with a brief transition from the previous chapter, referencing it with a proper Markdown link.\n\n- Begin with a high-level motivation explaining what problem this abstraction solves. Start with a central use case as a concrete example. The whole chapter should guide the reader to understand how to solve this use case. Make it very minimal and friendly to beginners.\n\n- If the abstraction is complex, break it down into key concepts. Explain each concept one-by-one in a very beginner-friendly way.\n\n- Explain how to use this abstraction to solve the use case. Give example inputs and outputs for code snippets (if the output isn't values, describe at a high level what will happen). \n\n- Each code block should be BELOW 20 lines! If longer code blocks are needed, break them down into smaller pieces and walk through them one-by-one. Aggresively simplify the code to make it minimal. Use comments to skip non-important implementation details. Each code block should have a beginner friendly explanation right after it.\n\n- Describe the internal implementation to help understand what's under the hood. First provide a non-code or code-light walkthrough on what happens step-by-step when the abstraction is called. It's recommended to use a simple sequenceDiagram with a dummy example - keep it minimal with at most 5 participants to ensure clarity. If participant name has space, use: \n`participant QP as Query Processing`\n\n- Then dive deeper into code for the internal implementation with references to files. Provide example code blocks, but make them similarly simple and beginner-friendly.\n\n- IMPORTANT: When you need to refer to other core abstractions covered in other chapters, ALWAYS use proper Markdown links like this: [Chapter Title](filename.md). Use the Complete Tutorial Structure above to find the correct filename. Example: \"we will talk about [Query Processing](03_query_processing.md) in Chapter 3\".\n\n- Use mermaid diagrams to illustrate complex concepts (```mermaid``` format).\n\n- Heavily use analogies and examples throughout to help beginners understand.\n\n- End the chapter with a brief conclusion that summarizes what was learned and provides a transition to the next chapter. If there is a next chapter, use a proper Markdown link: [Next Chapter Title](next_chapter_filename).\n\n- Ensure the tone is welcoming and easy for a newcomer to understand.\n\n- Output *only* the Markdown content for this chapter.\n\nNow, directly provide a super beginner-friendly Markdown output (DON'T need ```markdown``` tags):\n": "# Chapter 8: Logging\n\nWelcome to the final chapter! In [Chapter 7: HTML Generation](07_html_generation.md), we saw how our `pf1` pipeline takes all the analyzed data and creates a beautiful, easy-to-read HTML report. Our program successfully went from a YouTube URL to a full summary!\n\nBut wait... while the pipeline was running all those steps \u2013 fetching data, extracting transcripts, talking to AI, generating HTML \u2013 how did we know what was happening? What if something went wrong halfway through? How could we track the progress or diagnose a problem?\n\nThat's where **Logging** comes in!\n\n## What is Logging? The Ship's Logbook\n\nImagine a ship on a long voyage. The captain and crew keep a detailed **logbook**. They record important events: when they left port, the weather conditions, any interesting sightings, course changes, and crucially, any problems encountered (like a leak or equipment failure). Later, anyone can read this logbook to understand what happened during the voyage.\n\n**Logging** in a computer program is exactly like keeping this logbook. It's a way for the program to write down messages about what it's doing as it runs. These messages can include:\n*   Information about progress (\"Starting transcript extraction...\")\n*   Warnings about potential issues (\"API key not found, using default...\")\n*   Errors that occurred (\"Failed to connect to YouTube API!\")\n*   Detailed debugging information useful for developers.\n\n## Why Do We Need Logging?\n\nKeeping this \"program logbook\" is super important for several reasons:\n\n1.  **Monitoring Progress:** When you run the `pf1` summarizer, especially on a long video, it might take a while. Logging messages printed to the screen (the \"console\") tell you what step the program is currently working on, so you know it hasn't crashed.\n    *   Example: Seeing `[1/5] Starting Input Processing...`, `[2/5] Starting Content Extraction...` helps you track its progress.\n2.  **Diagnosing Problems (Debugging):** If the program crashes or gives a weird result, the log messages are often the first place developers look. Error messages pinpoint where things went wrong, and earlier informational messages show the sequence of events leading up to the failure. It's like reading the ship's log to figure out why it went off course.\n3.  **Understanding Behaviour:** Sometimes, you just want to understand *how* the program made a decision or what data it was working with at a certain point. Detailed logs can provide this insight.\n\nWithout logging, running a complex program would be like navigating a ship in thick fog with no instruments \u2013 you wouldn't know where you are, where you're going, or if you're about to hit something!\n\n## Logging Levels: Different Kinds of Messages\n\nNot all events are equally important. A ship's log might note \"Sighted dolphins\" (interesting info) but would highlight \"Taking on water!\" (critical error) much more prominently.\n\nLogging systems use different **levels** to categorize messages:\n\n*   **INFO:** General information about the program's progress. What's happening now? (e.g., `Starting HTML generation...`)\n*   **WARNING:** Indicates something unexpected happened, but the program can probably continue. It's a heads-up. (e.g., `Transcript language not specified, defaulting to English.`)\n*   **ERROR:** A serious problem occurred. The current task likely failed, but the program might try to continue with other tasks if possible. (e.g., `Failed to extract topics for chunk 3.`)\n*   **CRITICAL:** A very severe error, likely meaning the entire program has to stop. (e.g., `Cannot create output directory! Aborting.`)\n*   **DEBUG:** Very detailed information, mostly useful only for developers trying to fix bugs. (e.g., `Variable 'x' has value: 42`)\n\nYou can usually configure the logging system to only show messages above a certain level. For normal use, you might only want to see `INFO` and above. When debugging, you might enable `DEBUG` messages too.\n\n## Our Logging Setup in `pf1` (`src/utils/logger.py`)\n\nIn our `pf1` project, we use a popular and easy-to-use Python library called **Loguru** to handle logging. We have a dedicated setup file: `src/utils/logger.py`.\n\nThis file does one main thing: it sets up our \"logger\" object, telling it *how* and *where* to record messages.\n\nHere\u2019s a simplified look at the `setup_logger` function inside that file:\n\n```python\n# Simplified from src/utils/logger.py\nimport sys\nimport os\nfrom datetime import datetime\nfrom loguru import logger # Import the library\n\ndef setup_logger(log_dir=\"logs\"):\n    # Create a directory to store log files if it doesn't exist\n    os.makedirs(log_dir, exist_ok=True)\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    log_file = os.path.join(log_dir, f\"youtube_summarizer_{timestamp}.log\")\n\n    # --- Configuration Starts Here ---\n    logger.remove() # Clear any default settings\n\n    # 1. Log to the Console (what you see on screen)\n    logger.add(\n        sys.stdout, # 'stdout' means the standard console output\n        level=\"INFO\", # Show INFO messages and above (WARNING, ERROR)\n        colorize=True, # Make the output colorful!\n        format=\"<level>{level: <8}</level> | <level>{message}</level>\" # Simple format\n    )\n\n    # 2. Log to a File (for detailed history)\n    logger.add(\n        log_file, # The path to the log file (e.g., logs/youtube_summarizer_20231027_103000.log)\n        level=\"DEBUG\", # Record EVERYTHING (including detailed DEBUG messages) in the file\n        format=\"{time} | {level} | {name}:{function} - {message}\" # More detailed format\n    )\n    # --- Configuration Ends Here ---\n\n    logger.info(\"Logging initialized.\")\n    return logger\n\n# Create the logger instance that the rest of the project will use\nlogger = setup_logger()\n```\n**Explanation:**\n\n1.  **Import Loguru:** We bring in the `loguru` library.\n2.  **Create Log File Path:** It decides where to save the log file, putting it in a `logs` folder and adding a timestamp to the filename so each run gets its own file.\n3.  **Console Logging (`logger.add(sys.stdout, ...)`):**\n    *   This tells Loguru to send messages to your terminal screen (`sys.stdout`).\n    *   `level=\"INFO\"` means you'll see `INFO`, `WARNING`, `ERROR`, and `CRITICAL` messages on the screen, but not `DEBUG` messages (they are usually too noisy for regular users).\n    *   `colorize=True` makes the output easier to read with colors (e.g., errors might be red).\n    *   `format=\"...\"` defines how each log message looks on the screen (here, just the level and the message).\n4.  **File Logging (`logger.add(log_file, ...)`):**\n    *   This tells Loguru to also write messages to the `log_file` it prepared earlier.\n    *   `level=\"DEBUG\"` means *all* messages, including the detailed `DEBUG` ones, will be saved in the file. This is super helpful for developers later!\n    *   `format=\"...\"` uses a slightly different format for the file, including the timestamp, level, and where the message came from (file name, function name).\n5.  **Return Logger:** The function returns the configured `logger` object. The last line, `logger = setup_logger()`, makes this configured logger available for the rest of the project to import and use.\n\nSo, when our program logs something, it usually appears in two places: nicely formatted on your screen, and stored with full details in a log file!\n\n## How to Use the Logger in `pf1`\n\nUsing the logger in any part of our project (like inside a [Node (Pipeline Step)](02_node__pipeline_step_.md) or a utility function) is very simple:\n\n1.  **Import it:** Add this line at the top of your Python file:\n    ```python\n    from src.utils.logger import logger\n    ```\n\n2.  **Call its methods:** Whenever you want to record something, call the appropriate method on the `logger` object:\n    *   `logger.info(\"This is just information.\")`\n    *   `logger.warning(\"This might be a problem.\")`\n    *   `logger.error(\"Something definitely went wrong here.\")`\n    *   `logger.debug(\"Details for the developers.\")`\n\n**Example inside a hypothetical Node:**\n\n```python\n# hypothetical_node.py\nfrom src.utils.logger import logger # 1. Import the logger\n\nclass HypotheticalNode:\n    def __init__(self, shared_memory):\n        self.shared_memory = shared_memory\n        logger.info(\"HypotheticalNode created.\") # Log during creation\n\n    def prep(self):\n        logger.debug(\"Prep phase starting.\") # Detailed debug message\n        if \"input_data\" not in self.shared_memory:\n            logger.error(\"Input data is missing!\") # Log an error\n            self.shared_memory[\"error\"] = \"Missing input_data\"\n            return # Stop prep\n        self.data = self.shared_memory[\"input_data\"]\n        logger.info(\"Preparation complete. Data found.\") # Log success info\n\n    def exec(self):\n        if \"error\" in self.shared_memory: return # Don't run if prep failed\n        logger.info(\"Executing main task...\") # Log start of execution\n        # ... do the actual work ...\n        result = f\"Processed {self.data}\"\n        self.result = result\n        logger.debug(f\"Intermediate result: {result}\") # Log internal details\n\n    def post(self):\n        if \"error\" in self.shared_memory: return # Don't run if errors\n        logger.info(\"Post-processing results...\") # Log start of post\n        self.shared_memory[\"output\"] = self.result\n        logger.info(\"Results added to shared memory.\") # Log completion\n\n    def run(self):\n        self.prep()\n        self.exec()\n        self.post()\n        logger.info(\"HypotheticalNode finished.\") # Log node completion\n        return self.shared_memory\n\n# --- Example Usage (not part of the Node class) ---\n# node = HypotheticalNode({\"input_data\": \"some value\"})\n# node.run()\n```\nWhen this code runs:\n*   You'd see `INFO` and `ERROR` messages on your console (because we set console level to `INFO`).\n*   The log file (`logs/youtube_summarizer_....log`) would contain *all* the messages, including the `DEBUG` ones, along with timestamps and function names.\n\n## Under the Hood: How Loguru Knows Where You Are\n\nYou might wonder how the log message in the file includes details like the function name (`prep`, `exec`) where the log was called. Loguru is clever! When you call `logger.info(\"...\")`, Loguru automatically inspects the program's current state to find out which file, function, and line number made the call. It then includes this information in the log record, which the file handler uses in its format string.\n\nHere's a simplified view of what happens:\n\n```mermaid\nsequenceDiagram\n    participant Node as HypotheticalNode\n    participant Logger as logger (Loguru)\n    participant ConsoleH as Console Handler\n    participant FileH as File Handler\n    participant Console as Your Terminal\n    participant LogFile as logs/summarizer.log\n\n    Node->>Logger: logger.info(\"Executing main task...\")\n    Logger->>Logger: Create log record (message, level=INFO, function=exec, ...)\n    Logger->>ConsoleH: Process record\n    ConsoleH->>ConsoleH: Format message (INFO | Executing...)\n    ConsoleH->>Console: Print formatted message\n    Logger->>FileH: Process record\n    FileH->>FileH: Format message (Time | INFO | hypothetical_node:exec - Executing...)\n    FileH->>LogFile: Append formatted message\n```\nThis shows the Node calling the logger. The logger creates a detailed record and passes it to both configured handlers (Console and File). Each handler formats the record according to its settings and sends it to the final destination (screen or file).\n\n## Conclusion\n\nAnd that's logging! It's the essential practice of recording events during your program's execution. We learned:\n\n*   **What it is:** Like a ship's logbook for your code.\n*   **Why it's crucial:** For monitoring progress, diagnosing errors (debugging), and understanding behaviour.\n*   **Levels:** Messages have different severities (DEBUG, INFO, WARNING, ERROR, CRITICAL).\n*   **In `pf1`:** We use the **Loguru** library, configured in `src/utils/logger.py`.\n*   **Dual Output:** Logs go to the console (INFO+) with colors and to a timestamped file (DEBUG+) with full details.\n*   **Usage:** Simply `from src.utils.logger import logger` and call methods like `logger.info()`, `logger.error()`, etc.\n\nEffective logging turns a mysterious black box program into something transparent and understandable, making development and troubleshooting much easier.\n\n---\n\n**This concludes our tutorial series for the `pf1` YouTube Summarizer project!**\n\nWe've journeyed through the core concepts:\n1.  [Pipeline Orchestration](01_pipeline_orchestration.md): The overall manager coordinating the steps.\n2.  [Node (Pipeline Step)](02_node__pipeline_step_.md): The specialized workers performing individual tasks.\n3.  [Shared Memory](03_shared_memory.md): The central dictionary for passing data between Nodes.\n4.  [Topic Processing Orchestrator](04_topic_processing_orchestrator.md): A specialized Node managing parallel topic analysis.\n5.  [YouTube Data Utilities](05_youtube_data_utilities.md): Tools for fetching data from YouTube.\n6.  [LLM Utility](06_llm_utility.md): Our interface for talking to the AI.\n7.  [HTML Generation](07_html_generation.md): Creating the final user-friendly report.\n8.  [Logging](08_logging.md): Recording the program's journey.\n\nWe hope this gives you a solid foundation for understanding how `pf1` works and empowers you to explore, modify, and contribute to the project. Happy coding!"}