<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>06 Llm Utility</title>
    <style>
        pre { line-height: 125%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #ffffcc }
.highlight { background: #f8f8f8; }
.highlight .c { color: #3D7B7B; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #F00 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666 } /* Operator */
.highlight .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #9C6500 } /* Comment.Preproc */
.highlight .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */
.highlight .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #E40000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #008400 } /* Generic.Inserted */
.highlight .go { color: #717171 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #04D } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #687822 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #00F; font-weight: bold } /* Name.Class */
.highlight .no { color: #800 } /* Name.Constant */
.highlight .nd { color: #A2F } /* Name.Decorator */
.highlight .ni { color: #717171; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #00F } /* Name.Function */
.highlight .nl { color: #767600 } /* Name.Label */
.highlight .nn { color: #00F; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #A2F; font-weight: bold } /* Operator.Word */
.highlight .w { color: #BBB } /* Text.Whitespace */
.highlight .mb { color: #666 } /* Literal.Number.Bin */
.highlight .mf { color: #666 } /* Literal.Number.Float */
.highlight .mh { color: #666 } /* Literal.Number.Hex */
.highlight .mi { color: #666 } /* Literal.Number.Integer */
.highlight .mo { color: #666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #A45A77 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #00F } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666 } /* Literal.Number.Integer.Long */
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        .navigation {
            display: flex;
            justify-content: space-between;
            margin: 20px 0;
            padding: 10px;
            background-color: #f6f8fa;
            border-radius: 6px;
        }
    </style>
</head>
<body>
    <div class="navigation">
        <a class="nav-link" href="05_youtube_data_utilities.html">&larr; Previous</a>
        <a class="nav-link" href="index.html">Index</a>
        <a class="nav-link" href="07_html_generation.html">Next &rarr;</a>
    </div>
    <h1>Chapter 6: LLM Utility</h1>

<p>Welcome back! In <a rel="noopener" target="_blank" href="05_youtube_data_utilities.md">Chapter 5: YouTube Data Utilities</a>, we saw how our project uses specialized tools to fetch raw data from YouTube, like the video's title and the full transcript (the spoken words). We have the text, but how do we make <em>sense</em> of it? How do we automatically figure out the main topics discussed, generate questions and answers about them, or create super-simple explanations?</p>

<p>That sounds like a job for a super-smart assistant!</p>

<h2>The Problem: Understanding and Generating Content</h2>

<p>Imagine you have the complete script of a movie. Just reading the script doesn't automatically tell you the main plot points, the underlying themes, or answers to specific questions about the story. You need to <em>analyze</em> it.</p>

<p>Similarly, our Nodes, like the <code>TopicExtractionNode</code> or the <code>TopicProcessorNode</code>, need to analyze the video transcript. They need to:</p>

<ul>
<li>Identify the core subjects being discussed.</li>
<li>Maybe create questions a viewer might have about those subjects.</li>
<li>Perhaps summarize a complex idea in simple terms.</li>
</ul>

<p>Doing this analysis accurately and creatively requires advanced understanding of language and context – something traditional programming struggles with. We need help from a powerful Artificial Intelligence (AI).</p>

<h2>Our Solution: A Direct Line to an AI Expert (LLM Utility)</h2>

<p>Modern AI includes something called <strong>Large Language Models (LLMs)</strong>. Think of models like GPT-4 (from OpenAI) as incredibly knowledgeable experts you can chat with. They have read vast amounts of text and can understand language, generate creative text, answer questions, summarize information, and much more.</p>

<p>But how do our different Nodes talk to this AI expert? We don't want each Node to figure out the complex details of connecting to the AI service, handling authentication, formatting requests, and parsing responses every time. That would be repetitive and complicated!</p>

<p>So, we created the <strong>LLM Utility</strong>. It's like having a <strong>dedicated, standardized phone line</strong> straight to our AI expert (the LLM).</p>

<ul>
<li><strong>Standardized:</strong> Every Node uses the <em>same</em> simple function (<code>call_llm</code>) to talk to the AI.</li>
<li><strong>Simplified:</strong> The utility hides all the complex connection details. Nodes just need to know <em>what</em> question to ask.</li>
<li><strong>Centralized:</strong> All AI communication goes through this one utility, making it easier to manage and update.</li>
</ul>

<p>Any Node in our pipeline that needs help from the AI – whether it's the <code>TopicExtractionNode</code> asking "What are the main topics here?" or the <code>TopicProcessorNode</code> asking "Explain this topic like I'm 5" – uses this same utility function.</p>

<h2>Asking the Right Question: The Prompt</h2>

<p>When you call an expert, you don't just say "Hello?". You need to give them context and clearly state what you need. When we talk to an LLM, this is called a <strong>Prompt</strong>.</p>

<p>A prompt typically includes:</p>

<ol>
<li><strong>Instructions:</strong> What task should the AI perform? (e.g., "Identify the main topics", "Generate questions and answers", "Explain this simply")</li>
<li><strong>Context:</strong> What information does the AI need to do the task? (e.g., the video transcript segment, the specific topic name)</li>
<li><strong>Format Constraints (Optional):</strong> How should the AI format its answer? (e.g., "Respond with ONLY a JSON list", "Keep it under 100 words")</li>
</ol>

<p>Here's a simple example of a prompt the <code>TopicExtractionNode</code> might create:</p>

<div class="codehilite">
<pre><span></span><code>You are an expert at analyzing video content.
I have a segment of a video transcript. Please identify the 3 main topics discussed.
List them as short phrases.

Transcript Segment:
[... a chunk of the video transcript text goes here ...]

Respond with ONLY a JSON array of topic strings, like [&quot;Topic A&quot;, &quot;Topic B&quot;, &quot;Topic C&quot;].
</code></pre>
</div>

<p>This prompt tells the AI its role, the task, provides the necessary text, and specifies the output format.</p>

<h2>How Nodes Use the LLM Utility: <code>call_llm</code></h2>

<p>Our Nodes use a single, simple function from our toolbox (<code>src/utils/</code>) called <code>call_llm</code>. This function takes the carefully crafted prompt and handles sending it to the AI and getting the response back.</p>

<p>Let's look at a simplified example from the <code>TopicExtractionNode</code> (which we saw in the context provided earlier). Inside its <code>_process_chunk</code> method, it needs to find topics within a piece of the transcript (<code>chunk</code>):</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/nodes/topic_extraction_node.py</span>

<span class="c1"># Import the utility function</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">src.utils.call_llm</span><span class="w"> </span><span class="kn">import</span> <span class="n">call_llm</span>
<span class="c1"># ... other imports like textwrap ...</span>

<span class="k">class</span><span class="w"> </span><span class="nc">TopicExtractionNode</span><span class="p">(</span><span class="n">BaseNode</span><span class="p">):</span>
    <span class="c1"># ... other methods like __init__, prep ...</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_process_chunk</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Processing chunk </span><span class="si">{</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>

        <span class="c1"># 1. Create the prompt (instructions + context)</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">textwrap</span><span class="o">.</span><span class="n">dedent</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are an expert at analyzing video content.</span>
<span class="s2">        Identify the main topics in this transcript segment.</span>
<span class="s2">        Provide at most </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_topics</span><span class="si">}</span><span class="s2"> topics as short phrases.</span>

<span class="s2">        Transcript segment:</span>
<span class="s2">        </span><span class="si">{</span><span class="n">chunk</span><span class="p">[:</span><span class="mi">2000</span><span class="p">]</span><span class="si">}</span><span class="s2">...</span>

<span class="s2">        Respond with ONLY a JSON array of topic strings.</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">)</span>

        <span class="c1"># 2. Call the LLM Utility!</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Calling LLM for chunk </span><span class="si">{</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="c1"># Just one simple function call to talk to the AI!</span>
            <span class="n">response_text</span> <span class="o">=</span> <span class="n">call_llm</span><span class="p">(</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">temperature</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="c1"># Lower temperature = more focused/deterministic output</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="c1"># Limit the length of the response</span>
                <span class="n">timeout</span><span class="o">=</span><span class="mi">30</span>       <span class="c1"># Wait max 30 seconds for a response</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Received LLM response for chunk </span><span class="si">{</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

            <span class="c1"># 3. Process the response (e.g., parse the JSON)</span>
            <span class="c1"># ... (code to handle the response_text) ...</span>
            <span class="c1"># extracted_topics = parse_json(response_text)</span>
            <span class="c1"># return extracted_topics</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Error calling LLM for chunk </span><span class="si">{</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># ... (handle error) ...</span>
            <span class="k">return</span> <span class="p">[]</span>
</code></pre>
</div>

<p>In this example:</p>

<ol>
<li>The Node constructs the <code>prompt</code> string with instructions and the relevant <code>chunk</code> of the transcript.</li>
<li>It makes a single call to <code>call_llm</code>, passing the <code>prompt</code>. It can also pass optional parameters like:
<ul>
<li><code>temperature</code>: Controls creativity. Lower values (like 0.3) make the output more predictable and focused; higher values make it more random.</li>
<li><code>max_tokens</code>: Limits how long the AI's response can be.</li>
<li><code>timeout</code>: Sets a maximum time to wait for the AI to respond.</li>
</ul></li>
<li>The <code>call_llm</code> function handles all the communication complexity. It returns the AI's response as a simple text string (<code>response_text</code>).</li>
<li>The Node then processes this <code>response_text</code> (e.g., parsing it if it expects JSON).</li>
</ol>

<p>That's it! The Node doesn't need to worry about API keys, network requests, or complex error handling for the AI communication itself – the <code>call_llm</code> utility takes care of it.</p>

<h2>Under the Hood: Dialing the Expert</h2>

<p>What actually happens when a Node calls <code>call_llm</code>? Let's trace the call.</p>

<ol>
<li><strong>Get Credentials:</strong> The <code>call_llm</code> function first needs the secret "phone number" and "password" to connect to the AI service (like OpenAI). This is usually an <strong>API Key</strong>, which it securely reads from the system's environment settings (so we don't store secrets directly in the code).</li>
<li><strong>Prepare the Call:</strong> It packages the <code>prompt</code> and any other parameters (like <code>temperature</code>) into the specific format the AI service expects.</li>
<li><strong>Make the Connection:</strong> It uses a library (like OpenAI's official Python library) to send the request over the internet to the AI service's servers.</li>
<li><strong>Wait for Response:</strong> It waits for the AI to process the prompt and generate a response. This can take a few seconds. It also keeps an eye on the <code>timeout</code> limit.</li>
<li><strong>Receive Answer:</strong> It gets the response text back from the AI service.</li>
<li><strong>Return or Report Error:</strong> If successful, it returns the response text to the Node. If anything went wrong (bad API key, network error, timeout, rate limit), it logs the error and returns a helpful error message string.</li>
</ol>

<p>Here’s a simplified diagram of this flow:</p>

<pre><code>sequenceDiagram
    participant Node as Pipeline Node (e.g., TopicExtraction)
    participant LLMUtil as call_llm() Utility
    participant OpenAI_Lib as OpenAI Python Library
    participant OpenAI_API as OpenAI API Service

    Node-&gt;&gt;LLMUtil: call_llm(prompt="...", ...)
    LLMUtil-&gt;&gt;LLMUtil: Get API Key (from environment)
    Note right of LLMUtil: Prepare request data
    LLMUtil-&gt;&gt;OpenAI_Lib: client.chat.completions.create(...)
    OpenAI_Lib-&gt;&gt;OpenAI_API: Send HTTPS Request (prompt, key, etc.)
    Note over OpenAI_API: Processes request...
    OpenAI_API--&gt;&gt;OpenAI_Lib: Return HTTPS Response (text, usage, etc.)
    OpenAI_Lib--&gt;&gt;LLMUtil: Return response object
    LLMUtil-&gt;&gt;LLMUtil: Extract response text
    LLMUtil--&gt;&gt;Node: Return response string (or error message)
</code></pre>

<h2>Diving into the Code (<code>src/utils/call_llm.py</code>)</h2>

<p>Let's peek inside the <code>call_llm</code> function in <code>src/utils/call_llm.py</code>. Remember, we'll keep it simple!</p>

<p><strong>1. Getting the API Key</strong></p>

<p>The function first needs to find the secret API key. It looks for an "environment variable" named <code>OPENAI_API_KEY</code>. You need to set this variable up in your system separately.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/utils/call_llm.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>

<span class="k">def</span><span class="w"> </span><span class="nf">call_llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
    <span class="c1"># Get API key from environment variable</span>
    <span class="n">api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">api_key</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;OpenAI API key not found!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;Error: OpenAI API key not found.&quot;</span>

    <span class="c1"># ... (rest of the function)</span>
</code></pre>
</div>

<p>This code uses Python's <code>os</code> module to read the environment variable. If it's not found, it immediately returns an error message.</p>

<p><strong>2. Setting up the Connection (Client)</strong></p>

<p>Next, it uses the official <code>openai</code> library to create a "client" object, which knows how to talk to the OpenAI API.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/utils/call_llm.py</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span> <span class="c1"># The official library</span>
<span class="c1"># ... other imports ...</span>

<span class="k">def</span><span class="w"> </span><span class="nf">call_llm</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o&quot;</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="c1"># ... (get api_key) ...</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Initialize the client with the API key</span>
        <span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">)</span>

        <span class="c1"># ... (make the API call using &#39;client&#39;) ...</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">exception</span><span class="p">(</span><span class="s2">&quot;Error initializing OpenAI client&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error initializing client: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre>
</div>

<p>This sets up the communication channel using the API key.</p>

<p><strong>3. Making the Call</strong></p>

<p>Now, it uses the <code>client</code> object to actually send the prompt to the specified AI model.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/utils/call_llm.py</span>
<span class="c1"># ... inside the &#39;try&#39; block ...</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Calling OpenAI model </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">...&quot;</span><span class="p">)</span>
            <span class="c1"># This is the actual call to the AI service</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span> <span class="c1"># Pass the prompt</span>
                <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>     <span class="c1"># Pass other parameters</span>
                <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
                <span class="n">timeout</span><span class="o">=</span><span class="n">timeout</span>
            <span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;OpenAI call successful.&quot;</span><span class="p">)</span>

            <span class="c1"># ... (extract the text response) ...</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">api_error</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OpenAI API error: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">api_error</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="c1"># Handle specific errors like timeouts or rate limits</span>
            <span class="k">if</span> <span class="s2">&quot;timeout&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">api_error</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
                 <span class="k">return</span> <span class="s2">&quot;Error: LLM API call timed out.&quot;</span>
            <span class="c1"># ... other error handling ...</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Error calling LLM API: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">api_error</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
</code></pre>
</div>

<p>The <code>client.chat.completions.create</code> method does the heavy lifting of communicating with OpenAI. We wrap this in another <code>try...except</code> block to catch specific API errors like timeouts.</p>

<p><strong>4. Getting the Answer</strong></p>

<p>If the call is successful, the <code>response</code> object contains the AI's answer. We need to extract the actual text content.</p>

<div class="codehilite">
<pre><span></span><code><span class="c1"># Simplified from src/utils/call_llm.py</span>
<span class="c1"># ... inside the successful API call &#39;try&#39; block ...</span>

            <span class="c1"># Extract the text from the response object</span>
            <span class="n">content</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Received response length: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">content</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">content</span> <span class="c1"># Return the text to the Node</span>
</code></pre>
</div>

<p>The useful text is nested inside the <code>response</code> object. We extract it and return it as a simple string.</p>

<p>This utility simplifies AI interaction for all Nodes, providing a consistent and robust way to leverage the power of LLMs.</p>

<h2>Conclusion</h2>

<p>In this chapter, we learned about the <strong>LLM Utility</strong>, our project's standardized way of communicating with Large Language Models like GPT-4.</p>

<ul>
<li><strong>Problem:</strong> Nodes need to perform complex language tasks (topic extraction, Q&amp;A, summarization) that require AI.</li>
<li><strong>Solution:</strong> A central utility function (<code>call_llm</code>) provides a simple, consistent interface to the AI.</li>
<li><strong>Analogy:</strong> It's like a dedicated phone line to an AI expert.</li>
<li><strong>Prompts:</strong> Nodes craft instructions and context (<code>prompts</code>) to tell the AI what to do.</li>
<li><strong>Usage:</strong> Nodes call <code>call_llm(prompt, ...)</code> and receive the AI's response as text.</li>
<li><strong>Under the Hood:</strong> The utility handles API keys, network requests, timeouts, and basic error handling using the official OpenAI library.</li>
</ul>

<p>This utility is a key component, enabling sophisticated analysis and content generation within our pipeline. Without it, tasks like topic extraction or creating ELI5 summaries would be much harder!</p>

<p>Now that we've gathered video info (<a rel="noopener" target="_blank" href="05_youtube_data_utilities.md">Chapter 5</a>), extracted the transcript, and analyzed it using the LLM (<a rel="noopener" target="_blank" href="06_llm_utility.md">Chapter 6</a>) to get topics, Q&amp;A, and summaries stored in <a rel="noopener" target="_blank" href="03_shared_memory.md">Shared Memory</a>, how do we present all this information nicely to the user?</p>

<p>Let's move on to the final step: putting everything together into a user-friendly report with <a rel="noopener" target="_blank" href="07_html_generation.md">Chapter 7: HTML Generation</a>!</p>

<hr />

<p>Generated by <a rel="noopener" target="_blank" href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>

    <div class="navigation">
        <a class="nav-link" href="05_youtube_data_utilities.html">&larr; Previous</a>
        <a class="nav-link" href="index.html">Index</a>
        <a class="nav-link" href="07_html_generation.html">Next &rarr;</a>
    </div>
</body>
</html>